{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d84a327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (2025.7.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.2.0 (from torch)\n",
      "  Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl (766.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m  \u001b[33m0:00:08\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m180.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m183.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m  \u001b[33m0:00:07\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m141.6 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-cusparselt-cu12, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "\u001b[2K  Attempting uninstall: sympy━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/16\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K    Found existing installation: sympy 1.14.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/16\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K    Uninstalling sympy-1.14.0:0m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/16\u001b[0m [sympy]parselt-cu12]\n",
      "\u001b[2K      Successfully uninstalled sympy-1.14.0━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/16\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/16\u001b[0m [torch]m15/16\u001b[0m [torch]-cusolver-cu12]2]\n",
      "\u001b[1A\u001b[2KSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.6.0 triton-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43e77a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, glob, random, copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm # Use tqdm.notebook for Jupyter progress bars\n",
    "\n",
    "# --- PyTorch Imports for ML Model ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51ab87d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Determine if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cc0d443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ML Model Configuration ---\n",
    "# WINDOW_SIZE = 150       # Number of time steps (data points) in each input sequence/window \n",
    "WINDOW_SIZE = 1050       # 300 = 1 min 1050 = 3.5 min\n",
    "STRIDE = WINDOW_SIZE #5             # How many steps to move the window (1 means maximum overlap, covering every point)\n",
    "\n",
    "LR = 0.005  #1e-3\n",
    "EPOCHS = 50            # Number of training epochs for the autoencoder\n",
    "BATCH_SIZE = 256        # 128 Batch size for training\n",
    "EARLY_STOP_PATIENCE = 15\n",
    "DECISION_THR = 0.5 \n",
    "\n",
    "\n",
    "# --- Post-processing configuration (applied to ML predictions) ---\n",
    "MERGE_GAP_SECONDS = 6             # Maximum time gap between predicted wake segments to merge them\n",
    "MIN_WAKE_DURATION_SECONDS = 0.5     # Minimum duration for a detected event to be considered a wake\n",
    "\n",
    "# --- Paths ---\n",
    "base_data_dir = \"processed_ts\"\n",
    "output_base_dir_ml = \"detected_wakes_CLS\"\n",
    "plots_output_dir_ml = os.path.join(output_base_dir_ml, \"wake_plots/LSTM\", f\"w{WINDOW_SIZE},s{STRIDE}\")\n",
    "os.makedirs(plots_output_dir_ml, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41ebd868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads the CSV data and performs basic preprocessing.\n",
    "    Ensures 't_s' is sorted and resets index.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        df = df.sort_values(by='t_s').reset_index(drop=True)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or preprocessing data from {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b361706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ground_truth_wakes(df):\n",
    "    \"\"\"\n",
    "    Extracts ground truth wake intervals from the 'wake_label' column.\n",
    "    Returns a list of (start_time, end_time) tuples.\n",
    "    \"\"\"\n",
    "    ground_truth_wakes = []\n",
    "    in_wake = False\n",
    "    wake_start_time = None\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        t_s = row['t_s']\n",
    "        wake_label = row['wake_label']\n",
    "\n",
    "        if wake_label == 1 and not in_wake:\n",
    "            wake_start_time = t_s\n",
    "            in_wake = True\n",
    "        elif wake_label == 0 and in_wake:\n",
    "            wake_end_time = t_s\n",
    "            ground_truth_wakes.append((wake_start_time, wake_end_time))\n",
    "            in_wake = False\n",
    "    \n",
    "    # Handle case where wake extends to the end of the time series\n",
    "    if in_wake:\n",
    "        ground_truth_wakes.append((wake_start_time, df['t_s'].iloc[-1]))\n",
    "\n",
    "    return ground_truth_wakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e239500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(interval1, interval2):\n",
    "    \"\"\"\n",
    "    Calculates the Intersection over Union (IoU) of two time intervals.\n",
    "    Intervals are (start, end) tuples.\n",
    "    \"\"\"\n",
    "    start1, end1 = interval1\n",
    "    start2, end2 = interval2\n",
    "\n",
    "    # Calculate intersection duration\n",
    "    intersection_start = max(start1, start2)\n",
    "    intersection_end = min(end1, end2)\n",
    "    \n",
    "    intersection_duration = max(0, intersection_end - intersection_start)\n",
    "\n",
    "    # Calculate union duration\n",
    "    union_duration = (max(end1, end2) - min(start1, start2))\n",
    "\n",
    "    if union_duration == 0:\n",
    "        return 0.0 # No union means no overlap possible\n",
    "    \n",
    "    return intersection_duration / union_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd08df25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_detection(ground_truth_wakes, predicted_wakes, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluates the detection performance based on IoU.\n",
    "    Returns TP, FP, FN, Accuracy, Precision, Recall, F1-Score.\n",
    "    \"\"\"\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    # Sets to keep track of matched ground truth and used predicted wakes\n",
    "    matched_gt_indices = set()\n",
    "\n",
    "    # Iterate through predicted wakes to find the best ground truth match for each\n",
    "    for pred_idx, pred_wake in enumerate(predicted_wakes):\n",
    "        best_iou_for_pred = 0.0\n",
    "        potential_gt_idx_for_pred = -1\n",
    "\n",
    "        for gt_idx, gt_wake in enumerate(ground_truth_wakes):\n",
    "            if gt_idx in matched_gt_indices: # If this GT wake is already matched, skip it\n",
    "                continue\n",
    "\n",
    "            iou = calculate_iou(gt_wake, pred_wake)\n",
    "            if iou > best_iou_for_pred:\n",
    "                best_iou_for_pred = iou\n",
    "                potential_gt_idx_for_pred = gt_idx\n",
    "        \n",
    "        # If the best IoU for this predicted wake is above threshold AND it matched an unmatched GT wake\n",
    "        if best_iou_for_pred >= iou_threshold and potential_gt_idx_for_pred != -1:\n",
    "            tp += 1\n",
    "            matched_gt_indices.add(potential_gt_idx_for_pred) # Mark GT wake as matched\n",
    "        else:\n",
    "            fp += 1 # This predicted wake is a False Positive\n",
    "\n",
    "    # Calculate False Negatives: any ground truth wake that was not matched\n",
    "    fn = len(ground_truth_wakes) - len(matched_gt_indices)\n",
    "\n",
    "    total_ground_truth = len(ground_truth_wakes)\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    accuracy = tp / total_ground_truth if total_ground_truth > 0 else 0.0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    \n",
    "    # F1-Score is the harmonic mean of precision and recall\n",
    "    if (precision + recall) == 0:\n",
    "        f1_score = 0.0\n",
    "    else:\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    return tp, fp, fn, accuracy, precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0203779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wake_detection(df, file_name, save_dir=None, recon_col: str | None = \"z_m_recon\"):\n",
    "    \"\"\"\n",
    "    Plots the time series with GT (red spans) and predicted (green spans).\n",
    "    If recon_col is present in df, overlays the reconstructed signal as a line.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 6))\n",
    "\n",
    "    # Original signal\n",
    "    plt.plot(df['t_s'], df['z_m'], color='black', linewidth=0.8, label='z_m (Vertical Displacement)')\n",
    "\n",
    "    # --- Ground truth spans (red) ---\n",
    "    in_gt_wake = False\n",
    "    gt_wake_start = None\n",
    "    gt_label_added = False\n",
    "    for _, row in df.iterrows():\n",
    "        if row['wake_label'] == 1 and not in_gt_wake:\n",
    "            gt_wake_start = row['t_s']; in_gt_wake = True\n",
    "        elif row['wake_label'] == 0 and in_gt_wake:\n",
    "            plt.axvspan(gt_wake_start, row['t_s'], color='red', alpha=0.3,\n",
    "                        label='Ground Truth Wake' if not gt_label_added else \"\")\n",
    "            gt_label_added = True; in_gt_wake = False\n",
    "    if in_gt_wake:\n",
    "        plt.axvspan(gt_wake_start, df['t_s'].iloc[-1], color='red', alpha=0.3,\n",
    "                    label='Ground Truth Wake' if not gt_label_added else \"\")\n",
    "\n",
    "    # --- Predicted spans (green) ---\n",
    "    in_pred_wake = False\n",
    "    pred_wake_start = None\n",
    "    pred_label_added = False\n",
    "    if 'predicted_wake_label' in df.columns:\n",
    "        for _, row in df.iterrows():\n",
    "            if row['predicted_wake_label'] == 1 and not in_pred_wake:\n",
    "                pred_wake_start = row['t_s']; in_pred_wake = True\n",
    "            elif row['predicted_wake_label'] == 0 and in_pred_wake:\n",
    "                plt.axvspan(pred_wake_start, row['t_s'], color='green', alpha=0.3,\n",
    "                            label='Predicted Wake' if not pred_label_added else \"\")\n",
    "                pred_label_added = True; in_pred_wake = False\n",
    "        if in_pred_wake:\n",
    "            plt.axvspan(pred_wake_start, df['t_s'].iloc[-1], color='green', alpha=0.3,\n",
    "                        label='Predicted Wake' if not pred_label_added else \"\")\n",
    "\n",
    "    plt.title(f'Wake Detection for {file_name}', fontsize=16)\n",
    "    plt.xlabel('Time (s)'); plt.ylabel('z_m (m)')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_dir:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        plot_path = os.path.join(save_dir, f'{os.path.basename(file_name).replace(\".csv\", \"\")}_wake_plot.png')\n",
    "        plt.savefig(plot_path, dpi=300)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90ceccae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences_pytorch(df_data, window_size, stride, scaler=None, fit_scaler=True):\n",
    "    \"\"\"\n",
    "    Creates overlapping sequences (windows) from time series data ('z_m' column)\n",
    "    and returns them as NumPy arrays along with original start indices.\n",
    "    Manages StandardScaler fitting/transforming.\n",
    "    \"\"\"\n",
    "    data = df_data['z_m'].values.reshape(-1, 1) # Reshape for StandardScaler\n",
    "\n",
    "    if fit_scaler:\n",
    "        scaler = StandardScaler()\n",
    "        scaled_data = scaler.fit_transform(data)\n",
    "    else:\n",
    "        if scaler is None:\n",
    "            raise ValueError(\"Scaler must be provided if fit_scaler is False.\")\n",
    "        scaled_data = scaler.transform(data)\n",
    "\n",
    "    sequences = []\n",
    "    original_indices = [] # Stores the starting index in the original DataFrame for each sequence\n",
    "    \"\"\"\n",
    "    This list stores the starting index in the original scaled_data array for each created window.\n",
    "    This is very important later when we need to map the model's predictions (which are per-window)\n",
    "    back to the original time series data points.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Loop to create sliding windows\n",
    "    for i in range(0, len(scaled_data) - window_size + 1, stride):\n",
    "        sequences.append(scaled_data[i : i + window_size])\n",
    "        original_indices.append(i) # Record the start index of this window\n",
    "    \n",
    "    return np.array(sequences), np.array(original_indices), scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e59d145b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wake_labels_for_windows(df, starts, window_size):\n",
    "    y = []\n",
    "    w = df[\"wake_label\"].values\n",
    "    for s in starts:\n",
    "        y.append(int(w[s:s+window_size].max() == 1))\n",
    "    return np.array(y, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7ad1317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_window_dataset(split_files, window_size, stride, scaler, fit_scaler=False):\n",
    "    X, y, trips = [], [], []\n",
    "    for fp in split_files:\n",
    "        df = load_and_preprocess_data(fp)\n",
    "        if df is None or len(df)<window_size: \n",
    "            continue\n",
    "        seqs, starts, scaler = create_sequences_pytorch(df, window_size, stride, scaler, fit_scaler)\n",
    "        labels = get_wake_labels_for_windows(df, starts, window_size)\n",
    "        X.append(seqs[:,:,0])    # (N,L)\n",
    "        y.append(labels)\n",
    "        for s in starts:\n",
    "            trips.append((fp, int(s), int(s+window_size)))\n",
    "    if not X:\n",
    "        return None, None, None, scaler\n",
    "    X = np.concatenate(X, axis=0)[:,None,:]   # (N,1,L)\n",
    "    y = np.concatenate(y, axis=0).astype(np.float32)\n",
    "    return X, y, trips, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2424ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1DWake(nn.Module):\n",
    "    def __init__(self, in_ch=1, hid=64, blocks=3, k=7, pdrop=0.2):\n",
    "        super().__init__()\n",
    "        layers, ch = [], in_ch\n",
    "        for _ in range(blocks):\n",
    "            layers += [\n",
    "                nn.Conv1d(ch, hid, kernel_size=k, padding=k//2),\n",
    "                nn.BatchNorm1d(hid),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(pdrop),\n",
    "            ]\n",
    "            ch = hid\n",
    "        self.backbone = nn.Sequential(*layers)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1), nn.Flatten(), nn.Linear(hid, 1)\n",
    "        )\n",
    "    def forward(self, x):             # x: (B,1,L)\n",
    "        h = self.backbone(x)\n",
    "        logit = self.head(h).squeeze(1)  # (B,)\n",
    "        return logit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5340b5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW model: LSTM classifier\n",
    "class LSTMWake(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden=64, num_layers=2, bidir=True, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidir,\n",
    "            dropout=dropout if num_layers > 1 else 0.0\n",
    "        )\n",
    "        out_dim = hidden * (2 if bidir else 1)\n",
    "        self.head = nn.Linear(out_dim, 1)\n",
    "\n",
    "    def forward(self, x):        # x: (B, 1, L)\n",
    "        x = x.transpose(1, 2)    # (B, L, 1)  <-- **key change vs CNN**\n",
    "        h, _ = self.lstm(x)      # (B, L, H*D)\n",
    "        h_mean = h.mean(dim=1)   # mean-pool over time\n",
    "        logit = self.head(h_mean).squeeze(1)  # (B,)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5a28f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== post-processing: windows -> per-sample mask -> intervals ====\n",
    "def probs_to_intervals_for_file(df, idx_triplets, probs, thr, file_path):\n",
    "    # paint positives to per-sample mask\n",
    "    n = len(df)\n",
    "    mask = np.zeros(n, dtype=np.int8)\n",
    "    for (fp, s, e), p in zip(idx_triplets, probs):\n",
    "        if fp != file_path: continue\n",
    "        if p >= thr:\n",
    "            e = min(e, n)\n",
    "            mask[s:e] = 1\n",
    "\n",
    "    # mask -> raw intervals (in seconds)\n",
    "    t_s = df[\"t_s\"].to_numpy()\n",
    "    z = mask.astype(np.int8)\n",
    "    dz = np.diff(np.pad(z, (1,1)))\n",
    "    starts = np.where(dz == 1)[0]\n",
    "    ends   = np.where(dz == -1)[0]\n",
    "    raw = [(float(t_s[s]), float(t_s[e-1])) for s, e in zip(starts, ends)]\n",
    "\n",
    "    # merge & filter\n",
    "    if not raw: \n",
    "        return [], mask\n",
    "    raw.sort()\n",
    "    out = []\n",
    "    cs, ce = raw[0]\n",
    "    for s2, e2 in raw[1:]:\n",
    "        if (s2 - ce) <= MERGE_GAP_SECONDS:\n",
    "            ce = max(ce, e2)\n",
    "        else:\n",
    "            if (ce - cs) >= MIN_WAKE_DURATION_SECONDS:\n",
    "                out.append((cs, ce))\n",
    "            cs, ce = s2, e2\n",
    "    if (ce - cs) >= MIN_WAKE_DURATION_SECONDS:\n",
    "        out.append((cs, ce))\n",
    "    return out, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d578d472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ________________________ main _________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "602cfa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Scan dataset splits (DEFINE BEFORE USING!) =====\n",
    "dataset_splits = {'train': [], 'valid': [], 'test': []}\n",
    "for split in dataset_splits.keys():\n",
    "    split_path = os.path.join(base_data_dir, split)\n",
    "    if os.path.exists(split_path):\n",
    "        dataset_splits[split] = glob.glob(os.path.join(split_path, \"*.csv\"))\n",
    "    else:\n",
    "        print(f\"Warning: '{split_path}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d10bb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== build datasets (fit scaler on train only) ====\n",
    "data_scaler = None\n",
    "X_tr, y_tr, idx_tr, data_scaler = build_window_dataset(\n",
    "    dataset_splits[\"train\"], WINDOW_SIZE, STRIDE, scaler=data_scaler, fit_scaler=True\n",
    ")\n",
    "X_va, y_va, idx_va, _ = build_window_dataset(\n",
    "    dataset_splits[\"valid\"], WINDOW_SIZE, STRIDE, scaler=data_scaler, fit_scaler=False\n",
    ")\n",
    "X_te, y_te, idx_te, _ = build_window_dataset(\n",
    "    dataset_splits[\"test\"], WINDOW_SIZE, STRIDE, scaler=data_scaler, fit_scaler=False\n",
    ")\n",
    "\n",
    "if X_tr is None: raise RuntimeError(\"No training windows found.\")\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(torch.from_numpy(X_tr).float(), torch.from_numpy(y_tr).float()),\n",
    "                          batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "val_loader   = DataLoader(TensorDataset(torch.from_numpy(X_va).float(), torch.from_numpy(y_va).float()),\n",
    "                          batch_size=BATCH_SIZE, shuffle=False, drop_last=False) if X_va is not None else None\n",
    "test_loader  = DataLoader(TensorDataset(torch.from_numpy(X_te).float(),),\n",
    "                          batch_size=BATCH_SIZE, shuffle=False, drop_last=False) if X_te is not None else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "30efb8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files scanned: 19441\n",
      "Min length: 309\n",
      "Max length: 5800\n",
      "Mean length: 2977.5\n",
      "Most common lengths (top 10):\n",
      "  2937: 7933 files\n",
      "  2799: 2672 files\n",
      "  2800: 1621 files\n",
      "  2993: 1253 files\n",
      "  2994: 690 files\n",
      "  5799: 674 files\n",
      "  2795: 598 files\n",
      "  2794: 469 files\n",
      "  2796: 445 files\n",
      "  2992: 414 files\n"
     ]
    }
   ],
   "source": [
    "import glob, os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "def lengths_report(root=\"processed_ts\", pattern=\"*.csv\"):\n",
    "    files = []\n",
    "    for split in (\"train\",\"valid\",\"test\"):\n",
    "        d = os.path.join(root, split)\n",
    "        if os.path.isdir(d):\n",
    "            files += glob.glob(os.path.join(d, pattern))\n",
    "    lens = []\n",
    "    for fp in files:\n",
    "        try:\n",
    "            n = len(pd.read_csv(fp))\n",
    "            lens.append(n)\n",
    "        except Exception as e:\n",
    "            print(\"Error reading\", fp, \"->\", e)\n",
    "\n",
    "    if not lens:\n",
    "        print(\"No files found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Files scanned: {len(lens)}\")\n",
    "    print(f\"Min length: {min(lens)}\")\n",
    "    print(f\"Max length: {max(lens)}\")\n",
    "    print(f\"Mean length: {sum(lens)/len(lens):.1f}\")\n",
    "    # Show top 10 most common lengths\n",
    "    common = Counter(lens).most_common(10)\n",
    "    print(\"Most common lengths (top 10):\")\n",
    "    for L, c in common:\n",
    "        print(f\"  {L}: {c} files\")\n",
    "\n",
    "lengths_report(\"processed_ts\", \"*.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53607df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== loss (imbalanced) ====\n",
    "pos = max(1, int((y_tr == 1).sum()))\n",
    "neg = max(1, int((y_tr == 0).sum()))\n",
    "pos_weight = torch.tensor([neg/pos], device=device, dtype=torch.float32)  # >1 if positives are rare\n",
    "crit = nn.BCEWithLogitsLoss(pos_weight=pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c5eef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== train ====\n",
    "# model = CNN1DWake().to(device)\n",
    "model = LSTMWake(input_size=1, hidden=64, num_layers=2, bidir=True, dropout=0.2).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "def predict_probs(loader):\n",
    "    model.eval()\n",
    "    outs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            xb = batch[0].to(device)\n",
    "            p = torch.sigmoid(model(xb)).cpu().numpy()\n",
    "            outs.append(p)\n",
    "    return np.concatenate(outs) if outs else np.array([])\n",
    "\n",
    "def window_metrics(y_true, probs, thr=0.5):\n",
    "    preds = (probs >= thr).astype(np.int32)\n",
    "    TP = int(((preds==1) & (y_true==1)).sum())\n",
    "    FP = int(((preds==1) & (y_true==0)).sum())\n",
    "    FN = int(((preds==0) & (y_true==1)).sum())\n",
    "    TN = int(((preds==0) & (y_true==0)).sum())\n",
    "    prec = TP/(TP+FP) if (TP+FP)>0 else 0.0\n",
    "    rec  = TP/(TP+FN) if (TP+FN)>0 else 0.0\n",
    "    f1   = 2*prec*rec/(prec+rec) if (prec+rec)>0 else 0.0\n",
    "    loss = None\n",
    "    return {\"TP\":TP,\"FP\":FP,\"FN\":FN,\"TN\":TN,\"precision\":prec,\"recall\":rec,\"f1\":f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb0933b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train_loss=0.6039 | val_loss=1.2974 | val_F1@0.5=0.399\n",
      "Epoch 02 | train_loss=0.6195 | val_loss=1.2938 | val_F1@0.5=0.422\n",
      "Epoch 03 | train_loss=0.5940 | val_loss=1.3920 | val_F1@0.5=0.358\n",
      "Epoch 04 | train_loss=0.5850 | val_loss=1.2593 | val_F1@0.5=0.433\n",
      "Epoch 05 | train_loss=0.5409 | val_loss=1.3657 | val_F1@0.5=0.428\n",
      "Epoch 06 | train_loss=0.5299 | val_loss=1.2839 | val_F1@0.5=0.478\n",
      "Epoch 07 | train_loss=0.5265 | val_loss=1.2590 | val_F1@0.5=0.458\n",
      "Epoch 08 | train_loss=0.5207 | val_loss=1.3452 | val_F1@0.5=0.400\n",
      "Epoch 09 | train_loss=0.5993 | val_loss=1.3908 | val_F1@0.5=0.345\n",
      "Epoch 10 | train_loss=0.6257 | val_loss=1.2604 | val_F1@0.5=0.381\n",
      "Epoch 11 | train_loss=0.5917 | val_loss=1.1890 | val_F1@0.5=0.460\n",
      "Epoch 12 | train_loss=0.5677 | val_loss=1.1702 | val_F1@0.5=0.467\n",
      "Epoch 13 | train_loss=0.6373 | val_loss=1.2549 | val_F1@0.5=0.418\n",
      "Epoch 14 | train_loss=0.5902 | val_loss=1.1252 | val_F1@0.5=0.499\n",
      "Epoch 15 | train_loss=0.5632 | val_loss=1.2808 | val_F1@0.5=0.381\n",
      "Epoch 16 | train_loss=0.5461 | val_loss=1.2748 | val_F1@0.5=0.400\n",
      "Epoch 17 | train_loss=0.5845 | val_loss=1.3575 | val_F1@0.5=0.444\n",
      "Epoch 18 | train_loss=0.5535 | val_loss=1.3003 | val_F1@0.5=0.413\n",
      "Epoch 19 | train_loss=0.5326 | val_loss=1.2984 | val_F1@0.5=0.422\n",
      "Epoch 20 | train_loss=0.5242 | val_loss=1.2203 | val_F1@0.5=0.404\n",
      "Epoch 21 | train_loss=0.5180 | val_loss=1.2745 | val_F1@0.5=0.402\n",
      "Epoch 22 | train_loss=0.5132 | val_loss=1.1200 | val_F1@0.5=0.456\n",
      "Epoch 23 | train_loss=0.5122 | val_loss=1.4223 | val_F1@0.5=0.397\n",
      "Epoch 24 | train_loss=0.5355 | val_loss=1.1506 | val_F1@0.5=0.480\n",
      "Epoch 25 | train_loss=0.6428 | val_loss=1.5166 | val_F1@0.5=0.322\n",
      "Epoch 26 | train_loss=0.6469 | val_loss=1.5224 | val_F1@0.5=0.310\n",
      "Epoch 27 | train_loss=0.6246 | val_loss=1.2557 | val_F1@0.5=0.471\n",
      "Epoch 28 | train_loss=0.6036 | val_loss=1.2056 | val_F1@0.5=0.474\n",
      "Epoch 29 | train_loss=0.5771 | val_loss=1.0404 | val_F1@0.5=0.489\n",
      "Epoch 30 | train_loss=0.6039 | val_loss=1.3307 | val_F1@0.5=0.340\n",
      "Epoch 31 | train_loss=0.6051 | val_loss=1.1730 | val_F1@0.5=0.459\n",
      "Epoch 32 | train_loss=0.5825 | val_loss=1.1738 | val_F1@0.5=0.460\n",
      "Epoch 33 | train_loss=0.5634 | val_loss=1.3412 | val_F1@0.5=0.399\n",
      "Epoch 34 | train_loss=0.5728 | val_loss=1.2495 | val_F1@0.5=0.413\n",
      "Epoch 35 | train_loss=0.6221 | val_loss=1.3496 | val_F1@0.5=0.341\n",
      "Epoch 36 | train_loss=0.6259 | val_loss=1.1938 | val_F1@0.5=0.441\n",
      "Epoch 37 | train_loss=0.5993 | val_loss=1.1442 | val_F1@0.5=0.461\n",
      "Epoch 38 | train_loss=0.6026 | val_loss=1.3884 | val_F1@0.5=0.323\n",
      "Epoch 39 | train_loss=0.6441 | val_loss=1.3020 | val_F1@0.5=0.408\n",
      "Epoch 40 | train_loss=0.6320 | val_loss=1.2486 | val_F1@0.5=0.418\n",
      "Epoch 41 | train_loss=0.6181 | val_loss=1.4483 | val_F1@0.5=0.335\n",
      "Epoch 42 | train_loss=0.6462 | val_loss=1.2797 | val_F1@0.5=0.421\n",
      "Epoch 43 | train_loss=0.6238 | val_loss=1.3207 | val_F1@0.5=0.439\n",
      "Epoch 44 | train_loss=0.6055 | val_loss=1.2838 | val_F1@0.5=0.439\n",
      "Early stop @ epoch 44. Best val_loss=1.0404\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best_state, best_val, stale = None, float(\"inf\"), 0\n",
    "for epoch in range(EPOCHS):\n",
    "    # train\n",
    "    model.train()\n",
    "    run, seen = 0.0, 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss = crit(logits, yb)\n",
    "        opt.zero_grad(set_to_none=True); loss.backward(); opt.step()\n",
    "        run += loss.item() * xb.size(0); seen += xb.size(0)\n",
    "    train_loss = run / max(1, seen)\n",
    "\n",
    "    # validate\n",
    "    if val_loader is not None:\n",
    "        # val loss\n",
    "        model.eval()\n",
    "        vrun, vseen = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                vrun += crit(model(xb), yb).item() * xb.size(0)\n",
    "                vseen += xb.size(0)\n",
    "        val_loss = vrun / max(1, vseen)\n",
    "\n",
    "        # window-level F1 @ 0.5\n",
    "        probs_va = predict_probs(val_loader)\n",
    "        wm = window_metrics(y_va, probs_va, thr=DECISION_THR)\n",
    "        print(f\"Epoch {epoch+1:02d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_F1@0.5={wm['f1']:.3f}\")\n",
    "\n",
    "        # early stopping on val_loss\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss; best_state = copy.deepcopy(model.state_dict()); stale = 0\n",
    "        else:\n",
    "            stale += 1\n",
    "            if stale >= EARLY_STOP_PATIENCE:\n",
    "                print(f\"Early stop @ epoch {epoch+1}. Best val_loss={best_val:.4f}\")\n",
    "                break\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1:02d} | train_loss={train_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04d87dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore best\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4b5fc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Window-level @0.5: P=0.294 R=0.402 F1=0.340 (TP=274964 FP=659129 FN=408370 TN=2137946)\n"
     ]
    }
   ],
   "source": [
    "# ==== TEST: window-level ====\n",
    "if test_loader is not None:\n",
    "    probs_te = predict_probs(test_loader)           # per-window probs, in the same order as X_te/y_te/idx_te\n",
    "    wm_te = window_metrics(y_te, probs_te, thr=DECISION_THR)\n",
    "    print(f\"[TEST] Window-level @0.5: P={wm_te['precision']:.3f} R={wm_te['recall']:.3f} F1={wm_te['f1']:.3f} \"\n",
    "          f\"(TP={wm_te['TP']} FP={wm_te['FP']} FN={wm_te['FN']} TN={wm_te['TN']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3bb41b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Event-level IoU>=0.5: P=0.142 R=0.510 F1=0.223 (TP=1707 FP=10290 FN=1638)\n",
      "Saved 10 plots to: detected_wakes_CLS/wake_plots/LSTM/w150,s5\n"
     ]
    }
   ],
   "source": [
    "# ==== TEST: event-level (aggregate windows -> intervals) + plots ====\n",
    "if X_te is not None:\n",
    "    # group test windows by file\n",
    "    by_file = {}\n",
    "    for trip, p in zip(idx_te, probs_te):\n",
    "        by_file.setdefault(trip[0], []).append((trip, p))\n",
    "\n",
    "    agg_tp = agg_fp = agg_fn = 0\n",
    "    plotted_files = []\n",
    "\n",
    "    for fp, items in by_file.items():\n",
    "        df = load_and_preprocess_data(fp)\n",
    "        gt = get_ground_truth_wakes(df)\n",
    "        trips = [t for t,_ in items]\n",
    "        pvals = [p for _,p in items]\n",
    "\n",
    "        pred_intervals, mask = probs_to_intervals_for_file(\n",
    "            df, trips, pvals, DECISION_THR, fp\n",
    "        )\n",
    "        # add predicted_wake_label to df (for plots)\n",
    "        df_pred = df.copy()\n",
    "        df_pred[\"predicted_wake_label\"] = mask\n",
    "\n",
    "        # event metrics for this file\n",
    "        tp, fp_, fn, acc, prec, rec, f1 = evaluate_detection(gt, pred_intervals, iou_threshold=0.5)\n",
    "        agg_tp += tp; agg_fp += fp_; agg_fn += fn\n",
    "\n",
    "        # save a CSV with predictions if you want\n",
    "        rel = os.path.relpath(fp, base_data_dir)\n",
    "        save_csv = os.path.join(output_base_dir_ml, \"test\", os.path.basename(rel))\n",
    "        os.makedirs(os.path.dirname(save_csv), exist_ok=True)\n",
    "        df_pred.to_csv(save_csv, index=False)\n",
    "\n",
    "        # collect for random plotting later\n",
    "        plotted_files.append((fp, df_pred))\n",
    "\n",
    "    # aggregate event-level\n",
    "    P = agg_tp/(agg_tp+agg_fp) if (agg_tp+agg_fp)>0 else 0.0\n",
    "    R = agg_tp/(agg_tp+agg_fn) if (agg_tp+agg_fn)>0 else 0.0\n",
    "    F1 = 2*P*R/(P+R) if (P+R)>0 else 0.0\n",
    "    print(f\"[TEST] Event-level IoU>=0.5: P={P:.3f} R={R:.3f} F1={F1:.3f} (TP={agg_tp} FP={agg_fp} FN={agg_fn})\")\n",
    "\n",
    "    # plot 10 random test files (original in red, predicted in green)\n",
    "    k = min(10, len(plotted_files))\n",
    "    for (fp, dfp) in random.sample(plotted_files, k):\n",
    "        plot_wake_detection(dfp, os.path.basename(fp), save_dir=plots_output_dir_ml)\n",
    "\n",
    "    print(f\"Saved {k} plots to: {plots_output_dir_ml}\")\n",
    "else:\n",
    "    print(\"No TEST set found; skipping event-level eval and plots.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32f2e6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "################################\n",
    "######################\n",
    "###########\n",
    "###\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68551c72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
