{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5321413a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (2025.7.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.2.0 (from torch)\n",
      "  Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl (766.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m  \u001b[33m0:00:11\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m155.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m144.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m111.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m  \u001b[33m0:00:07\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-cusparselt-cu12, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "\u001b[2K  Attempting uninstall: sympy━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/16\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K    Found existing installation: sympy 1.14.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/16\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K    Uninstalling sympy-1.14.0:0m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/16\u001b[0m [sympy]parselt-cu12]\n",
      "\u001b[2K      Successfully uninstalled sympy-1.14.0━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/16\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/16\u001b[0m [torch]m15/16\u001b[0m [torch]-cusolver-cu12]2]2]\n",
      "\u001b[1A\u001b[2KSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.6.0 triton-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "605965b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# ============================================================\n",
    "# Many-to-Many LSTM for Wake Detection (windowed, vote-merge)\n",
    "# - Windows: L=150, stride=5\n",
    "# - Per-timestep labels & loss\n",
    "# - Reconstruct full-length probabilities by averaging votes\n",
    "# - Event-level IoU evaluation (TP/FP/FN/Precision/Recall/F1)\n",
    "# - Plots: red = ground-truth wake, green = predicted wake\n",
    "# ============================================================\n",
    "\n",
    "import os, glob, random, math, json\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "365ba9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[INFO] Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8b3e0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DATA_DIR = \"processed_ts\"   # expects train/valid/test/*.csv\n",
    "WINDOW_SIZE   = 300\n",
    "STRIDE        = 5\n",
    "TAIL_MIN_KEEP = WINDOW_SIZE // 2   # if tail >= 75, pad to full; else drop\n",
    "USE_TAIL      = True               # enable tail handling\n",
    "\n",
    "LR            = 1e-3\n",
    "BATCH_SIZE    = 256\n",
    "EPOCHS        = 30\n",
    "POS_WEIGHT    = 1.5                # increase to bias recall\n",
    "WEIGHT_DECAY  = 1e-4\n",
    "GRAD_CLIP     = 1.0\n",
    "EARLY_STOP    = 8                  # patience (epochs)\n",
    "THR_MASK      = 0.5                # per-sample probability threshold for mask\n",
    "IOU_EVENT_THR = 0.5                # IoU threshold for event matching\n",
    "MERGE_GAP_S   = 6.0                # seconds: merge predicted intervals if gap <= this\n",
    "MIN_DUR_S     = 0.5                # seconds: drop predicted intervals shorter than this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c06f190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 2) Data loading & helpers\n",
    "# -------------------------\n",
    "def scan_splits(root):\n",
    "    splits = {}\n",
    "    for split in (\"train\",\"valid\",\"test\"):\n",
    "        d = os.path.join(root, split)\n",
    "        files = glob.glob(os.path.join(d, \"*.csv\")) if os.path.isdir(d) else []\n",
    "        files.sort()\n",
    "        splits[split] = files\n",
    "    return splits\n",
    "\n",
    "def load_df(fp):\n",
    "    df = pd.read_csv(fp)\n",
    "    df = df.sort_values(\"t_s\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def build_windows_many2many(df, window_size, stride, scaler=None, fit_scaler=False,\n",
    "                            use_tail=True, tail_min_keep=0):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      X: (N,1,L) float32 scaled z_m\n",
    "      Y: (N,L)   float32 in {0,1}\n",
    "      M: (N,L)   float32 mask (1 = valid, 0 = padded)\n",
    "      starts: list of start indices in original series\n",
    "    \"\"\"\n",
    "    z = df[\"z_m\"].to_numpy(dtype=np.float32).reshape(-1,1)\n",
    "    y = df[\"wake_label\"].to_numpy(dtype=np.int64)\n",
    "    N = len(z)\n",
    "\n",
    "    # scale\n",
    "    if fit_scaler:\n",
    "        scaler = StandardScaler()\n",
    "        z_scaled = scaler.fit_transform(z)\n",
    "    else:\n",
    "        z_scaled = scaler.transform(z)\n",
    "\n",
    "    Xs, Ys, Ms, starts = [], [], [], []\n",
    "    i = 0\n",
    "    while i + window_size <= N:\n",
    "        Xs.append(z_scaled[i:i+window_size])          # (L,1)\n",
    "        Ys.append(y[i:i+window_size])                 # (L,)\n",
    "        Ms.append(np.ones(window_size, dtype=np.float32))\n",
    "        starts.append(i)\n",
    "        i += stride\n",
    "\n",
    "    if use_tail and i < N:\n",
    "        tail = N - i\n",
    "        if tail >= tail_min_keep:\n",
    "            x_tail = z_scaled[i:N]\n",
    "            y_tail = y[i:N]\n",
    "            pad_needed = window_size - tail\n",
    "            pad_x = np.repeat(x_tail[-1:], pad_needed, axis=0)\n",
    "            pad_y = np.zeros(pad_needed, dtype=np.int64)\n",
    "            mask = np.concatenate([np.ones(tail, dtype=np.float32),\n",
    "                                   np.zeros(pad_needed, dtype=np.float32)])\n",
    "            Xs.append(np.vstack([x_tail, pad_x]))\n",
    "            Ys.append(np.concatenate([y_tail, pad_y]))\n",
    "            Ms.append(mask)\n",
    "            starts.append(i)\n",
    "        # else: drop tail\n",
    "\n",
    "    if not Xs:\n",
    "        return None, None, None, [], scaler\n",
    "\n",
    "    X = np.stack(Xs, axis=0).transpose(0,2,1).astype(np.float32)  # (N,1,L)\n",
    "    Y = np.stack(Ys, axis=0).astype(np.float32)                    # (N,L)\n",
    "    M = np.stack(Ms, axis=0).astype(np.float32)                    # (N,L)\n",
    "    return X, Y, M, starts, scaler\n",
    "\n",
    "def streaming_mean_std_over_train(train_files):\n",
    "    \"\"\"One-pass mean/std of z_m over TRAIN files (Welford).\"\"\"\n",
    "    n_total = 0\n",
    "    mean = 0.0\n",
    "    M2 = 0.0\n",
    "    for fp in train_files:\n",
    "        df = load_df(fp)\n",
    "        z = df[\"z_m\"].to_numpy(dtype=np.float64)\n",
    "        for x in z:\n",
    "            n_total += 1\n",
    "            delta = x - mean\n",
    "            mean += delta / n_total\n",
    "            M2 += delta * (x - mean)\n",
    "    if n_total < 2:\n",
    "        return float(mean), 1.0\n",
    "    var = M2 / (n_total - 1)\n",
    "    std = sqrt(max(var, 1e-12))\n",
    "    return float(mean), float(std)\n",
    "\n",
    "def build_window_index(files, window_size, stride, use_tail=True, tail_min_keep=0):\n",
    "    \"\"\"\n",
    "    Returns list of (file_path, start, end, valid_len).\n",
    "    If a tail window is included, valid_len < window_size.\n",
    "    \"\"\"\n",
    "    index = []\n",
    "    for fp in files:\n",
    "        df = load_df(fp)\n",
    "        N = len(df)\n",
    "        i = 0\n",
    "        while i + window_size <= N:\n",
    "            index.append((fp, i, i + window_size, window_size))\n",
    "            i += stride\n",
    "        if use_tail and i < N:\n",
    "            tail = N - i\n",
    "            if tail >= tail_min_keep:\n",
    "                index.append((fp, i, N, tail))\n",
    "    return index\n",
    "\n",
    "class LazyWindowDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Loads one window on demand; scales with global (train) mean/std.\n",
    "    Returns: x (1,L), y (L), m (L), fp, s, e\n",
    "    \"\"\"\n",
    "    def __init__(self, index, mean, std, window_size):\n",
    "        self.index = index\n",
    "        self.mean = float(mean)\n",
    "        self.std = float(std if std > 0 else 1.0)\n",
    "        self.window_size = int(window_size)\n",
    "\n",
    "    def __len__(self): return len(self.index)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        fp, s, e, valid_len = self.index[i]\n",
    "        df = load_df(fp)\n",
    "        z = df[\"z_m\"].to_numpy(dtype=np.float32)\n",
    "        y = df[\"wake_label\"].to_numpy(dtype=np.float32)\n",
    "        seg_z = z[s:e]\n",
    "        seg_y = y[s:e]\n",
    "        L = len(seg_z)\n",
    "        if L < self.window_size:\n",
    "            pad = self.window_size - L\n",
    "            seg_z = np.concatenate([seg_z, np.repeat(seg_z[-1], pad).astype(np.float32)])\n",
    "            seg_y = np.concatenate([seg_y, np.zeros(pad, dtype=np.float32)])\n",
    "        m = np.zeros(self.window_size, dtype=np.float32); m[:valid_len] = 1.0\n",
    "        seg_z = (seg_z - self.mean) / self.std\n",
    "        x = torch.from_numpy(seg_z[None, :]).float()\n",
    "        y = torch.from_numpy(seg_y).float()\n",
    "        m = torch.from_numpy(m).float()\n",
    "        return x, y, m, fp, s, e\n",
    "\n",
    "\n",
    "def build_dataset(split_files, window_size, stride, scaler=None, fit_scaler=False,\n",
    "                  use_tail=True, tail_min_keep=0):\n",
    "    X_list, Y_list, M_list, idx_triplets = [], [], [], []\n",
    "    for fp in split_files:\n",
    "        df = load_df(fp)\n",
    "        res = build_windows_many2many(df, window_size, stride, scaler, fit_scaler,\n",
    "                                      use_tail, tail_min_keep)\n",
    "        X, Y, M, starts, scaler = res\n",
    "        if X is None: \n",
    "            continue\n",
    "        X_list.append(X)\n",
    "        Y_list.append(Y)\n",
    "        M_list.append(M)\n",
    "        for s in starts:\n",
    "            e = min(s + window_size, len(df))\n",
    "            idx_triplets.append((fp, int(s), int(e)))  # map window back to file\n",
    "    if not X_list:\n",
    "        return None, None, None, [], scaler\n",
    "    X = np.concatenate(X_list, axis=0)\n",
    "    Y = np.concatenate(Y_list, axis=0)\n",
    "    M = np.concatenate(M_list, axis=0)\n",
    "    return X, Y, M, idx_triplets, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89d99d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 3) Model (per-timestep output)\n",
    "# -------------------------\n",
    "class LSTMWakeSeq(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden=96, num_layers=2, bidir=True, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size, hidden_size=hidden, num_layers=num_layers,\n",
    "            batch_first=True, bidirectional=bidir, dropout=dropout if num_layers>1 else 0.0\n",
    "        )\n",
    "        out_dim = hidden * (2 if bidir else 1)\n",
    "        self.head = nn.Linear(out_dim, 1)\n",
    "\n",
    "    def forward(self, x):        # x: (B,1,L)\n",
    "        x = x.transpose(1,2)     # (B,L,1)\n",
    "        h, _ = self.lstm(x)      # (B,L,H*)\n",
    "        logits = self.head(h).squeeze(-1)   # (B,L)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edbf5a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 4) Training/Eval utilities\n",
    "# -------------------------\n",
    "@torch.no_grad()\n",
    "def compute_pos_weight_from_loader(loader):\n",
    "    pos = 0.0; neg = 0.0\n",
    "    for xb, yb, mb, *_ in loader:\n",
    "        y = yb.numpy(); m = mb.numpy()\n",
    "        y = y[m > 0.5]\n",
    "        pos += (y > 0.5).sum()\n",
    "        neg += (y <= 0.5).sum()\n",
    "    if pos == 0: return 1.0\n",
    "    return max(1.0, float(neg / pos))\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, grad_clip=1.0):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for xb, yb, mb, *_ in loader:\n",
    "        xb = xb.to(device)              # (B,1,L)\n",
    "        yb = yb.to(device)              # (B,L)\n",
    "        mb = mb.to(device)              # (B,L)\n",
    "        logits = model(xb)              # (B,L)\n",
    "        loss_mat = criterion(logits, yb)    # (B,L)\n",
    "        loss = (loss_mat * mb).sum() / (mb.sum() + 1e-8)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "    return float(np.mean(losses))\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer_probs(model, loader):\n",
    "    \"\"\"Forward pass to collect per-timestep probabilities and (fp, s, e) mapping.\"\"\"\n",
    "    model.eval()\n",
    "    probs_all = []\n",
    "    idx_triplets = []  # (fp, s, e) per window, aligned with probs_all rows\n",
    "    for xb, yb, mb, fp, s, e in loader:\n",
    "        xb = xb.to(device)\n",
    "        p = torch.sigmoid(model(xb)).cpu().numpy()   # (B,L)\n",
    "        probs_all.append(p)\n",
    "        for i in range(len(fp)):\n",
    "            idx_triplets.append((fp[i], int(s[i]), int(e[i])))\n",
    "    return np.concatenate(probs_all, axis=0), idx_triplets\n",
    "\n",
    "\n",
    "def mask_to_intervals(mask, t_s):\n",
    "    \"\"\"Binary per-sample mask -> list of (start_time, end_time) seconds (closed-open).\"\"\"\n",
    "    z = mask.astype(np.int8)\n",
    "    dz = np.diff(np.pad(z, (1,1)))\n",
    "    starts = np.where(dz == 1)[0]\n",
    "    ends   = np.where(dz == -1)[0]\n",
    "    return [(float(t_s[s]), float(t_s[e-1])) for s, e in zip(starts, ends)]\n",
    "\n",
    "def merge_and_filter(intervals, merge_gap_s=MERGE_GAP_S, min_dur_s=MIN_DUR_S):\n",
    "    if not intervals: \n",
    "        return []\n",
    "    intervals = sorted(intervals)\n",
    "    out = []\n",
    "    cs, ce = intervals[0]\n",
    "    for s2, e2 in intervals[1:]:\n",
    "        if (s2 - ce) <= merge_gap_s:\n",
    "            ce = max(ce, e2)\n",
    "        else:\n",
    "            if (ce - cs) >= min_dur_s:\n",
    "                out.append((cs, ce))\n",
    "            cs, ce = s2, e2\n",
    "    if (ce - cs) >= min_dur_s:\n",
    "        out.append((cs, ce))\n",
    "    return out\n",
    "\n",
    "def get_ground_truth_wakes(df):\n",
    "    w = df[\"wake_label\"].to_numpy().astype(int)\n",
    "    t = df[\"t_s\"].to_numpy()\n",
    "    return merge_and_filter(mask_to_intervals(w, t), 0.0, 0.0)  # no merge/filter on GT\n",
    "\n",
    "def iou_interval(a, b):\n",
    "    s1,e1 = a; s2,e2 = b\n",
    "    inter = max(0.0, min(e1, e2) - max(s1, s2))\n",
    "    union = max(e1,e2) - min(s1,s2)\n",
    "    return 0.0 if union <= 0 else inter/union\n",
    "\n",
    "def evaluate_events_iou(gt_intervals, pred_intervals, iou_thr=IOU_EVENT_THR):\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    matched = set()\n",
    "    for p in pred_intervals:\n",
    "        best_iou, best_idx = 0.0, -1\n",
    "        for i, g in enumerate(gt_intervals):\n",
    "            if i in matched: \n",
    "                continue\n",
    "            iou = iou_interval(g, p)\n",
    "            if iou > best_iou:\n",
    "                best_iou, best_idx = iou, i\n",
    "        if best_iou >= iou_thr and best_idx != -1:\n",
    "            tp += 1\n",
    "            matched.add(best_idx)\n",
    "        else:\n",
    "            fp += 1\n",
    "    fn = len(gt_intervals) - len(matched)\n",
    "    prec = tp/(tp+fp) if (tp+fp)>0 else 0.0\n",
    "    rec  = tp/(tp+fn) if (tp+fn)>0 else 0.0\n",
    "    f1   = 2*prec*rec/(prec+rec) if (prec+rec)>0 else 0.0\n",
    "    return tp, fp, fn, prec, rec, f1\n",
    "\n",
    "def reconstruct_probs_for_files(dataset_splits, idx_triplets, probs_windows):\n",
    "    \"\"\"\n",
    "    For each file: average-vote the per-timestep probs from overlapping windows.\n",
    "    Returns dict: fp -> prob_full (length = len(df))\n",
    "    \"\"\"\n",
    "    # Group windows by file\n",
    "    by_file = defaultdict(list)\n",
    "    for (fp, s, e), p in zip(idx_triplets, probs_windows):\n",
    "        by_file[fp].append((s, e, p))\n",
    "\n",
    "    out = {}\n",
    "    for fp, items in by_file.items():\n",
    "        df = load_df(fp)\n",
    "        N = len(df)\n",
    "        sum_probs = np.zeros(N, dtype=np.float32)\n",
    "        counts    = np.zeros(N, dtype=np.int32)\n",
    "        for s, e, pw in items:\n",
    "            L = e - s\n",
    "            sum_probs[s:e] += pw[:L]\n",
    "            counts[s:e]    += 1\n",
    "        counts[counts == 0] = 1\n",
    "        out[fp] = sum_probs / counts\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a607a029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] computing streaming mean/std over TRAIN files...\n",
      "[INFO] scaler: mean=0.000002 std=0.032041\n",
      "[IDX] train=5302646 | valid=1835017 | test=3302900\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 5) Build datasets (streaming)\n",
    "# -------------------------\n",
    "dataset_splits = scan_splits(BASE_DATA_DIR)\n",
    "train_files = dataset_splits[\"train\"]\n",
    "valid_files = dataset_splits[\"valid\"]\n",
    "test_files  = dataset_splits[\"test\"]\n",
    "\n",
    "print(\"[INFO] computing streaming mean/std over TRAIN files...\")\n",
    "z_mean, z_std = streaming_mean_std_over_train(train_files)\n",
    "print(f\"[INFO] scaler: mean={z_mean:.6f} std={z_std:.6f}\")\n",
    "\n",
    "train_index = build_window_index(train_files, WINDOW_SIZE, STRIDE, USE_TAIL, TAIL_MIN_KEEP)\n",
    "valid_index = build_window_index(valid_files, WINDOW_SIZE, STRIDE, USE_TAIL, TAIL_MIN_KEEP)\n",
    "test_index  = build_window_index(test_files,  WINDOW_SIZE, STRIDE, USE_TAIL, TAIL_MIN_KEEP)\n",
    "print(f\"[IDX] train={len(train_index)} | valid={len(valid_index)} | test={len(test_index)}\")\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(\n",
    "    LazyWindowDataset(train_index, z_mean, z_std, WINDOW_SIZE),\n",
    "    batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    LazyWindowDataset(valid_index, z_mean, z_std, WINDOW_SIZE),\n",
    "    batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True\n",
    ") if len(valid_index) else None\n",
    "test_loader = DataLoader(\n",
    "    LazyWindowDataset(test_index, z_mean, z_std, WINDOW_SIZE),\n",
    "    batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True\n",
    ") if len(test_index) else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7272ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] pos_weight used = 2.137\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 6) Train\n",
    "# -------------------------\n",
    "model = LSTMWakeSeq().to(device)\n",
    "pw_val = max(POS_WEIGHT, compute_pos_weight_from_loader(train_loader))\n",
    "pos_weight = torch.tensor(pw_val, dtype=torch.float32, device=device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight, reduction='none')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "print(f\"[INFO] pos_weight used = {pos_weight.item():.3f}\")\n",
    "\n",
    "# --- train loop with early stopping on val loss ---\n",
    "best_val = float(\"inf\")\n",
    "best_state = None\n",
    "no_imp = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss = train_one_epoch(model, train_loader, criterion, optimizer, grad_clip=GRAD_CLIP)\n",
    "    \n",
    "    if val_loader is not None:\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb, mb, *_ in val_loader:\n",
    "                xb = xb.to(device); yb = yb.to(device); mb = mb.to(device)\n",
    "                logits = model(xb)                     # (B,L)\n",
    "                loss_mat = criterion(logits, yb)       # (B,L)\n",
    "                vloss = (loss_mat * mb).sum() / (mb.sum() + 1e-8)\n",
    "                val_losses.append(vloss.item())\n",
    "        val_loss = float(np.mean(val_losses))\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch:03d} | TrainLoss {train_loss:.4f} | ValLoss {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val - 1e-6:\n",
    "            best_val = val_loss\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            no_imp = 0\n",
    "        else:\n",
    "            no_imp += 1\n",
    "            if no_imp >= EARLY_STOP:\n",
    "                print(f\"[INFO] Early stop at epoch {epoch} (best ValLoss={best_val:.4f})\")\n",
    "                break\n",
    "    else:\n",
    "        print(f\"Epoch {epoch:03d} | TrainLoss {train_loss:.4f}\")\n",
    "\n",
    "# restore best weights\n",
    "if best_state is not None:\n",
    "    model.load_state_dict({k: v.to(device) for k, v in best_state.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8403c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 7) Inference: per-window probs, reconstruction to per-file probs\n",
    "# -------------------------\n",
    "p_tr, _, _ = infer_probs(model, train_loader)\n",
    "p_va, _, _ = (infer_probs(model, val_loader) if val_loader is not None else (None,None,None))\n",
    "p_te, _, _ = (infer_probs(model, test_loader) if test_loader is not None else (None,None,None))\n",
    "\n",
    "# Reconstruct per-file probabilities (average vote)\n",
    "recon_tr = reconstruct_probs_for_files(dataset_splits, idx_tr, p_tr)\n",
    "recon_va = reconstruct_probs_for_files(dataset_splits, idx_va, p_va) if p_va is not None else {}\n",
    "recon_te = reconstruct_probs_for_files(dataset_splits, idx_te, p_te) if p_te is not None else {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5417866b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 8) Event-level IoU evaluation per split\n",
    "# -------------------------\n",
    "def eval_split(recon_dict, file_list, thr=THR_MASK):\n",
    "    total = dict(tp=0, fp=0, fn=0)\n",
    "    for fp in file_list:\n",
    "        df = load_df(fp)\n",
    "        prob = recon_dict.get(fp, None)\n",
    "        if prob is None:\n",
    "            continue\n",
    "        mask = (prob >= thr).astype(np.int8)\n",
    "\n",
    "        # post-process predicted\n",
    "        t_s = df[\"t_s\"].to_numpy()\n",
    "        pred_intervals = merge_and_filter(mask_to_intervals(mask, t_s),\n",
    "                                          merge_gap_s=MERGE_GAP_S, min_dur_s=MIN_DUR_S)\n",
    "        gt_intervals = get_ground_truth_wakes(df)\n",
    "\n",
    "        tp, fp_, fn, prec, rec, f1 = evaluate_events_iou(gt_intervals, pred_intervals, IOU_EVENT_THR)\n",
    "        total[\"tp\"] += tp\n",
    "        total[\"fp\"] += fp_\n",
    "        total[\"fn\"] += fn\n",
    "\n",
    "    tp, fp_, fn = total[\"tp\"], total[\"fp\"], total[\"fn\"]\n",
    "    prec = tp/(tp+fp_) if (tp+fp_)>0 else 0.0\n",
    "    rec  = tp/(tp+fn ) if (tp+fn )>0 else 0.0\n",
    "    f1   = 2*prec*rec/(prec+rec) if (prec+rec)>0 else 0.0\n",
    "    return dict(TP=tp, FP=fp_, FN=fn, Precision=prec, Recall=rec, F1=f1)\n",
    "\n",
    "print(\"\\n===== Event-level IoU metrics (IoU>=%.2f, thr=%.2f) =====\" % (IOU_EVENT_THR, THR_MASK))\n",
    "train_evt = eval_split(recon_tr, dataset_splits[\"train\"])\n",
    "print(\"[Train]\", train_evt)\n",
    "if val_loader is not None:\n",
    "    valid_evt = eval_split(recon_va, dataset_splits[\"valid\"])\n",
    "    print(\"[Valid]\", valid_evt)\n",
    "if test_loader is not None:\n",
    "    test_evt  = eval_split(recon_te, dataset_splits[\"test\"])\n",
    "    print(\"[Test ]\", test_evt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281ea003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 9) Plots (test set): red = GT wakes, green = Pred wakes\n",
    "# -------------------------\n",
    "def plot_file(fp, prob_full, thr=THR_MASK, nmax=3):\n",
    "    df = load_df(fp)\n",
    "    t = df[\"t_s\"].to_numpy()\n",
    "    z = df[\"z_m\"].to_numpy()\n",
    "    gt = get_ground_truth_wakes(df)\n",
    "\n",
    "    pred_mask = (prob_full >= thr).astype(np.int8)\n",
    "    pred_intv = merge_and_filter(mask_to_intervals(pred_mask, t), MERGE_GAP_S, MIN_DUR_S)\n",
    "\n",
    "    fig, ax = plt.subplots(1,1, figsize=(12,4))\n",
    "    ax.plot(t, z, lw=1.0)\n",
    "    # plot GT (red)\n",
    "    for s,e in gt:\n",
    "        ax.axvspan(s, e, color=\"red\", alpha=0.25)\n",
    "    # plot Pred (green)\n",
    "    for s,e in pred_intv:\n",
    "        ax.axvspan(s, e, color=\"green\", alpha=0.25)\n",
    "\n",
    "    ax.set_title(os.path.basename(fp))\n",
    "    ax.set_xlabel(\"time (s)\")\n",
    "    ax.set_ylabel(\"z_m\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# show a few random test files\n",
    "to_plot = random.sample(dataset_splits[\"test\"], k=min(3, len(dataset_splits[\"test\"])))\n",
    "for fp in to_plot:\n",
    "    prob = recon_te.get(fp, None)\n",
    "    if prob is not None:\n",
    "        plot_file(fp, prob, THR_MASK)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
