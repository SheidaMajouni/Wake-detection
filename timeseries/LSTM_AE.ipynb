{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d84a327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (2025.7.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.2.0 (from torch)\n",
      "  Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl (766.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m  \u001b[33m0:00:08\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m180.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m164.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m113.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m  \u001b[33m0:00:06\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m138.4 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m149.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m135.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m134.4 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m138.3 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m135.5 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m183.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m158.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-cusparselt-cu12, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "\u001b[2K  Attempting uninstall: sympy━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/16\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K    Found existing installation: sympy 1.14.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/16\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K    Uninstalling sympy-1.14.0:0m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/16\u001b[0m [sympy]parselt-cu12]\n",
      "\u001b[2K      Successfully uninstalled sympy-1.14.0━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/16\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/16\u001b[0m [torch]m15/16\u001b[0m [torch]-cusolver-cu12]2]2]\n",
      "\u001b[1A\u001b[2KSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.6.0 triton-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43e77a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tqdm.notebook import tqdm # Use tqdm.notebook for Jupyter progress bars\n",
    "\n",
    "# --- PyTorch Imports for ML Model ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51ab87d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Determine if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cc0d443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ML Model Configuration ---\n",
    "WINDOW_SIZE = 150       # Number of time steps (data points) in each input sequence/window\n",
    "STRIDE = 5             # How many steps to move the window (1 means maximum overlap, covering every point)\n",
    "HIDDEN_SIZE = 64\n",
    "LATENT_SIZE = 32\n",
    "EPOCHS = 50            # Number of training epochs for the autoencoder\n",
    "BATCH_SIZE = 64        # Batch size for training\n",
    "ANOMALY_THRESHOLD_QUANTILE = 0.70 # Quantile of reconstruction errors on NORMAL data to set anomaly threshold\n",
    "\n",
    "\n",
    "# --- Post-processing configuration (applied to ML predictions) ---\n",
    "MERGE_GAP_SECONDS = 3             # Maximum time gap between predicted wake segments to merge them\n",
    "MIN_WAKE_DURATION_SECONDS = 0.5     # Minimum duration for a detected event to be considered a wake\n",
    "\n",
    "\n",
    "# --- Global paths ---\n",
    "base_data_dir = 'processed_ts'\n",
    "output_base_dir_ml = 'detected_wakes_LSTM_AE' # New output directory for ML predictions\n",
    "plots_output_dir_ml = 'detected_wakes_LSTM_AE/wake_plots/w150,s5,th70'         # New directory for ML plots\n",
    "model_save_dir = 'detected_wakes_LSTM_AE/saved_models/w150,s5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41ebd868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads the CSV data and performs basic preprocessing.\n",
    "    Ensures 't_s' is sorted and resets index.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        df = df.sort_values(by='t_s').reset_index(drop=True)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or preprocessing data from {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b361706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ground_truth_wakes(df):\n",
    "    \"\"\"\n",
    "    Extracts ground truth wake intervals from the 'wake_label' column.\n",
    "    Returns a list of (start_time, end_time) tuples.\n",
    "    \"\"\"\n",
    "    ground_truth_wakes = []\n",
    "    in_wake = False\n",
    "    wake_start_time = None\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        t_s = row['t_s']\n",
    "        wake_label = row['wake_label']\n",
    "\n",
    "        if wake_label == 1 and not in_wake:\n",
    "            wake_start_time = t_s\n",
    "            in_wake = True\n",
    "        elif wake_label == 0 and in_wake:\n",
    "            wake_end_time = t_s\n",
    "            ground_truth_wakes.append((wake_start_time, wake_end_time))\n",
    "            in_wake = False\n",
    "    \n",
    "    # Handle case where wake extends to the end of the time series\n",
    "    if in_wake:\n",
    "        ground_truth_wakes.append((wake_start_time, df['t_s'].iloc[-1]))\n",
    "\n",
    "    return ground_truth_wakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e239500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(interval1, interval2):\n",
    "    \"\"\"\n",
    "    Calculates the Intersection over Union (IoU) of two time intervals.\n",
    "    Intervals are (start, end) tuples.\n",
    "    \"\"\"\n",
    "    start1, end1 = interval1\n",
    "    start2, end2 = interval2\n",
    "\n",
    "    # Calculate intersection duration\n",
    "    intersection_start = max(start1, start2)\n",
    "    intersection_end = min(end1, end2)\n",
    "    \n",
    "    intersection_duration = max(0, intersection_end - intersection_start)\n",
    "\n",
    "    # Calculate union duration\n",
    "    union_duration = (max(end1, end2) - min(start1, start2))\n",
    "\n",
    "    if union_duration == 0:\n",
    "        return 0.0 # No union means no overlap possible\n",
    "    \n",
    "    return intersection_duration / union_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd08df25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_detection(ground_truth_wakes, predicted_wakes, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluates the detection performance based on IoU.\n",
    "    Returns TP, FP, FN, Accuracy, Precision, Recall, F1-Score.\n",
    "    \"\"\"\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    # Sets to keep track of matched ground truth and used predicted wakes\n",
    "    matched_gt_indices = set()\n",
    "    used_pred_indices = set()\n",
    "\n",
    "    # Iterate through predicted wakes to find the best ground truth match for each\n",
    "    for pred_idx, pred_wake in enumerate(predicted_wakes):\n",
    "        best_iou_for_pred = 0.0\n",
    "        potential_gt_idx_for_pred = -1\n",
    "\n",
    "        for gt_idx, gt_wake in enumerate(ground_truth_wakes):\n",
    "            if gt_idx in matched_gt_indices: # If this GT wake is already matched, skip it\n",
    "                continue\n",
    "\n",
    "            iou = calculate_iou(gt_wake, pred_wake)\n",
    "            if iou > best_iou_for_pred:\n",
    "                best_iou_for_pred = iou\n",
    "                potential_gt_idx_for_pred = gt_idx\n",
    "        \n",
    "        # If the best IoU for this predicted wake is above threshold AND it matched an unmatched GT wake\n",
    "        if best_iou_for_pred >= iou_threshold and potential_gt_idx_for_pred != -1:\n",
    "            tp += 1\n",
    "            matched_gt_indices.add(potential_gt_idx_for_pred) # Mark GT wake as matched\n",
    "            used_pred_indices.add(pred_idx) # Mark predicted wake as used for a TP\n",
    "        else:\n",
    "            fp += 1 # This predicted wake is a False Positive\n",
    "\n",
    "    # Calculate False Negatives: any ground truth wake that was not matched\n",
    "    fn = len(ground_truth_wakes) - len(matched_gt_indices)\n",
    "\n",
    "    total_ground_truth = len(ground_truth_wakes)\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    accuracy = tp / total_ground_truth if total_ground_truth > 0 else 0.0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    \n",
    "    # F1-Score is the harmonic mean of precision and recall\n",
    "    if (precision + recall) == 0:\n",
    "        f1_score = 0.0\n",
    "    else:\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    return tp, fp, fn, accuracy, precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0203779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wake_detection(df, file_name, save_dir=None, recon_col: str | None = \"z_m_recon\"):\n",
    "    \"\"\"\n",
    "    Plots the time series with GT (red spans) and predicted (green spans).\n",
    "    If recon_col is present in df, overlays the reconstructed signal as a line.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 6))\n",
    "\n",
    "    # Original signal\n",
    "    plt.plot(df['t_s'], df['z_m'], color='black', linewidth=0.8, label='z_m (Vertical Displacement)')\n",
    "\n",
    "    # (NEW) Reconstructed signal overlay if available\n",
    "    if recon_col is not None and recon_col in df.columns:\n",
    "        plt.plot(df['t_s'], df[recon_col], linewidth=0.9, label='Reconstruction')\n",
    "\n",
    "    # --- Ground truth spans (red) ---\n",
    "    in_gt_wake = False\n",
    "    gt_wake_start = None\n",
    "    gt_label_added = False\n",
    "    for _, row in df.iterrows():\n",
    "        if row['wake_label'] == 1 and not in_gt_wake:\n",
    "            gt_wake_start = row['t_s']; in_gt_wake = True\n",
    "        elif row['wake_label'] == 0 and in_gt_wake:\n",
    "            plt.axvspan(gt_wake_start, row['t_s'], color='red', alpha=0.3,\n",
    "                        label='Ground Truth Wake' if not gt_label_added else \"\")\n",
    "            gt_label_added = True; in_gt_wake = False\n",
    "    if in_gt_wake:\n",
    "        plt.axvspan(gt_wake_start, df['t_s'].iloc[-1], color='red', alpha=0.3,\n",
    "                    label='Ground Truth Wake' if not gt_label_added else \"\")\n",
    "\n",
    "    # --- Predicted spans (green) ---\n",
    "    in_pred_wake = False\n",
    "    pred_wake_start = None\n",
    "    pred_label_added = False\n",
    "    if 'predicted_wake_label' in df.columns:\n",
    "        for _, row in df.iterrows():\n",
    "            if row['predicted_wake_label'] == 1 and not in_pred_wake:\n",
    "                pred_wake_start = row['t_s']; in_pred_wake = True\n",
    "            elif row['predicted_wake_label'] == 0 and in_pred_wake:\n",
    "                plt.axvspan(pred_wake_start, row['t_s'], color='green', alpha=0.3,\n",
    "                            label='Predicted Wake' if not pred_label_added else \"\")\n",
    "                pred_label_added = True; in_pred_wake = False\n",
    "        if in_pred_wake:\n",
    "            plt.axvspan(pred_wake_start, df['t_s'].iloc[-1], color='green', alpha=0.3,\n",
    "                        label='Predicted Wake' if not pred_label_added else \"\")\n",
    "\n",
    "    plt.title(f'Wake Detection for {file_name}', fontsize=16)\n",
    "    plt.xlabel('Time (s)'); plt.ylabel('z_m (m)')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_dir:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        plot_path = os.path.join(save_dir, f'{os.path.basename(file_name).replace(\".csv\", \"\")}_wake_plot.png')\n",
    "        plt.savefig(plot_path, dpi=300)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f58f6298",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom PyTorch Dataset for time series sequences.\n",
    "    \"\"\"\n",
    "    def __init__(self, sequences, original_indices):\n",
    "        self.sequences = torch.tensor(sequences, dtype=torch.float32)\n",
    "        self.original_indices = original_indices # Keep track of original start indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.original_indices[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90ceccae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences_pytorch(df_data, window_size, stride, scaler=None, fit_scaler=True):\n",
    "    \"\"\"\n",
    "    Creates overlapping sequences (windows) from time series data ('z_m' column)\n",
    "    and returns them as NumPy arrays along with original start indices.\n",
    "    Manages StandardScaler fitting/transforming.\n",
    "    \"\"\"\n",
    "    data = df_data['z_m'].values.reshape(-1, 1) # Reshape for StandardScaler\n",
    "\n",
    "    if fit_scaler:\n",
    "        scaler = StandardScaler()\n",
    "        scaled_data = scaler.fit_transform(data)\n",
    "    else:\n",
    "        if scaler is None:\n",
    "            raise ValueError(\"Scaler must be provided if fit_scaler is False.\")\n",
    "        scaled_data = scaler.transform(data)\n",
    "\n",
    "    sequences = []\n",
    "    original_indices = [] # Stores the starting index in the original DataFrame for each sequence\n",
    "    \"\"\"\n",
    "    This list stores the starting index in the original scaled_data array for each created window.\n",
    "    This is very important later when we need to map the model's predictions (which are per-window)\n",
    "    back to the original time series data points.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Loop to create sliding windows\n",
    "    for i in range(0, len(scaled_data) - window_size + 1, stride):\n",
    "        sequences.append(scaled_data[i : i + window_size])\n",
    "        original_indices.append(i) # Record the start index of this window\n",
    "    \n",
    "    return np.array(sequences), np.array(original_indices), scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e305c0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMAutoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple LSTM Autoencoder model in PyTorch.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_features, hidden_size=HIDDEN_SIZE, latent_size=LATENT_SIZE, window_size=WINDOW_SIZE):\n",
    "        super(LSTMAutoencoder, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.input_features = input_features\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder_lstm1 = nn.LSTM(input_features, hidden_size, batch_first=True)\n",
    "        self.encoder_lstm2 = nn.LSTM(hidden_size, latent_size, batch_first=True) # Last hidden state is latent representation\n",
    "\n",
    "        # Decoder\n",
    "        # Input to decoder is the repeated latent vector\n",
    "        self.decoder_lstm1 = nn.LSTM(latent_size, hidden_size, batch_first=True)\n",
    "        self.decoder_lstm2 = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, input_features) # Output layer to match original feature size\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        # Pass through first LSTM, get all hidden states, we only need the last one (h_n, c_n)\n",
    "        _, (h_n, _) = self.encoder_lstm1(x)\n",
    "        # Pass through second LSTM, get only the last hidden state from the last layer (h_n)\n",
    "        # h_n will be of shape (num_layers * num_directions, batch_size, hidden_size)\n",
    "        # We want the last layer's hidden state, which is h_n[-1] if num_layers=1 for second LSTM\n",
    "        _, (h_n2, _) = self.encoder_lstm2(h_n.permute(1, 0, 2)) # Permute to (batch_size, num_layers, hidden_size) for next LSTM\n",
    "        \n",
    "        # Latent representation is h_n2[-1] (last layer's last hidden state)\n",
    "        latent_vector = h_n2[-1] # Shape: (batch_size, latent_size)\n",
    "\n",
    "        # RepeatVector equivalent: repeat the latent vector for each time step of the decoder\n",
    "        # latent_vector.unsqueeze(1) changes from (batch_size, latent_size) to (batch_size, 1, latent_size)\n",
    "        # .repeat(1, self.window_size, 1) repeats it across the sequence length dimension\n",
    "        decoder_input = latent_vector.unsqueeze(1).repeat(1, self.window_size, 1)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_output1, _ = self.decoder_lstm1(decoder_input)\n",
    "        decoder_output2, _ = self.decoder_lstm2(decoder_output1)\n",
    "        \n",
    "        # Apply linear layer to each time step using TimeDistributed equivalent\n",
    "        # Reshape to (batch_size * window_size, hidden_size) for linear layer\n",
    "        output = self.output_layer(decoder_output2.reshape(-1, decoder_output2.shape[-1]))\n",
    "        # Reshape back to (batch_size, window_size, input_features)\n",
    "        output = output.reshape(x.shape[0], self.window_size, self.input_features)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3d887ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_wake_intervals_from_errors(df, original_indices, errors, anomaly_threshold, window_size, \n",
    "                                             merge_gap_seconds, min_wake_duration_seconds):\n",
    "    \"\"\"\n",
    "    Converts window-level anomaly scores (reconstruction errors) into continuous wake intervals.\n",
    "    It then populates a 'predicted_wake_label' column in the DataFrame and performs\n",
    "    post-processing (merging close intervals, filtering by min duration).\n",
    "    \"\"\"\n",
    "    df_copy = df.copy() \n",
    "\n",
    "    df_copy['predicted_wake_label'] = 0\n",
    "\n",
    "    anomalous_window_indices_in_errors = np.where(errors > anomaly_threshold)[0]\n",
    "\n",
    "    for error_idx in anomalous_window_indices_in_errors:\n",
    "        start_df_idx = original_indices[error_idx] \n",
    "        end_df_idx = start_df_idx + window_size    \n",
    "        \n",
    "        end_df_idx = min(end_df_idx, len(df_copy))\n",
    "        \n",
    "        df_copy.loc[start_df_idx:end_df_idx-1, 'predicted_wake_label'] = 1\n",
    "\n",
    "    predicted_wakes_raw = []\n",
    "    in_wake_segment = False\n",
    "    segment_start_time = None\n",
    "\n",
    "    for i, row in df_copy.iterrows():\n",
    "        t_s = row['t_s']\n",
    "        predicted_label = row['predicted_wake_label']\n",
    "\n",
    "        if predicted_label == 1 and not in_wake_segment:\n",
    "            segment_start_time = t_s\n",
    "            in_wake_segment = True\n",
    "        elif predicted_label == 0 and in_wake_segment:\n",
    "            segment_end_time = t_s\n",
    "            predicted_wakes_raw.append((segment_start_time, segment_end_time))\n",
    "            in_wake_segment = False\n",
    "    \n",
    "    if in_wake_segment:\n",
    "        predicted_wakes_raw.append((segment_start_time, df_copy['t_s'].iloc[-1]))\n",
    "    \n",
    "    final_predicted_wakes = []\n",
    "    if predicted_wakes_raw:\n",
    "        predicted_wakes_raw.sort() \n",
    "\n",
    "        current_wake_start_time = predicted_wakes_raw[0][0]\n",
    "        current_wake_end_time = predicted_wakes_raw[0][1]\n",
    "\n",
    "        for i in range(1, len(predicted_wakes_raw)):\n",
    "            next_segment_start_time = predicted_wakes_raw[i][0]\n",
    "            next_segment_end_time = predicted_wakes_raw[i][1]\n",
    "\n",
    "            if (next_segment_start_time - current_wake_end_time) <= merge_gap_seconds:\n",
    "                current_wake_end_time = max(current_wake_end_time, next_segment_end_time)\n",
    "            else:\n",
    "                if (current_wake_end_time - current_wake_start_time) >= min_wake_duration_seconds:\n",
    "                    final_predicted_wakes.append((current_wake_start_time, current_wake_end_time))\n",
    "                current_wake_start_time = next_segment_start_time\n",
    "                current_wake_end_time = next_segment_end_time\n",
    "        \n",
    "        if (current_wake_end_time - current_wake_start_time) >= min_wake_duration_seconds:\n",
    "            final_predicted_wakes.append((current_wake_start_time, current_wake_end_time))\n",
    "\n",
    "    return final_predicted_wakes, df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e59d145b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wake_labels_for_windows(df, original_indices, window_size):\n",
    "    \"\"\"\n",
    "    Assigns a label to each window: 1 if ANY point in the window is a ground truth wake (label 1),\n",
    "    0 otherwise (meaning the entire window is composed of normal points).\n",
    "    \"\"\"\n",
    "    window_labels = []\n",
    "    wake_labels_series = df['wake_label']\n",
    "    \n",
    "    for start_idx in original_indices:\n",
    "        # Check if any point within the current window has a wake_label of 1\n",
    "        end_idx = start_idx + window_size\n",
    "        # Use .max() to check if there's any '1' (wake) in the window\n",
    "        if wake_labels_series.iloc[start_idx:end_idx].max() == 1:\n",
    "            window_labels.append(1) # This window contains a wake\n",
    "        else:\n",
    "            window_labels.append(0) # This window contains only normal points\n",
    "    return np.array(window_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0922b82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstructing predicted signals.\n",
    "import numpy as np\n",
    "\n",
    "def reconstruct_series_from_windows(\n",
    "    original_len: int,\n",
    "    original_indices,            # starts OR (start,end) pairs; list/array/tuples/dicts ok\n",
    "    recon_windows: np.ndarray,   # (N, L) or (N, L, C)\n",
    "    feature_idx: int | None = None,\n",
    "    scaler=None,\n",
    "    inverse_scale: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      recon_series: (original_len,) averaged reconstruction per sample\n",
    "      cover_count:  (original_len,) number of windows contributing per sample\n",
    "    \"\"\"\n",
    "    W = recon_windows\n",
    "\n",
    "    # pick channel if multivariate\n",
    "    if W.ndim == 3:\n",
    "        if feature_idx is None:\n",
    "            feature_idx = 0\n",
    "        W = W[:, :, feature_idx]\n",
    "    elif W.ndim != 2:\n",
    "        raise ValueError(f\"Unexpected recon_windows shape {recon_windows.shape}; expected (N,L) or (N,L,C)\")\n",
    "\n",
    "    N, L = W.shape\n",
    "\n",
    "    # Normalize original_indices into a list of (start, end) pairs\n",
    "    pairs = []\n",
    "    orig = original_indices\n",
    "\n",
    "    # numpy array?\n",
    "    if isinstance(orig, np.ndarray):\n",
    "        if orig.ndim == 1:\n",
    "            # 1-D starts\n",
    "            for s in orig.tolist():\n",
    "                s = int(s)\n",
    "                pairs.append((s, s + L - 1))\n",
    "        elif orig.ndim == 2 and orig.shape[1] >= 2:\n",
    "            # (N,2) -> (start,end)\n",
    "            for row in orig.tolist():\n",
    "                pairs.append((int(row[0]), int(row[1])))\n",
    "        else:\n",
    "            # fallback: treat as starts\n",
    "            for s in orig.reshape(-1).tolist():\n",
    "                pairs.append((int(s), int(s) + L - 1))\n",
    "    else:\n",
    "        # list-like\n",
    "        for item in list(orig):\n",
    "            if isinstance(item, dict):\n",
    "                s = int(item.get(\"start\", item.get(\"s\", 0)))\n",
    "                e = int(item.get(\"end\", item.get(\"e\", s + L - 1)))\n",
    "                pairs.append((s, e))\n",
    "            elif isinstance(item, (tuple, list)) and len(item) >= 2:\n",
    "                s, e = int(item[0]), int(item[1])\n",
    "                pairs.append((s, e))\n",
    "            else:\n",
    "                # assume it's a start\n",
    "                s = int(item)\n",
    "                pairs.append((s, s + L - 1))\n",
    "\n",
    "    # Optionally inverse-scale the windows (for plotting in meters)\n",
    "    if inverse_scale and scaler is not None:\n",
    "        W_flat = W.reshape(-1, 1)               # assumes 1 feature for plotting\n",
    "        W_inv = scaler.inverse_transform(W_flat)\n",
    "        W = W_inv.reshape(N, L)\n",
    "\n",
    "    recon_series = np.zeros((original_len,), dtype=np.float32)\n",
    "    cover_count  = np.zeros((original_len,), dtype=np.int32)\n",
    "\n",
    "    for (s, e), w in zip(pairs, W):\n",
    "        # clamp to series bounds\n",
    "        s_clamp = max(0, min(int(s), original_len - 1))\n",
    "        e_clamp = max(0, min(int(e), original_len - 1))\n",
    "        if e_clamp < s_clamp:\n",
    "            continue\n",
    "        span = e_clamp - s_clamp + 1\n",
    "        # align window length with span (handles off-by-ones)\n",
    "        span = min(span, len(w))\n",
    "        recon_series[s_clamp:s_clamp + span] += w[:span]\n",
    "        cover_count[s_clamp:s_clamp + span]  += 1\n",
    "\n",
    "    mask = cover_count > 0\n",
    "    recon_series[mask] = recon_series[mask] / cover_count[mask]\n",
    "    return recon_series, cover_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d578d472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ________________________ main _________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d10bb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting ML Model Training and Evaluation ---\n"
     ]
    }
   ],
   "source": [
    "dataset_splits = {\n",
    "    'train': [], \n",
    "    'valid': [], \n",
    "    'test': []   \n",
    "}\n",
    "\n",
    "for split_name in dataset_splits.keys():\n",
    "    split_path = os.path.join(base_data_dir, split_name)\n",
    "    if os.path.exists(split_path):\n",
    "        dataset_splits[split_name] = glob.glob(os.path.join(split_path, '*.csv'))\n",
    "    else:\n",
    "        print(f\"Warning: Directory '{split_path}' not found. Skipping {split_name} split.\")\n",
    "        dataset_splits[split_name] = [] \n",
    "\n",
    "\n",
    "print(\"--- Starting ML Model Training and Evaluation ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82609a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to plot a batch sample\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "PLOT_OUT_DIR = \"detected_wakes_LSTM_AE/normal_window_plots_LSTM_batch\"\n",
    "os.makedirs(PLOT_OUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d695fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing training data (normal sequences) for LSTM Autoencoder...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61e03075e2b0462dac0ab3f57dce7c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing training files:   0%|          | 0/9909 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total normal sequences for training: 3611548\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Prepare Training Data (Normal sequences only from 'train' split) ---\n",
    "print(\"\\nPreparing training data (normal sequences) for LSTM Autoencoder...\")\n",
    "all_train_normal_sequences_np = []\n",
    "data_scaler = None \n",
    "\n",
    "if not dataset_splits['train']:\n",
    "    print(\"Error: No training files found. Cannot train the model. Please check 'processed_ts/train' directory.\")\n",
    "else:\n",
    "    for file_path in tqdm(dataset_splits['train'], desc=\"Processing training files\"):\n",
    "        df = load_and_preprocess_data(file_path)\n",
    "        if df is not None:\n",
    "            # Fit scaler on first file, then transform others\n",
    "            if data_scaler is None:\n",
    "                sequences, original_indices, data_scaler = create_sequences_pytorch(df, WINDOW_SIZE, STRIDE, fit_scaler=True)\n",
    "            else:\n",
    "                sequences, original_indices, _ = create_sequences_pytorch(df, WINDOW_SIZE, STRIDE, scaler=data_scaler, fit_scaler=False)\n",
    "\n",
    "\n",
    "            window_labels = get_wake_labels_for_windows(df, original_indices, WINDOW_SIZE)\n",
    "\n",
    "            normal_sequences_in_file = sequences[window_labels == 0]\n",
    "            if len(normal_sequences_in_file) > 0:\n",
    "                all_train_normal_sequences_np.append(normal_sequences_in_file)\n",
    "\n",
    "    if all_train_normal_sequences_np:\n",
    "        all_train_normal_sequences_np = np.concatenate(all_train_normal_sequences_np, axis=0)\n",
    "        print(f\"Total normal sequences for training: {all_train_normal_sequences_np.shape[0]}\")\n",
    "    else:\n",
    "        print(\"No normal sequences found for training. Model will not be trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b96f811",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building and training LSTM Autoencoder...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03044a3f64294495bdf396033d27278c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Autoencoder Epochs:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Training Loss: 0.035123 (New best model saved!)\n",
      "Epoch 2/50, Training Loss: 0.034856 (New best model saved!)\n",
      "Epoch 3/50, Training Loss: 0.034554 (New best model saved!)\n",
      "Epoch 4/50, Training Loss: 0.034909\n",
      "Epoch 5/50, Training Loss: 0.032608 (New best model saved!)\n",
      "Epoch 6/50, Training Loss: 0.028787 (New best model saved!)\n",
      "Epoch 7/50, Training Loss: 0.031763\n",
      "Epoch 8/50, Training Loss: 0.028859\n",
      "Epoch 9/50, Training Loss: 0.032883\n",
      "Epoch 10/50, Training Loss: 0.028301 (New best model saved!)\n",
      "Epoch 11/50, Training Loss: 0.028731\n",
      "Epoch 12/50, Training Loss: 0.025714 (New best model saved!)\n",
      "Epoch 13/50, Training Loss: 0.031437\n",
      "Epoch 14/50, Training Loss: 0.028402\n",
      "Epoch 15/50, Training Loss: 0.025063 (New best model saved!)\n",
      "Epoch 16/50, Training Loss: 0.021327 (New best model saved!)\n",
      "Epoch 17/50, Training Loss: 0.017832 (New best model saved!)\n",
      "Epoch 18/50, Training Loss: 0.018139\n",
      "Epoch 19/50, Training Loss: 0.021772\n",
      "Epoch 20/50, Training Loss: 0.018443\n",
      "Epoch 21/50, Training Loss: 0.016932 (New best model saved!)\n",
      "Epoch 22/50, Training Loss: 0.015580 (New best model saved!)\n",
      "Epoch 23/50, Training Loss: 0.018672\n",
      "Epoch 24/50, Training Loss: 0.016678\n",
      "Epoch 25/50, Training Loss: 0.014593 (New best model saved!)\n",
      "Epoch 26/50, Training Loss: 0.013954 (New best model saved!)\n",
      "Epoch 27/50, Training Loss: 0.014497\n",
      "Epoch 28/50, Training Loss: 0.015347\n",
      "Epoch 29/50, Training Loss: 0.012844 (New best model saved!)\n",
      "Epoch 30/50, Training Loss: 0.012947\n",
      "Epoch 31/50, Training Loss: 0.022021\n",
      "Epoch 32/50, Training Loss: 0.039149\n",
      "Epoch 33/50, Training Loss: 0.034689\n",
      "Epoch 34/50, Training Loss: 0.029877\n",
      "Epoch 35/50, Training Loss: 0.064804\n",
      "Epoch 36/50, Training Loss: 0.036227\n",
      "Epoch 37/50, Training Loss: 0.036448\n",
      "Epoch 38/50, Training Loss: 0.034218\n",
      "Epoch 39/50, Training Loss: 0.031176\n",
      "Epoch 40/50, Training Loss: 0.045550\n",
      "Epoch 41/50, Training Loss: 0.035594\n",
      "Epoch 42/50, Training Loss: 0.035602\n",
      "Epoch 43/50, Training Loss: 0.035558\n",
      "Epoch 44/50, Training Loss: 0.035572\n",
      "Epoch 45/50, Training Loss: 0.035726\n",
      "Epoch 46/50, Training Loss: 0.035325\n",
      "Epoch 47/50, Training Loss: 0.034722\n",
      "Epoch 48/50, Training Loss: 0.034033\n",
      "Epoch 49/50, Training Loss: 0.032854\n",
      "Epoch 50/50, Training Loss: 0.033141\n",
      "LSTM Autoencoder training complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2: Build and Train LSTM Autoencoder ---\n",
    "lstm_autoencoder = None\n",
    "# Corrected check: ensure all_train_normal_sequences_np is a NumPy array and has elements\n",
    "if all_train_normal_sequences_np is not None and all_train_normal_sequences_np.shape[0] > 0: \n",
    "    print(\"\\nBuilding and training LSTM Autoencoder...\")\n",
    "    input_features = all_train_normal_sequences_np.shape[2] \n",
    "    lstm_autoencoder = LSTMAutoencoder(input_features, window_size=WINDOW_SIZE).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(lstm_autoencoder.parameters())\n",
    "    criterion = nn.MSELoss() \n",
    "\n",
    "    train_dataset = TimeSeriesDataset(all_train_normal_sequences_np, \n",
    "                                      np.arange(len(all_train_normal_sequences_np)))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    # Define path for saving the best model\n",
    "    os.makedirs(model_save_dir, exist_ok=True)\n",
    "    best_model_path = os.path.join(model_save_dir, 'best_lstm_autoencoder.pth')\n",
    "\n",
    "    best_loss = float('inf') # Initialize best_loss to infinity\n",
    "\n",
    "    # Add tqdm to the epoch loop for better visibility\n",
    "    for epoch in tqdm(range(EPOCHS), desc=\"Training Autoencoder Epochs\"):\n",
    "        lstm_autoencoder.train() \n",
    "        train_loss = 0\n",
    "        for batch_idx, (data, _) in enumerate(train_loader):\n",
    "            data = data.to(device) \n",
    "\n",
    "            optimizer.zero_grad() \n",
    "            outputs = lstm_autoencoder(data) \n",
    "            loss = criterion(outputs, data) \n",
    "            loss.backward() \n",
    "            optimizer.step() \n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        # Check if current model is the best\n",
    "        if avg_train_loss < best_loss:\n",
    "            best_loss = avg_train_loss\n",
    "            torch.save(lstm_autoencoder.state_dict(), best_model_path)\n",
    "            tqdm.write(f\"Epoch {epoch+1}/{EPOCHS}, Training Loss: {avg_train_loss:.6f} (New best model saved!)\")\n",
    "        else:\n",
    "            tqdm.write(f\"Epoch {epoch+1}/{EPOCHS}, Training Loss: {avg_train_loss:.6f}\")\n",
    "\n",
    "    print(\"LSTM Autoencoder training complete.\")\n",
    "else:\n",
    "    print(\"Skipping LSTM Autoencoder training due to insufficient normal data.\")\n",
    "\n",
    "os.makedirs(os.path.join(output_base_dir_ml, 'train'), exist_ok=True) \n",
    "os.makedirs(os.path.join(output_base_dir_ml, 'valid'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_base_dir_ml, 'test'), exist_ok=True)\n",
    "os.makedirs(plots_output_dir_ml, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7bb7363d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "best_model_path = model_save_dir + \"/best_lstm_autoencoder.pth\"  # detected_wakes_LSTM_AE/saved_models/w300,s5,th90\n",
    "input_features = 1 # Assuming z_m is the only feature\n",
    "lstm_autoencoder = LSTMAutoencoder(input_features, window_size=WINDOW_SIZE).to(device)\n",
    "\n",
    "lstm_autoencoder.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "lstm_autoencoder.eval() # Set to evaluation mode after loading\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13e45067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Determining anomaly threshold from validation set normal data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcb47a3a275547789b65e33936a449ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analyzing validation files for threshold:   0%|          | 0/3411 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determined Anomaly Threshold (Q70 on normal validation errors): 0.007237\n"
     ]
    }
   ],
   "source": [
    "# --- Step 3: Anomaly Threshold Determination (from 'valid' set's normal data) ---\n",
    "anomaly_threshold = 0.0 \n",
    "valid_normal_errors = []\n",
    "\n",
    "if lstm_autoencoder and dataset_splits['valid'] and data_scaler:\n",
    "    print(\"\\nDetermining anomaly threshold from validation set normal data...\")\n",
    "    lstm_autoencoder.eval() # Set model to evaluation mode\n",
    "    with torch.no_grad(): # Disable gradient calculation for inference\n",
    "        for file_path in tqdm(dataset_splits['valid'], desc=\"Analyzing validation files for threshold\"):\n",
    "            df_valid = load_and_preprocess_data(file_path)\n",
    "            if df_valid is not None:\n",
    "                valid_sequences_np, valid_original_indices_np, _ = create_sequences_pytorch(df_valid, WINDOW_SIZE, STRIDE, scaler=data_scaler, fit_scaler=False)\n",
    "                valid_window_labels = get_wake_labels_for_windows(df_valid, valid_original_indices_np, WINDOW_SIZE)\n",
    "\n",
    "                if len(valid_sequences_np) > 0:\n",
    "                    valid_sequences_torch = torch.tensor(valid_sequences_np, dtype=torch.float32).to(device)\n",
    "                    reconstructions_torch = lstm_autoencoder(valid_sequences_torch)\n",
    "\n",
    "                    # Calculate MSE reconstruction errors (NumPy conversion for mean_squared_error)\n",
    "                    errors = np.array([\n",
    "                        mean_squared_error(valid_sequences_np[i].flatten(), \n",
    "                                           reconstructions_torch[i].cpu().numpy().flatten())\n",
    "                        for i in range(len(valid_sequences_np))\n",
    "                    ])\n",
    "\n",
    "                    valid_normal_errors.extend(errors[valid_window_labels == 0])\n",
    "\n",
    "        if valid_normal_errors:\n",
    "            anomaly_threshold = np.quantile(valid_normal_errors, ANOMALY_THRESHOLD_QUANTILE)\n",
    "            print(f\"Determined Anomaly Threshold (Q{int(ANOMALY_THRESHOLD_QUANTILE*100)} on normal validation errors): {anomaly_threshold:.6f}\")\n",
    "        else:\n",
    "            print(\"No normal data found in validation set to determine anomaly threshold. Using default 0.0.\")\n",
    "elif not lstm_autoencoder:\n",
    "    print(\"Skipping anomaly threshold determination because the LSTM Autoencoder was not trained.\")\n",
    "elif not data_scaler:\n",
    "    print(\"Skipping anomaly threshold determination because the data scaler was not fitted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da414fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=FutureWarning,\n",
    "    message=r\"`torch\\.cuda\\.amp\\.autocast\\(.*\\)` is deprecated\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82158da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing TRAIN Set (9909 files) for ML prediction ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "970c041f84a7417cb4652f10cf7384b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting on train files:   0%|          | 0/9909 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ML TRAIN Evaluation Results ---\n",
      "  Total Ground Truth Wakes: 9672\n",
      "  Total Predicted Wakes: 13773\n",
      "  True Positives (TP): 8593\n",
      "  False Positives (FP): 5180\n",
      "  False Negatives (FN): 1079\n",
      "  Accuracy (TP / Total GT): 0.8884\n",
      "  Precision: 0.6239\n",
      "  Recall: 0.8884\n",
      "  F1-Score: 0.7330\n",
      "_______Window level_________\n",
      "  True Positives (TP): 670799\n",
      "  False Positives (FP): 1101510\n",
      "  False Negatives (FN): 1307660\n",
      "  True Negatives (TN): 2510038\n",
      "  Accuracy (window): 5590.2232\n",
      "  Precision: 0.3785\n",
      "  Recall: 0.3391\n",
      "  F1-Score: 0.3577\n",
      "\n",
      "--- Processing VALID Set (3411 files) for ML prediction ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce37fced3a834e6b9906f258f12c59a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting on valid files:   0%|          | 0/3411 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ML VALID Evaluation Results ---\n",
      "  Total Ground Truth Wakes: 3332\n",
      "  Total Predicted Wakes: 4879\n",
      "  True Positives (TP): 2993\n",
      "  False Positives (FP): 1886\n",
      "  False Negatives (FN): 339\n",
      "  Accuracy (TP / Total GT): 0.8983\n",
      "  Precision: 0.6134\n",
      "  Recall: 0.8983\n",
      "  F1-Score: 0.7290\n",
      "_______Window level_________\n",
      "  True Positives (TP): 222323\n",
      "  False Positives (FP): 374791\n",
      "  False Negatives (FN): 462663\n",
      "  True Negatives (TN): 874159\n",
      "  Accuracy (window): 1965.0215\n",
      "  Precision: 0.3723\n",
      "  Recall: 0.3246\n",
      "  F1-Score: 0.3468\n",
      "\n",
      "--- Processing TEST Set (6121 files) for ML prediction ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "106994a3372f41bcad03d2ebecdf3210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting on test files:   0%|          | 0/6121 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ML TEST Evaluation Results ---\n",
      "  Total Ground Truth Wakes: 3345\n",
      "  Total Predicted Wakes: 8145\n",
      "  True Positives (TP): 3312\n",
      "  False Positives (FP): 4833\n",
      "  False Negatives (FN): 33\n",
      "  Accuracy (TP / Total GT): 0.9901\n",
      "  Precision: 0.4066\n",
      "  Recall: 0.9901\n",
      "  F1-Score: 0.5765\n",
      "_______Window level_________\n",
      "  True Positives (TP): 225247\n",
      "  False Positives (FP): 1270687\n",
      "  False Negatives (FN): 458087\n",
      "  True Negatives (TN): 1526388\n",
      "  Accuracy (window): 3139.1308\n",
      "  Precision: 0.1506\n",
      "  Recall: 0.3296\n",
      "  F1-Score: 0.2067\n",
      "\n",
      "--- Overall ML Dataset Evaluation Results ---\n",
      "Total Ground Truth Wakes: 16349\n",
      "Total Predicted Wakes: 26797\n",
      "True Positives (TP): 15045\n",
      "False Positives (FP): 11752\n",
      "False Negatives (FN): 1304\n",
      "Accuracy (TP / Total GT): 0.9202\n",
      "Precision: 0.5614\n",
      "Recall: 0.9202\n",
      "F1-Score: 0.6974\n"
     ]
    }
   ],
   "source": [
    "# --- Step 4: Predict and Evaluate on Train, Validation, and Test Sets ---\n",
    "overall_ground_truth_wakes_ml = []\n",
    "overall_predicted_wakes_ml = []\n",
    "\n",
    "for split_name in dataset_splits.keys():\n",
    "    file_paths = dataset_splits[split_name]\n",
    "    if not file_paths or not lstm_autoencoder or not data_scaler:\n",
    "        print(f\"\\nSkipping {split_name.upper()} evaluation due to no files, model not trained, or scaler not fitted.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n--- Processing {split_name.upper()} Set ({len(file_paths)} files) for ML prediction ---\")\n",
    "\n",
    "    split_ground_truth_wakes = []\n",
    "    split_predicted_wakes = []\n",
    "    window_level_counts = { \"TP\": 0, \"FP\": 0, \"FN\": 0, \"TN\": 0 }\n",
    "\n",
    "    lstm_autoencoder.eval() # Set model to evaluation mode\n",
    "    with torch.no_grad(): # Disable gradient calculation\n",
    "        for file_path in tqdm(file_paths, desc=f\"Predicting on {split_name} files\"):\n",
    "            df = load_and_preprocess_data(file_path)\n",
    "            if df is not None:\n",
    "                sequences_to_predict_np, original_indices_predict_np, _ = create_sequences_pytorch(\n",
    "                    df, WINDOW_SIZE, STRIDE, scaler=data_scaler, fit_scaler=False\n",
    "                )\n",
    "\n",
    "                if len(sequences_to_predict_np) > 0:\n",
    "                    sequences_to_predict_torch = torch.tensor(sequences_to_predict_np, dtype=torch.float32).to(device)\n",
    "                    reconstructions_torch = lstm_autoencoder(sequences_to_predict_torch)\n",
    "\n",
    "#                     #Calculate errors using numpy after moving to CPU\n",
    "#                     errors_predict = np.array([\n",
    "#                         mean_squared_error(sequences_to_predict_np[i].flatten(), \n",
    "#                                            reconstructions_torch[i].cpu().numpy().flatten())\n",
    "#                         for i in range(len(sequences_to_predict_np))\n",
    "#                     ])\n",
    "\n",
    "                    ####### AFTER (fast, torch, batched)\n",
    "                    seqs = torch.as_tensor(sequences_to_predict_np, dtype=torch.float32, device=device)\n",
    "\n",
    "                    BATCH = 4096 # tune for your GPU RAM; 1024–8192 usually fine for 1D windows\n",
    "                    errors_predict = np.empty((len(seqs),), dtype=np.float32)\n",
    "\n",
    "                    lstm_autoencoder.eval()\n",
    "                    with torch.no_grad():\n",
    "                        # mixed precision can speed up on GPUs\n",
    "                        try:\n",
    "                            use_amp = (device.type == \"cuda\")\n",
    "                        except Exception:\n",
    "                            use_amp = False\n",
    "                            autocast = lambda enabled: contextlib.nullcontext()\n",
    "\n",
    "                        for i in range(0, len(seqs), BATCH):\n",
    "                            batch = seqs[i:i+BATCH]\n",
    "                            with autocast(enabled=use_amp):\n",
    "                                recon = lstm_autoencoder(batch)\n",
    "                            diff = batch - recon\n",
    "                            if diff.ndim == 3:\n",
    "                                # (N, L, C)\n",
    "                                batch_err = (diff ** 2).mean(dim=(1, 2))\n",
    "                            else:\n",
    "                                # (N, L)\n",
    "                                batch_err = (diff ** 2).mean(dim=1)\n",
    "\n",
    "                            errors_predict[i:i+BATCH] = batch_err.detach().cpu().numpy()\n",
    "                    ###########\n",
    "                    \n",
    "                    # **** WINDOW-LEVEL metrics \n",
    "                    gt_win = get_wake_labels_for_windows(df, original_indices_predict_np, WINDOW_SIZE)\n",
    "                    pred_win = (errors_predict >= anomaly_threshold).astype(np.int32)\n",
    "                    \n",
    "                    # Confusion counts (window-level)\n",
    "                    TP_w = int(((gt_win == 1) & (pred_win == 1)).sum())\n",
    "                    FP_w = int(((gt_win == 0) & (pred_win == 1)).sum())\n",
    "                    FN_w = int(((gt_win == 1) & (pred_win == 0)).sum())\n",
    "                    TN_w = int(((gt_win == 0) & (pred_win == 0)).sum())\n",
    "                    \n",
    "                    window_level_counts[\"TP\"] += TP_w\n",
    "                    window_level_counts[\"FP\"] += FP_w\n",
    "                    window_level_counts[\"FN\"] += FN_w\n",
    "                    window_level_counts[\"TN\"] += TN_w\n",
    "                    # ****\n",
    "\n",
    "                    # EVENT-LEVEL intervals (post-processing)\n",
    "                    predicted_wake_intervals, df_with_predictions = \\\n",
    "                        get_predicted_wake_intervals_from_errors(df.copy(), original_indices_predict_np, \n",
    "                                                                errors_predict, anomaly_threshold, \n",
    "                                                                WINDOW_SIZE, MERGE_GAP_SECONDS, MIN_WAKE_DURATION_SECONDS)\n",
    "\n",
    "                    gt_wakes = get_ground_truth_wakes(df)\n",
    "                    split_ground_truth_wakes.extend(gt_wakes)\n",
    "                    split_predicted_wakes.extend(predicted_wake_intervals)\n",
    "\n",
    "                    relative_path = os.path.relpath(file_path, base_data_dir)\n",
    "                    output_file_path = os.path.join(output_base_dir_ml, relative_path)\n",
    "                    os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n",
    "                    \n",
    "                    ####3 for ploting the predictions as well\n",
    "                    recon_np = reconstructions_torch.detach().cpu().numpy()\n",
    "\n",
    "                    # build per-sample reconstruction\n",
    "                    recon_series, cover = reconstruct_series_from_windows(\n",
    "                        original_len=len(df),\n",
    "                        original_indices=original_indices_predict_np,\n",
    "                        recon_windows=recon_np,\n",
    "                        feature_idx=0,\n",
    "                        scaler=data_scaler,\n",
    "                        inverse_scale=True\n",
    "                    )\n",
    "                    recon_series_filled = recon_series.copy()\n",
    "                    holes = (cover == 0)\n",
    "                    if holes.any():\n",
    "                        recon_series_filled[holes] = df[\"z_m\"].values[holes]\n",
    "\n",
    "                    df_with_predictions[\"z_m_recon\"] = recon_series_filled\n",
    "\n",
    "                    #######\n",
    "                    \n",
    "                    \n",
    "                    df_with_predictions.to_csv(output_file_path, index=False)\n",
    "                else:\n",
    "                    print(f\"\\n  No sequences to predict for {file_path}. Skipping.\")\n",
    "            else:\n",
    "                print(f\"\\n  Skipping processing for file: {file_path} due to load error.\")\n",
    "\n",
    "    if split_ground_truth_wakes or split_predicted_wakes:\n",
    "        tp, fp, fn, accuracy, precision, recall, f1_score = evaluate_detection(\n",
    "            split_ground_truth_wakes, split_predicted_wakes\n",
    "        )\n",
    "\n",
    "        print(f\"\\n--- ML {split_name.upper()} Evaluation Results ---\")\n",
    "        print(f\"  Total Ground Truth Wakes: {len(split_ground_truth_wakes)}\")\n",
    "        print(f\"  Total Predicted Wakes: {len(split_predicted_wakes)}\")\n",
    "        print(f\"  True Positives (TP): {tp}\")\n",
    "        print(f\"  False Positives (FP): {fp}\")\n",
    "        print(f\"  False Negatives (FN): {fn}\")\n",
    "        print(f\"  Accuracy (TP / Total GT): {accuracy:.4f}\")\n",
    "        print(f\"  Precision: {precision:.4f}\")\n",
    "        print(f\"  Recall: {recall:.4f}\")\n",
    "        print(f\"  F1-Score: {f1_score:.4f}\")\n",
    "        \n",
    "        print(f\"_______Window level_________\")\n",
    "        TP = window_level_counts[\"TP\"]\n",
    "        FP = window_level_counts[\"FP\"]\n",
    "        FN = window_level_counts[\"FN\"]\n",
    "        TN = window_level_counts[\"TN\"]\n",
    "        prec_w = TP / (TP + FP) if (TP + FP) else 0.0\n",
    "        rec_w  = TP / (TP + FN) if (TP + FN) else 0.0\n",
    "        f1_w   = 2*prec_w*rec_w / (prec_w + rec_w) if (prec_w + rec_w) else 0.0\n",
    "        acc_w  = (TP + TN) / len(gt_win) if len(gt_win) else 0.0\n",
    "        print(f\"  True Positives (TP): {TP}\")\n",
    "        print(f\"  False Positives (FP): {FP}\")\n",
    "        print(f\"  False Negatives (FN): {FN}\")\n",
    "        print(f\"  True Negatives (TN): {TN}\")\n",
    "        print(f\"  Accuracy (window): {acc_w:.4f}\")\n",
    "        print(f\"  Precision: {prec_w:.4f}\")\n",
    "        print(f\"  Recall: {rec_w:.4f}\")\n",
    "        print(f\"  F1-Score: {f1_w:.4f}\")\n",
    "        \n",
    "        overall_ground_truth_wakes_ml.extend(split_ground_truth_wakes)\n",
    "        overall_predicted_wakes_ml.extend(split_predicted_wakes)\n",
    "    else:\n",
    "        print(f\"  No ground truth or predicted wakes found in {split_name} set for ML evaluation.\")\n",
    "\n",
    "if overall_ground_truth_wakes_ml or overall_predicted_wakes_ml:\n",
    "    tp_overall, fp_overall, fn_overall, accuracy_overall, precision_overall, recall_overall, f1_score_overall = evaluate_detection(\n",
    "        overall_ground_truth_wakes_ml, overall_predicted_wakes_ml\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Overall ML Dataset Evaluation Results ---\")\n",
    "    print(f\"Total Ground Truth Wakes: {len(overall_ground_truth_wakes_ml)}\")\n",
    "    print(f\"Total Predicted Wakes: {len(overall_predicted_wakes_ml)}\")\n",
    "    print(f\"True Positives (TP): {tp_overall}\")\n",
    "    print(f\"False Positives (FP): {fp_overall}\")\n",
    "    print(f\"False Negatives (FN): {fn_overall}\")\n",
    "    print(f\"Accuracy (TP / Total GT): {accuracy_overall:.4f}\")\n",
    "    print(f\"Precision: {precision_overall:.4f}\")\n",
    "    print(f\"Recall: {recall_overall:.4f}\")\n",
    "    print(f\"F1-Score: {f1_score_overall:.4f}\")\n",
    "else:\n",
    "    print(\"\\nNo ground truth or predicted wakes found across the entire dataset for overall ML evaluation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab974050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  display: none;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  overflow: visible;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".estimator-table summary {\n",
       "    padding: .5rem;\n",
       "    font-family: monospace;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".estimator-table details[open] {\n",
       "    padding-left: 0.1rem;\n",
       "    padding-right: 0.1rem;\n",
       "    padding-bottom: 0.3rem;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table {\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(odd) {\n",
       "    background-color: #fff;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(even) {\n",
       "    background-color: #f6f6f6;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:hover {\n",
       "    background-color: #e0e0e0;\n",
       "}\n",
       "\n",
       ".estimator-table table td {\n",
       "    border: 1px solid rgba(106, 105, 104, 0.232);\n",
       "}\n",
       "\n",
       ".user-set td {\n",
       "    color:rgb(255, 94, 0);\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td.value pre {\n",
       "    color:rgb(255, 94, 0) !important;\n",
       "    background-color: transparent !important;\n",
       "}\n",
       "\n",
       ".default td {\n",
       "    color: black;\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td i,\n",
       ".default td i {\n",
       "    color: black;\n",
       "}\n",
       "\n",
       ".copy-paste-icon {\n",
       "    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);\n",
       "    background-repeat: no-repeat;\n",
       "    background-size: 14px 14px;\n",
       "    background-position: 0;\n",
       "    display: inline-block;\n",
       "    width: 14px;\n",
       "    height: 14px;\n",
       "    cursor: pointer;\n",
       "}\n",
       "</style><body><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StandardScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>StandardScaler</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('copy',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">copy&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('with_mean',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">with_mean&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('with_std',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">with_std&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.fa-regular.fa-copy').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling.textContent.trim();\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "</script></body>"
      ],
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "454697e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse transform both original and reconstructed columns if they are scaled\n",
    "df_with_predictions[[\"z_m_recon\"]] = data_scaler.inverse_transform(\n",
    "    df_with_predictions[[\"z_m_recon\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f05a6762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating Plots for 10 Random Validation Files (ML Model) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82d70aa300c348b29fbab1ba643a711b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating ML plots:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML Plots saved to: detected_wakes_LSTM_AE/wake_plots/w150,s5,th70\n"
     ]
    }
   ],
   "source": [
    "# --- Plotting 10 Random Validation Files for ML Model ---\n",
    "if lstm_autoencoder and dataset_splits['valid'] and data_scaler:\n",
    "    print(\"\\n--- Generating Plots for 10 Random Validation Files (ML Model) ---\")\n",
    "\n",
    "    valid_files_processed_path = os.path.join(output_base_dir_ml, 'valid')\n",
    "    if not os.path.exists(valid_files_processed_path):\n",
    "        print(f\"Error: Validation output directory '{valid_files_processed_path}' not found. Cannot generate plots.\")\n",
    "    else:\n",
    "        validation_output_files = glob.glob(os.path.join(valid_files_processed_path, '*.csv'))\n",
    "\n",
    "        if len(validation_output_files) == 0:\n",
    "            print(f\"No processed files found in '{valid_files_processed_path}'. Cannot generate plots.\")\n",
    "        else:\n",
    "            num_plots_to_generate = min(10, len(validation_output_files))\n",
    "            files_to_plot = random.sample(validation_output_files, num_plots_to_generate)\n",
    "\n",
    "            os.makedirs(plots_output_dir_ml, exist_ok=True)\n",
    "\n",
    "            for file_path in tqdm(files_to_plot, desc=\"Generating ML plots\"):\n",
    "                try:\n",
    "                    df_to_plot = pd.read_csv(file_path)\n",
    "                    plot_file_name = os.path.basename(file_path)\n",
    "                    plot_wake_detection(df_to_plot, plot_file_name, save_dir=plots_output_dir_ml)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error plotting {file_path}: {e}\")\n",
    "            print(f\"ML Plots saved to: {plots_output_dir_ml}\")\n",
    "else:\n",
    "    print(\"\\nSkipping ML plotting as the LSTM Autoencoder was not trained or data not processed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db773771",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c40df3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
