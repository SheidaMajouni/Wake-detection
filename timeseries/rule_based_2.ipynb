{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8ec04d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fa339b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# Based on observations of the provided CSV data (z_m values are typically small)\n",
    "DETECTION_THRESHOLD_MULTIPLIER = 1.5  # Multiplier for STD to define peak detection threshold\n",
    "EXIT_THRESHOLD_MULTIPLIER = 1.0       # Multiplier for STD to define wake end threshold\n",
    "MIN_WAKE_DURATION_SECONDS = 0.5       # Minimum duration in seconds for a detected event to be considered a wake\n",
    "MERGE_GAP_SECONDS = 1.5               # Maximum gap between two wake segments to merge them\n",
    "BUFFER_SECONDS = 0.1                  # Small buffer to add at the start/end of detected segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7cff9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads the CSV data and performs basic preprocessing.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        # Ensure 't_s' is sorted, though typically it is in time series data\n",
    "        df = df.sort_values(by='t_s').reset_index(drop=True)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or preprocessing data from {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a24799ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ground_truth_wakes(df):\n",
    "    \"\"\"\n",
    "    Extracts ground truth wake intervals from the 'wake_label' column.\n",
    "    Returns a list of (start_time, end_time) tuples.\n",
    "    \"\"\"\n",
    "    ground_truth_wakes = []\n",
    "    in_wake = False\n",
    "    wake_start_time = None\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        t_s = row['t_s']\n",
    "        wake_label = row['wake_label']\n",
    "\n",
    "        if wake_label == 1 and not in_wake:\n",
    "            wake_start_time = t_s\n",
    "            in_wake = True\n",
    "        elif wake_label == 0 and in_wake:\n",
    "            wake_end_time = t_s\n",
    "            ground_truth_wakes.append((wake_start_time, wake_end_time))\n",
    "            in_wake = False\n",
    "    \n",
    "    # Handle case where wake extends to the end of the time series\n",
    "    if in_wake:\n",
    "        ground_truth_wakes.append((wake_start_time, df['t_s'].iloc[-1]))\n",
    "\n",
    "    return ground_truth_wakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10885d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_wakes_rule_based(df):\n",
    "    \"\"\"\n",
    "    Detects wakes using a simple rule-based method based on z_m.\n",
    "    Returns a list of (start_time, end_time) tuples for detected wakes.\n",
    "    \"\"\"\n",
    "    z_m_abs = df['z_m'].abs()\n",
    "    \n",
    "    # Calculate Mean Absolute Deviation and Standard Deviation of z_m_abs for dynamic thresholds\n",
    "    mean_abs_z = z_m_abs.mean()\n",
    "    std_abs_z = z_m_abs.std()\n",
    "\n",
    "    if std_abs_z == 0: # Avoid division by zero if z_m is constant\n",
    "        # print(\"Warning: Standard deviation of z_m is zero. Cannot apply dynamic thresholds.\")\n",
    "        return []\n",
    "\n",
    "    # Define thresholds\n",
    "    detection_threshold = mean_abs_z + DETECTION_THRESHOLD_MULTIPLIER * std_abs_z\n",
    "    exit_threshold = mean_abs_z + EXIT_THRESHOLD_MULTIPLIER * std_abs_z\n",
    "\n",
    "    detected_segments = []\n",
    "    in_segment = False\n",
    "    segment_start_idx = None\n",
    "    \n",
    "    # Calculate time step for buffering\n",
    "    time_diffs = df['t_s'].diff().dropna().unique()\n",
    "    if len(time_diffs) > 0:\n",
    "        avg_time_step = np.mean(time_diffs)\n",
    "    else:\n",
    "        avg_time_step = 0.001 # Default small time step if no diffs (e.g., single point)\n",
    "\n",
    "    buffer_points = int(BUFFER_SECONDS / avg_time_step) if avg_time_step > 0 else 0\n",
    "    min_wake_points = int(MIN_WAKE_DURATION_SECONDS / avg_time_step) if avg_time_step > 0 else 0\n",
    "    merge_gap_points = int(MERGE_GAP_SECONDS / avg_time_step) if avg_time_step > 0 else 0\n",
    "\n",
    "    # Step 1: Identify points above the detection threshold\n",
    "    is_above_detection_threshold = z_m_abs > detection_threshold\n",
    "\n",
    "    # Step 2: Identify continuous segments using a state machine\n",
    "    for i in range(len(df)):\n",
    "        current_t_s = df['t_s'].iloc[i]\n",
    "        \n",
    "        if is_above_detection_threshold.iloc[i] and not in_segment:\n",
    "            segment_start_idx = i\n",
    "            in_segment = True\n",
    "        elif not is_above_detection_threshold.iloc[i] and in_segment:\n",
    "            # Simple check: if current value is below exit threshold, mark segment end\n",
    "            if z_m_abs.iloc[i] < exit_threshold:\n",
    "                segment_end_idx = i\n",
    "                # Apply buffer\n",
    "                actual_start_idx = max(0, segment_start_idx - buffer_points)\n",
    "                actual_end_idx = min(len(df) - 1, segment_end_idx + buffer_points)\n",
    "                \n",
    "                detected_segments.append((actual_start_idx, actual_end_idx))\n",
    "                in_segment = False\n",
    "    \n",
    "    # Handle case where wake extends to the end of the time series\n",
    "    if in_segment:\n",
    "        actual_start_idx = max(0, segment_start_idx - buffer_points)\n",
    "        actual_end_idx = len(df) - 1 # Extends to the end\n",
    "        detected_segments.append((actual_start_idx, actual_end_idx))\n",
    "\n",
    "    # Convert detected segments from indices to time and merge close segments\n",
    "    predicted_wakes_time = []\n",
    "    if detected_segments:\n",
    "        # Initial conversion to time values\n",
    "        current_wake_start_time = df['t_s'].iloc[detected_segments[0][0]]\n",
    "        current_wake_end_time = df['t_s'].iloc[detected_segments[0][1]]\n",
    "\n",
    "        for i in range(1, len(detected_segments)):\n",
    "            next_segment_start_time = df['t_s'].iloc[detected_segments[i][0]]\n",
    "            next_segment_end_time = df['t_s'].iloc[detected_segments[i][1]]\n",
    "\n",
    "            # Check if the gap between current and next segment is small enough to merge\n",
    "            if (next_segment_start_time - current_wake_end_time) <= MERGE_GAP_SECONDS:\n",
    "                current_wake_end_time = max(current_wake_end_time, next_segment_end_time)\n",
    "            else:\n",
    "                predicted_wakes_time.append((current_wake_start_time, current_wake_end_time))\n",
    "                current_wake_start_time = next_segment_start_time\n",
    "                current_wake_end_time = next_segment_end_time\n",
    "        \n",
    "        predicted_wakes_time.append((current_wake_start_time, current_wake_end_time))\n",
    "    \n",
    "    # Filter out very short wakes\n",
    "    final_predicted_wakes = []\n",
    "    for start, end in predicted_wakes_time:\n",
    "        if (end - start) >= MIN_WAKE_DURATION_SECONDS:\n",
    "            final_predicted_wakes.append((start, end))\n",
    "\n",
    "    return final_predicted_wakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f21e643d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_predicted_labels_to_df(df, predicted_wakes_intervals):\n",
    "    \"\"\"\n",
    "    Adds a 'predicted_wake_label' column to the DataFrame based on detected wake intervals.\n",
    "    \"\"\"\n",
    "    # Initialize all predicted_wake_label to 0 (no wake)\n",
    "    df['predicted_wake_label'] = 0\n",
    "\n",
    "    # Iterate through detected intervals and set label to 1 for timestamps within them\n",
    "    for start_time, end_time in predicted_wakes_intervals:\n",
    "        # Find indices where t_s is within the detected wake interval\n",
    "        # Using vectorized operation for efficiency\n",
    "        mask = (df['t_s'] >= start_time) & (df['t_s'] < end_time)\n",
    "        df.loc[mask, 'predicted_wake_label'] = 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7931df9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(interval1, interval2):\n",
    "    \"\"\"\n",
    "    Calculates the Intersection over Union (IoU) of two time intervals.\n",
    "    Intervals are (start, end) tuples.\n",
    "    \"\"\"\n",
    "    start1, end1 = interval1\n",
    "    start2, end2 = interval2\n",
    "\n",
    "    # Calculate intersection\n",
    "    intersection_start = max(start1, start2)\n",
    "    intersection_end = min(end1, end2)\n",
    "    \n",
    "    intersection_duration = max(0, intersection_end - intersection_start)\n",
    "\n",
    "    # Calculate union\n",
    "    union_duration = (max(end1, end2) - min(start1, start2))\n",
    "\n",
    "    if union_duration == 0:\n",
    "        return 0.0 # No union, no overlap\n",
    "    \n",
    "    return intersection_duration / union_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3052adb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_detection(ground_truth_wakes, predicted_wakes, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluates the detection performance based on IoU.\n",
    "    Returns TP, FP, FN, Accuracy, Precision, Recall, F1-Score.\n",
    "    \"\"\"\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    # Keep track of ground truth wakes that have been matched\n",
    "    matched_gt_indices = set()\n",
    "    # Keep track of predicted wakes that have been used as TP\n",
    "    used_pred_indices = set()\n",
    "\n",
    "    # Iterate through predicted wakes to find matches\n",
    "    for pred_idx, pred_wake in enumerate(predicted_wakes):\n",
    "        # Find the best ground truth match for the current predicted wake\n",
    "        best_iou_for_pred = 0.0\n",
    "        potential_gt_idx_for_pred = -1\n",
    "\n",
    "        for gt_idx, gt_wake in enumerate(ground_truth_wakes):\n",
    "            if gt_idx in matched_gt_indices: # Ground truth already matched, skip\n",
    "                continue\n",
    "\n",
    "            iou = calculate_iou(gt_wake, pred_wake)\n",
    "            if iou > best_iou_for_pred:\n",
    "                best_iou_for_pred = iou\n",
    "                potential_gt_idx_for_pred = gt_idx\n",
    "        \n",
    "        # If a predicted wake has a good enough overlap with an unmatched ground truth wake\n",
    "        if best_iou_for_pred >= iou_threshold and potential_gt_idx_for_pred != -1:\n",
    "            tp += 1\n",
    "            matched_gt_indices.add(potential_gt_idx_for_pred)\n",
    "            used_pred_indices.add(pred_idx) # Mark this predicted wake as used for a TP\n",
    "        else:\n",
    "            fp += 1 # This predicted wake didn't match any unmatched ground truth wake sufficiently\n",
    "\n",
    "    # Calculate False Negatives: ground truth wakes that were not matched by any predicted wake\n",
    "    fn = len(ground_truth_wakes) - len(matched_gt_indices)\n",
    "\n",
    "    total_ground_truth = len(ground_truth_wakes)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    # Accuracy here refers to the proportion of correctly detected ground truth wakes\n",
    "    accuracy = tp / total_ground_truth if total_ground_truth > 0 else 0.0\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    \n",
    "    if (precision + recall) == 0:\n",
    "        f1_score = 0.0\n",
    "    else:\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    return tp, fp, fn, accuracy, precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c1fa375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Dataset Evaluation and Prediction Saving ---\n",
      "\n",
      "--- Processing TRAIN Set (9909 files) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train files: 100%|██████████| 9909/9909 [27:30<00:00,  6.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TRAIN Evaluation Results ---\n",
      "  Total Ground Truth Wakes: 9672\n",
      "  Total Predicted Wakes: 303561\n",
      "  True Positives (TP): 1799\n",
      "  False Positives (FP): 301762\n",
      "  False Negatives (FN): 7873\n",
      "  Accuracy (TP / Total GT): 0.1860\n",
      "  Precision: 0.0059\n",
      "  Recall: 0.1860\n",
      "  F1-Score: 0.0115\n",
      "\n",
      "--- Processing VALID Set (3411 files) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing valid files: 100%|██████████| 3411/3411 [09:29<00:00,  5.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- VALID Evaluation Results ---\n",
      "  Total Ground Truth Wakes: 3332\n",
      "  Total Predicted Wakes: 104902\n",
      "  True Positives (TP): 600\n",
      "  False Positives (FP): 104302\n",
      "  False Negatives (FN): 2732\n",
      "  Accuracy (TP / Total GT): 0.1801\n",
      "  Precision: 0.0057\n",
      "  Recall: 0.1801\n",
      "  F1-Score: 0.0111\n",
      "\n",
      "--- Processing TEST Set (6120 files) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test files: 100%|██████████| 6120/6120 [17:17<00:00,  5.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TEST Evaluation Results ---\n",
      "  Total Ground Truth Wakes: 3343\n",
      "  Total Predicted Wakes: 212650\n",
      "  True Positives (TP): 626\n",
      "  False Positives (FP): 212024\n",
      "  False Negatives (FN): 2717\n",
      "  Accuracy (TP / Total GT): 0.1873\n",
      "  Precision: 0.0029\n",
      "  Recall: 0.1873\n",
      "  F1-Score: 0.0058\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 82\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Overall evaluation\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m overall_ground_truth_wakes \u001b[38;5;129;01mor\u001b[39;00m overall_predicted_wakes:\n\u001b[0;32m---> 82\u001b[0m     tp_overall, fp_overall, fn_overall, accuracy_overall, precision_overall, recall_overall, f1_score_overall \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_detection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverall_ground_truth_wakes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverall_predicted_wakes\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Overall Dataset Evaluation Results ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Ground Truth Wakes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(overall_ground_truth_wakes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 25\u001b[0m, in \u001b[0;36mevaluate_detection\u001b[0;34m(ground_truth_wakes, predicted_wakes, iou_threshold)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gt_idx \u001b[38;5;129;01min\u001b[39;00m matched_gt_indices: \u001b[38;5;66;03m# Ground truth already matched, skip\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m iou \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_iou\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt_wake\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_wake\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m iou \u001b[38;5;241m>\u001b[39m best_iou_for_pred:\n\u001b[1;32m     27\u001b[0m     best_iou_for_pred \u001b[38;5;241m=\u001b[39m iou\n",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m, in \u001b[0;36mcalculate_iou\u001b[0;34m(interval1, interval2)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcalculate_iou\u001b[39m(interval1, interval2):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    Calculates the Intersection over Union (IoU) of two time intervals.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    Intervals are (start, end) tuples.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     start1, end1 \u001b[38;5;241m=\u001b[39m interval1\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    base_data_dir = 'processed_ts' \n",
    "    output_base_dir = 'detected_wakes_rule_based2' \n",
    "\n",
    "    dataset_splits = {\n",
    "        'train': [], \n",
    "        'valid': [], \n",
    "        'test': []   \n",
    "    }\n",
    "\n",
    "    # Dynamically find files in your actual directory structure\n",
    "    for split_name in dataset_splits.keys():\n",
    "        split_path = os.path.join(base_data_dir, split_name)\n",
    "        if os.path.exists(split_path):\n",
    "            dataset_splits[split_name] = glob.glob(os.path.join(split_path, '*.csv'))\n",
    "        else:\n",
    "            print(f\"Warning: Directory '{split_path}' not found. Skipping {split_name} split.\")\n",
    "            dataset_splits[split_name] = [] # Ensure it's an empty list if directory doesn't exist\n",
    "\n",
    "\n",
    "    print(\"--- Starting Dataset Evaluation and Prediction Saving ---\")\n",
    "\n",
    "    overall_ground_truth_wakes = []\n",
    "    overall_predicted_wakes = []\n",
    "\n",
    "    for split_name, file_paths in dataset_splits.items():\n",
    "        if not file_paths:\n",
    "            print(f\"\\nNo files found for '{split_name}' split. Skipping evaluation for this split.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n--- Processing {split_name.upper()} Set ({len(file_paths)} files) ---\")\n",
    "        \n",
    "        split_ground_truth_wakes = []\n",
    "        split_predicted_wakes = []\n",
    "\n",
    "        # Wrap the file_paths iteration with tqdm for a progress bar\n",
    "        for file_path in tqdm(file_paths, desc=f\"Processing {split_name} files\"):\n",
    "            df = load_and_preprocess_data(file_path)\n",
    "            if df is not None:\n",
    "                gt_wakes = get_ground_truth_wakes(df)\n",
    "                pred_wake_intervals = detect_wakes_rule_based(df)\n",
    "                \n",
    "                df_with_predictions = assign_predicted_labels_to_df(df.copy(), pred_wake_intervals) \n",
    "                \n",
    "                split_ground_truth_wakes.extend(gt_wakes)\n",
    "                split_predicted_wakes.extend(pred_wake_intervals) \n",
    "                \n",
    "                relative_path = os.path.relpath(file_path, base_data_dir)\n",
    "                output_file_path = os.path.join(output_base_dir, relative_path)\n",
    "                \n",
    "                os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n",
    "                \n",
    "                df_with_predictions.to_csv(output_file_path, index=False)\n",
    "            else:\n",
    "                # Print errors for specific files outside of tqdm to not mess up progress bar\n",
    "                print(f\"\\n  Skipping processing for file: {file_path} due to load error.\")\n",
    "\n",
    "        if split_ground_truth_wakes or split_predicted_wakes:\n",
    "            tp, fp, fn, accuracy, precision, recall, f1_score = evaluate_detection(\n",
    "                split_ground_truth_wakes, split_predicted_wakes\n",
    "            )\n",
    "\n",
    "            print(f\"\\n--- {split_name.upper()} Evaluation Results ---\")\n",
    "            print(f\"  Total Ground Truth Wakes: {len(split_ground_truth_wakes)}\")\n",
    "            print(f\"  Total Predicted Wakes: {len(split_predicted_wakes)}\")\n",
    "            print(f\"  True Positives (TP): {tp}\")\n",
    "            print(f\"  False Positives (FP): {fp}\")\n",
    "            print(f\"  False Negatives (FN): {fn}\")\n",
    "            print(f\"  Accuracy (TP / Total GT): {accuracy:.4f}\")\n",
    "            print(f\"  Precision: {precision:.4f}\")\n",
    "            print(f\"  Recall: {recall:.4f}\")\n",
    "            print(f\"  F1-Score: {f1_score:.4f}\")\n",
    "            \n",
    "            overall_ground_truth_wakes.extend(split_ground_truth_wakes)\n",
    "            overall_predicted_wakes.extend(split_predicted_wakes)\n",
    "\n",
    "        else:\n",
    "            print(f\"  No ground truth or predicted wakes found in {split_name} set.\")\n",
    "\n",
    "    # Overall evaluation\n",
    "    if overall_ground_truth_wakes or overall_predicted_wakes:\n",
    "        tp_overall, fp_overall, fn_overall, accuracy_overall, precision_overall, recall_overall, f1_score_overall = evaluate_detection(\n",
    "            overall_ground_truth_wakes, overall_predicted_wakes\n",
    "        )\n",
    "\n",
    "        print(\"\\n--- Overall Dataset Evaluation Results ---\")\n",
    "        print(f\"Total Ground Truth Wakes: {len(overall_ground_truth_wakes)}\")\n",
    "        print(f\"Total Predicted Wakes: {len(overall_predicted_wakes)}\")\n",
    "        print(f\"True Positives (TP): {tp_overall}\")\n",
    "        print(f\"False Positives (FP): {fp_overall}\")\n",
    "        print(f\"False Negatives (FN): {fn_overall}\")\n",
    "        print(f\"Accuracy (TP / Total GT): {accuracy_overall:.4f}\")\n",
    "        print(f\"Precision: {precision_overall:.4f}\")\n",
    "        print(f\"Recall: {recall_overall:.4f}\")\n",
    "        print(f\"F1-Score: {f1_score_overall:.4f}\")\n",
    "    else:\n",
    "        print(\"\\nNo ground truth or predicted wakes found across the entire dataset for overall evaluation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066990d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall evaluation\n",
    "if overall_ground_truth_wakes or overall_predicted_wakes:\n",
    "    tp_overall, fp_overall, fn_overall, accuracy_overall, precision_overall, recall_overall, f1_score_overall = evaluate_detection(\n",
    "        overall_ground_truth_wakes, overall_predicted_wakes\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Overall Dataset Evaluation Results ---\")\n",
    "    print(f\"Total Ground Truth Wakes: {len(overall_ground_truth_wakes)}\")\n",
    "    print(f\"Total Predicted Wakes: {len(overall_predicted_wakes)}\")\n",
    "    print(f\"True Positives (TP): {tp_overall}\")\n",
    "    print(f\"False Positives (FP): {fp_overall}\")\n",
    "    print(f\"False Negatives (FN): {fn_overall}\")\n",
    "    print(f\"Accuracy (TP / Total GT): {accuracy_overall:.4f}\")\n",
    "    print(f\"Precision: {precision_overall:.4f}\")\n",
    "    print(f\"Recall: {recall_overall:.4f}\")\n",
    "    print(f\"F1-Score: {f1_score_overall:.4f}\")\n",
    "else:\n",
    "    print(\"\\nNo ground truth or predicted wakes found across the entire dataset for overall evaluation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76a9c45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the results\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50a6a37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_base_dir = 'detected_wakes_rule_based2' # Directory where processed CSVs are saved\n",
    "plots_output_dir = 'wake_plots'          # Directory where plots will be saved\n",
    "\n",
    "# --- Plotting Function ---\n",
    "def plot_wake_detection(df, file_name, save_dir=None):\n",
    "    \"\"\"\n",
    "    Plots the time series data with ground truth and predicted wake labels.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing 't_s', 'z_m', 'wake_label',\n",
    "                           and 'predicted_wake_label' columns.\n",
    "        file_name (str): The name of the original CSV file for the plot title.\n",
    "        save_dir (str, optional): Directory to save the plots. If None, plots are shown.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Plot the entire time series in black\n",
    "    plt.plot(df['t_s'], df['z_m'], color='black', linewidth=0.8, label='z_m (Vertical Displacement)')\n",
    "\n",
    "    # Highlight Ground Truth Wakes (Red Background)\n",
    "    in_gt_wake = False\n",
    "    gt_wake_start = None\n",
    "    gt_label_added = False # Flag to add label only once\n",
    "    for i, row in df.iterrows():\n",
    "        if row['wake_label'] == 1 and not in_gt_wake:\n",
    "            gt_wake_start = row['t_s']\n",
    "            in_gt_wake = True\n",
    "        elif row['wake_label'] == 0 and in_gt_wake:\n",
    "            plt.axvspan(gt_wake_start, row['t_s'], color='red', alpha=0.3, label='Ground Truth Wake' if not gt_label_added else \"\")\n",
    "            gt_label_added = True\n",
    "            in_gt_wake = False\n",
    "    # Handle case where GT wake extends to the end of the series\n",
    "    if in_gt_wake:\n",
    "        plt.axvspan(gt_wake_start, df['t_s'].iloc[-1], color='red', alpha=0.3, label='Ground Truth Wake' if not gt_label_added else \"\")\n",
    "        gt_label_added = True\n",
    "\n",
    "    # Highlight Predicted Wakes (Green Background)\n",
    "    in_pred_wake = False\n",
    "    pred_wake_start = None\n",
    "    pred_label_added = False # Flag to add label only once\n",
    "    for i, row in df.iterrows():\n",
    "        if row['predicted_wake_label'] == 1 and not in_pred_wake:\n",
    "            pred_wake_start = row['t_s']\n",
    "            in_pred_wake = True\n",
    "        elif row['predicted_wake_label'] == 0 and in_pred_wake:\n",
    "            plt.axvspan(pred_wake_start, row['t_s'], color='green', alpha=0.3, label='Predicted Wake' if not pred_label_added else \"\")\n",
    "            pred_label_added = True\n",
    "            in_pred_wake = False\n",
    "    # Handle case where Predicted wake extends to the end of the series\n",
    "    if in_pred_wake:\n",
    "        plt.axvspan(pred_wake_start, df['t_s'].iloc[-1], color='green', alpha=0.3, label='Predicted Wake' if not pred_label_added else \"\")\n",
    "        pred_label_added = True\n",
    "\n",
    "    plt.title(f'Wake Detection for {file_name}', fontsize=16)\n",
    "    plt.xlabel('Time (s)', fontsize=12)\n",
    "    plt.ylabel('z_m (m)', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend()\n",
    "    plt.tight_layout() # Adjust plot to prevent labels from overlapping\n",
    "\n",
    "    if save_dir:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        plot_path = os.path.join(save_dir, f'{os.path.basename(file_name).replace(\".csv\", \"\")}_wake_plot.png')\n",
    "        plt.savefig(plot_path, dpi=300)\n",
    "        plt.close() # Close the plot to free memory\n",
    "    else:\n",
    "        plt.show() # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83401445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating Plots for 10 Random Validation Files ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating plots: 100%|██████████| 10/10 [00:10<00:00,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plots saved to: wake_plots\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Generating Plots for 10 Random Validation Files ---\")\n",
    "\n",
    "valid_files_processed_path = os.path.join(output_base_dir, 'valid')\n",
    "if not os.path.exists(valid_files_processed_path):\n",
    "    print(f\"Error: Validation output directory '{valid_files_processed_path}' not found. \"\n",
    "          \"Please ensure your previous code has run and created this directory.\")\n",
    "else:\n",
    "    validation_output_files = glob.glob(os.path.join(valid_files_processed_path, '*.csv'))\n",
    "    \n",
    "    if len(validation_output_files) == 0:\n",
    "        print(f\"No processed files found in '{valid_files_processed_path}'. Cannot generate plots.\")\n",
    "    else:\n",
    "        # Select up to 10 random files, or all if less than 10\n",
    "        num_plots_to_generate = min(10, len(validation_output_files))\n",
    "        files_to_plot = random.sample(validation_output_files, num_plots_to_generate)\n",
    "        \n",
    "        os.makedirs(plots_output_dir, exist_ok=True) # Create directory for plots\n",
    "\n",
    "        for file_path in tqdm(files_to_plot, desc=\"Generating plots\"):\n",
    "            try:\n",
    "                df_to_plot = pd.read_csv(file_path)\n",
    "                plot_file_name = os.path.basename(file_path)\n",
    "                plot_wake_detection(df_to_plot, plot_file_name, save_dir=plots_output_dir)\n",
    "            except Exception as e:\n",
    "                print(f\"Error plotting {file_path}: {e}\")\n",
    "        print(f\"Plots saved to: {plots_output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd9d5c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
