{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61a54e59",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01myaml\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import yaml\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76afd7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_yolo_to_fasterrcnn(yolo_data_dir, output_json_path, class_names_list):\n",
    "    \"\"\"\n",
    "    Converts YOLO format annotations to a JSON suitable for Faster R-CNN training.\n",
    "\n",
    "    Args:\n",
    "        yolo_data_dir (str): Path to the root of your Dataset directory (e.g., '../Dataset').\n",
    "        output_json_path (str): Path where the converted JSON file will be saved.\n",
    "        class_names_list (list): List of class names, where the index corresponds to the YOLO class_id.\n",
    "                                (e.g., ['wake']).\n",
    "    \"\"\"\n",
    "    all_annotations = []\n",
    "    image_id_counter = 0\n",
    "\n",
    "    for split in ['train', 'valid','test']: \n",
    "        image_dir = os.path.join(yolo_data_dir, 'images', split)\n",
    "        label_dir = os.path.join(yolo_data_dir, 'labels', split)\n",
    "\n",
    "        if not os.path.exists(image_dir) or not os.path.exists(label_dir):\n",
    "            print(f\"Skipping {split} split as directories not found: {image_dir}, {label_dir}\")\n",
    "            continue\n",
    "\n",
    "        image_files = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        print(f\"Processing {split} split with {len(image_files)} images...\")\n",
    "\n",
    "        for img_filename in image_files:\n",
    "            img_path = os.path.join(image_dir, img_filename)\n",
    "            label_filename = os.path.splitext(img_filename)[0] + '.txt'\n",
    "            label_path = os.path.join(label_dir, label_filename)\n",
    "\n",
    "            if not os.path.exists(label_path):\n",
    "                print(f\"Warning: No label file found for {img_filename}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Read image to get its dimensions\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                print(f\"Warning: Could not read image {img_path}. Skipping.\")\n",
    "                continue\n",
    "            height, width, _ = img.shape\n",
    "\n",
    "            image_annotations = {\n",
    "                \"image_id\": image_id_counter,\n",
    "                \"file_name\": os.path.join('images', split, img_filename), # CORRECTED LINE\n",
    "                \"width\": width,\n",
    "                \"height\": height,\n",
    "                \"annotations\": []\n",
    "            }\n",
    "\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) < 5:\n",
    "                        continue # Skip malformed lines\n",
    "\n",
    "                    class_id = int(parts[0])\n",
    "                    x_center_norm, y_center_norm, width_norm, height_norm = map(float, parts[1:5])\n",
    "\n",
    "                    # Convert normalized YOLO format to absolute [xmin, ymin, xmax, ymax]\n",
    "                    x_center = x_center_norm * width\n",
    "                    y_center = y_center_norm * height\n",
    "                    box_width = width_norm * width\n",
    "                    box_height = height_norm * height\n",
    "\n",
    "                    xmin = int(x_center - box_width / 2)\n",
    "                    ymin = int(y_center - box_height / 2)\n",
    "                    xmax = int(x_center + box_width / 2)\n",
    "                    ymax = int(y_center + box_height / 2)\n",
    "\n",
    "                    # Clamp coordinates to image boundaries\n",
    "                    xmin = max(0, xmin)\n",
    "                    ymin = max(0, ymin)\n",
    "                    xmax = min(width, xmax)\n",
    "                    ymax = min(height, ymax)\n",
    "\n",
    "                    # Calculate area\n",
    "                    area = (xmax - xmin) * (ymax - ymin)\n",
    "\n",
    "                    # IMPORTANT: Map YOLO class_id (0-indexed) to Faster R-CNN label (1-indexed)\n",
    "                    # For Faster R-CNN in torchvision, class 0 is BACKGROUND, so shift your actual classes by +1.\n",
    "                    fasterrcnn_label = class_id + 1\n",
    "\n",
    "                    annotation = {\n",
    "                        \"bbox\": [xmin, ymin, xmax, ymax],\n",
    "                        \"category_id\": fasterrcnn_label,\n",
    "                        \"area\": area,\n",
    "                        \"iscrowd\": 0 # Assuming no crowd annotations\n",
    "                    }\n",
    "                    image_annotations[\"annotations\"].append(annotation)\n",
    "\n",
    "            all_annotations.append(image_annotations)\n",
    "            image_id_counter += 1\n",
    "\n",
    "    # Save to JSON\n",
    "    with open(output_json_path, 'w') as f:\n",
    "        json.dump(all_annotations, f, indent=4)\n",
    "    print(f\"Conversion complete. Annotations saved to {output_json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d077551b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3099c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected class names: ['wake']\n",
      "Faster R-CNN class mapping (index 0 is background): ['__background__', 'wake']\n",
      "Processing train split with 9997 images...\n",
      "Processing valid split with 3443 images...\n",
      "Processing test split with 6165 images...\n",
      "Conversion complete. Annotations saved to ../Dataset/faster_rcnn_annotations.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Configuration ---\n",
    "    yaml_path = \"../Dataset/vessel_wakes.yaml\"\n",
    "    yolo_dataset_root = \"../Dataset\"\n",
    "    output_json = \"../Dataset/faster_rcnn_annotations.json\"\n",
    "\n",
    "    # Load class names from your YAML file\n",
    "    try:\n",
    "        with open(yaml_path, 'r') as f:\n",
    "            yaml_config = yaml.safe_load(f)\n",
    "            # Get the values from the 'names' dictionary\n",
    "            # and sort them by key to maintain the correct class ID order.\n",
    "            class_names_dict = yaml_config['names']\n",
    "            # Convert dictionary values to a list, ensuring order by class ID\n",
    "            class_names = [class_names_dict[i] for i in sorted(class_names_dict.keys())]\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {yaml_path} not found. Please check the path.\")\n",
    "        exit()\n",
    "    except KeyError:\n",
    "        print(\"Error: 'names' key not found in your YAML config. Ensure it defines your class names.\")\n",
    "        exit()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading YAML: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # Prepend a '__background__' class to match torchvision's convention\n",
    "    fasterrcnn_class_names = ['__background__'] + class_names\n",
    "    print(f\"Detected class names: {class_names}\")\n",
    "    print(f\"Faster R-CNN class mapping (index 0 is background): {fasterrcnn_class_names}\")\n",
    "\n",
    "    # Pass the list of class names to the conversion function\n",
    "    convert_yolo_to_fasterrcnn(yolo_dataset_root, output_json, class_names) # Passing class_names_list here is not strictly used by the function itself,\n",
    "                                                                             # but it's good practice to pass the correctly formatted list.\n",
    "                                                                             # The function primarily uses class_id + 1 for category_id.\n",
    "                                                                             # The list is mainly for printing and validation in this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f81e869",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
      "Collecting numpy<2.3.0,>=2 (from opencv-python)\n",
      "  Downloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (67.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m162.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m154.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy, opencv-python\n",
      "\u001b[2K  Attempting uninstall: numpy\n",
      "\u001b[2K    Found existing installation: numpy 1.26.4\n",
      "\u001b[2K    Uninstalling numpy-1.26.4:━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/2\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled numpy-1.26.4━━━━━\u001b[0m \u001b[32m0/2\u001b[0m [numpy]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [opencv-python]0m [opencv-python]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sagemaker 2.248.2 requires numpy==1.26.4, but you have numpy 2.2.6 which is incompatible.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-2.2.6 opencv-python-4.12.0.88\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bde5337",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (2025.7.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.2.0 (from torch)\n",
      "  Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl (766.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m171.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m143.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m107.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-cusparselt-cu12, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "\u001b[2K  Attempting uninstall: sympy━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/16\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K    Found existing installation: sympy 1.14.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/16\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K    Uninstalling sympy-1.14.0:0m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/16\u001b[0m [sympy]parselt-cu12]\n",
      "\u001b[2K      Successfully uninstalled sympy-1.14.0━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/16\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/16\u001b[0m [torch]m15/16\u001b[0m [torch]-cusolver-cu12]2]2]\n",
      "\u001b[1A\u001b[2KSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.6.0 triton-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db101edd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.21.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: torch==2.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torchvision) (2.6.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (4.14.1)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (2025.7.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
      "Downloading torchvision-0.21.0-cp310-cp310-manylinux1_x86_64.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchvision\n",
      "Successfully installed torchvision-0.21.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78a90b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b861661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WakeDetectionDataset(Dataset):\n",
    "    def __init__(self, root_dir, annotation_file, split, transform=None):\n",
    "        self.root_dir = root_dir # '../Dataset'\n",
    "        self.transform = transform\n",
    "        self.split = split # 'train' or 'valid'\n",
    "\n",
    "        with open(annotation_file, 'r') as f:\n",
    "            all_annotations = json.load(f)\n",
    "\n",
    "        # Filter annotations based on the specified split\n",
    "        expected_split_prefix = os.path.join('images', self.split) + os.sep\n",
    "        self.annotations = [\n",
    "            ann for ann in all_annotations\n",
    "            if ann['file_name'].startswith(expected_split_prefix)\n",
    "        ]\n",
    "\n",
    "        # Create a mapping from image_id to its annotations\n",
    "        self.img_data = {ann['image_id']: ann for ann in self.annotations}\n",
    "        self.image_ids = list(self.img_data.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx]\n",
    "        data = self.img_data[img_id]\n",
    "\n",
    "        img_path = os.path.join(self.root_dir, data['file_name'])\n",
    "        image = Image.open(img_path).convert(\"RGB\") # Use PIL for image loading\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        areas = []\n",
    "        iscrowd = []\n",
    "\n",
    "        for annotation in data['annotations']:\n",
    "            boxes.append(annotation['bbox']) # [xmin, ymin, xmax, ymax]\n",
    "            labels.append(annotation['category_id']) # 1-indexed labels\n",
    "            areas.append(annotation['area'])\n",
    "            iscrowd.append(annotation['iscrowd'])\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "        iscrowd = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
    "\n",
    "        # Handle cases where an image might have no annotations (empty boxes/labels)\n",
    "        if boxes.numel() == 0:\n",
    "            # Create dummy tensors for consistency if no objects are present\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros((0,), dtype=torch.int64)\n",
    "            areas = torch.zeros((0,), dtype=torch.float32)\n",
    "            iscrowd = torch.zeros((0,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = torch.tensor([img_id])\n",
    "        target[\"area\"] = areas\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image, target = self.transform(image, target) # Custom transform if it handles target\n",
    "\n",
    "        return image, target\n",
    "\n",
    "# Helper function for data collation (required for object detection datasets)\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# Example of a transform (basic, can be expanded)\n",
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __call__(self, image, target):\n",
    "        image = F.to_tensor(image)\n",
    "        return image, target\n",
    "\n",
    "# You would add more transforms here, e.g., RandomHorizontalFlip, Resize, Normalize\n",
    "# For object detection, transformations usually need to apply to both image and bounding boxes.\n",
    "# This requires custom transform classes or libraries like Albumentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1ecac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Inferred 1 custom classes. Setting NUM_CLASSES for Faster R-CNN to 2 (including background).\n",
      "Train dataset size: 9997\n",
      "Validation dataset size: 3443\n",
      "Starting training...\n",
      "Epoch: 1/50, Iter: 10/2500, Train Loss: 0.2283\n",
      "Epoch: 1/50, Iter: 20/2500, Train Loss: 0.1431\n",
      "Epoch: 1/50, Iter: 30/2500, Train Loss: 0.1928\n",
      "Epoch: 1/50, Iter: 40/2500, Train Loss: 0.1913\n",
      "Epoch: 1/50, Iter: 50/2500, Train Loss: 0.2420\n",
      "Epoch: 1/50, Iter: 60/2500, Train Loss: 0.1634\n",
      "Epoch: 1/50, Iter: 70/2500, Train Loss: 0.2237\n",
      "Epoch: 1/50, Iter: 80/2500, Train Loss: 0.1444\n",
      "Epoch: 1/50, Iter: 90/2500, Train Loss: 0.1130\n",
      "Epoch: 1/50, Iter: 100/2500, Train Loss: 0.1830\n",
      "Epoch: 1/50, Iter: 110/2500, Train Loss: 0.1909\n",
      "Epoch: 1/50, Iter: 120/2500, Train Loss: 0.1044\n",
      "Epoch: 1/50, Iter: 130/2500, Train Loss: 0.1682\n",
      "Epoch: 1/50, Iter: 140/2500, Train Loss: 0.1290\n",
      "Epoch: 1/50, Iter: 150/2500, Train Loss: 0.1683\n",
      "Epoch: 1/50, Iter: 160/2500, Train Loss: 0.1953\n",
      "Epoch: 1/50, Iter: 170/2500, Train Loss: 0.3509\n",
      "Epoch: 1/50, Iter: 180/2500, Train Loss: 0.1553\n",
      "Epoch: 1/50, Iter: 190/2500, Train Loss: 0.1680\n",
      "Epoch: 1/50, Iter: 200/2500, Train Loss: 0.0604\n",
      "Epoch: 1/50, Iter: 210/2500, Train Loss: 0.1370\n",
      "Epoch: 1/50, Iter: 220/2500, Train Loss: 0.2001\n",
      "Epoch: 1/50, Iter: 230/2500, Train Loss: 0.0837\n",
      "Epoch: 1/50, Iter: 240/2500, Train Loss: 0.1471\n",
      "Epoch: 1/50, Iter: 250/2500, Train Loss: 0.1758\n",
      "Epoch: 1/50, Iter: 260/2500, Train Loss: 0.4409\n",
      "Epoch: 1/50, Iter: 270/2500, Train Loss: 0.1367\n",
      "Epoch: 1/50, Iter: 280/2500, Train Loss: 0.0879\n",
      "Epoch: 1/50, Iter: 290/2500, Train Loss: 0.1567\n",
      "Epoch: 1/50, Iter: 300/2500, Train Loss: 0.1479\n",
      "Epoch: 1/50, Iter: 310/2500, Train Loss: 0.1553\n",
      "Epoch: 1/50, Iter: 320/2500, Train Loss: 0.1489\n",
      "Epoch: 1/50, Iter: 330/2500, Train Loss: 0.0917\n",
      "Epoch: 1/50, Iter: 340/2500, Train Loss: 0.1428\n",
      "Epoch: 1/50, Iter: 350/2500, Train Loss: 0.1499\n",
      "Epoch: 1/50, Iter: 360/2500, Train Loss: 0.2714\n",
      "Epoch: 1/50, Iter: 370/2500, Train Loss: 0.0366\n",
      "Epoch: 1/50, Iter: 380/2500, Train Loss: 0.1401\n",
      "Epoch: 1/50, Iter: 390/2500, Train Loss: 0.1190\n",
      "Epoch: 1/50, Iter: 400/2500, Train Loss: 0.2115\n",
      "Epoch: 1/50, Iter: 410/2500, Train Loss: 0.1016\n",
      "Epoch: 1/50, Iter: 420/2500, Train Loss: 0.2754\n",
      "Epoch: 1/50, Iter: 430/2500, Train Loss: 0.1421\n",
      "Epoch: 1/50, Iter: 440/2500, Train Loss: 0.1337\n",
      "Epoch: 1/50, Iter: 450/2500, Train Loss: 0.1063\n",
      "Epoch: 1/50, Iter: 460/2500, Train Loss: 0.0963\n",
      "Epoch: 1/50, Iter: 470/2500, Train Loss: 0.1921\n",
      "Epoch: 1/50, Iter: 480/2500, Train Loss: 0.1041\n",
      "Epoch: 1/50, Iter: 490/2500, Train Loss: 0.0629\n",
      "Epoch: 1/50, Iter: 500/2500, Train Loss: 0.0663\n",
      "Epoch: 1/50, Iter: 510/2500, Train Loss: 0.0663\n",
      "Epoch: 1/50, Iter: 520/2500, Train Loss: 0.1503\n",
      "Epoch: 1/50, Iter: 530/2500, Train Loss: 0.0783\n",
      "Epoch: 1/50, Iter: 540/2500, Train Loss: 0.0926\n",
      "Epoch: 1/50, Iter: 550/2500, Train Loss: 0.1658\n",
      "Epoch: 1/50, Iter: 560/2500, Train Loss: 0.1553\n",
      "Epoch: 1/50, Iter: 570/2500, Train Loss: 0.0509\n",
      "Epoch: 1/50, Iter: 580/2500, Train Loss: 0.1895\n",
      "Epoch: 1/50, Iter: 590/2500, Train Loss: 0.1378\n",
      "Epoch: 1/50, Iter: 600/2500, Train Loss: 0.1324\n",
      "Epoch: 1/50, Iter: 610/2500, Train Loss: 0.1067\n",
      "Epoch: 1/50, Iter: 620/2500, Train Loss: 0.0878\n",
      "Epoch: 1/50, Iter: 630/2500, Train Loss: 0.0603\n",
      "Epoch: 1/50, Iter: 640/2500, Train Loss: 0.0771\n",
      "Epoch: 1/50, Iter: 650/2500, Train Loss: 0.0820\n",
      "Epoch: 1/50, Iter: 660/2500, Train Loss: 0.1209\n",
      "Epoch: 1/50, Iter: 670/2500, Train Loss: 0.0989\n",
      "Epoch: 1/50, Iter: 680/2500, Train Loss: 0.1176\n",
      "Epoch: 1/50, Iter: 690/2500, Train Loss: 0.0971\n",
      "Epoch: 1/50, Iter: 700/2500, Train Loss: 0.1805\n",
      "Epoch: 1/50, Iter: 710/2500, Train Loss: 0.1229\n",
      "Epoch: 1/50, Iter: 720/2500, Train Loss: 0.0866\n",
      "Epoch: 1/50, Iter: 730/2500, Train Loss: 0.0651\n",
      "Epoch: 1/50, Iter: 740/2500, Train Loss: 0.0910\n",
      "Epoch: 1/50, Iter: 750/2500, Train Loss: 0.0764\n",
      "Epoch: 1/50, Iter: 760/2500, Train Loss: 0.0929\n",
      "Epoch: 1/50, Iter: 770/2500, Train Loss: 0.1306\n",
      "Epoch: 1/50, Iter: 780/2500, Train Loss: 0.1253\n",
      "Epoch: 1/50, Iter: 790/2500, Train Loss: 0.1925\n",
      "Epoch: 1/50, Iter: 800/2500, Train Loss: 0.1176\n",
      "Epoch: 1/50, Iter: 810/2500, Train Loss: 0.0906\n",
      "Epoch: 1/50, Iter: 820/2500, Train Loss: 0.0934\n",
      "Epoch: 1/50, Iter: 830/2500, Train Loss: 0.1874\n",
      "Epoch: 1/50, Iter: 840/2500, Train Loss: 0.0870\n",
      "Epoch: 1/50, Iter: 850/2500, Train Loss: 0.0535\n",
      "Epoch: 1/50, Iter: 860/2500, Train Loss: 0.0892\n",
      "Epoch: 1/50, Iter: 870/2500, Train Loss: 0.0553\n",
      "Epoch: 1/50, Iter: 880/2500, Train Loss: 0.0960\n",
      "Epoch: 1/50, Iter: 890/2500, Train Loss: 0.0875\n",
      "Epoch: 1/50, Iter: 900/2500, Train Loss: 0.0465\n",
      "Epoch: 1/50, Iter: 910/2500, Train Loss: 0.0782\n",
      "Epoch: 1/50, Iter: 920/2500, Train Loss: 0.1357\n",
      "Epoch: 1/50, Iter: 930/2500, Train Loss: 0.1064\n",
      "Epoch: 1/50, Iter: 940/2500, Train Loss: 0.1115\n",
      "Epoch: 1/50, Iter: 950/2500, Train Loss: 0.1059\n",
      "Epoch: 1/50, Iter: 960/2500, Train Loss: 0.1080\n",
      "Epoch: 1/50, Iter: 970/2500, Train Loss: 0.1417\n",
      "Epoch: 1/50, Iter: 980/2500, Train Loss: 0.0970\n",
      "Epoch: 1/50, Iter: 990/2500, Train Loss: 0.0903\n",
      "Epoch: 1/50, Iter: 1000/2500, Train Loss: 0.1783\n",
      "Epoch: 1/50, Iter: 1010/2500, Train Loss: 0.0329\n",
      "Epoch: 1/50, Iter: 1020/2500, Train Loss: 0.1158\n",
      "Epoch: 1/50, Iter: 1030/2500, Train Loss: 0.1711\n",
      "Epoch: 1/50, Iter: 1040/2500, Train Loss: 0.1040\n",
      "Epoch: 1/50, Iter: 1050/2500, Train Loss: 0.3178\n",
      "Epoch: 1/50, Iter: 1060/2500, Train Loss: 0.1246\n",
      "Epoch: 1/50, Iter: 1070/2500, Train Loss: 0.0706\n",
      "Epoch: 1/50, Iter: 1080/2500, Train Loss: 0.1440\n",
      "Epoch: 1/50, Iter: 1090/2500, Train Loss: 0.0836\n",
      "Epoch: 1/50, Iter: 1100/2500, Train Loss: 0.1579\n",
      "Epoch: 1/50, Iter: 1110/2500, Train Loss: 0.1385\n",
      "Epoch: 1/50, Iter: 1120/2500, Train Loss: 0.0881\n",
      "Epoch: 1/50, Iter: 1130/2500, Train Loss: 0.1069\n",
      "Epoch: 1/50, Iter: 1140/2500, Train Loss: 0.1317\n",
      "Epoch: 1/50, Iter: 1150/2500, Train Loss: 0.2333\n",
      "Epoch: 1/50, Iter: 1160/2500, Train Loss: 0.1210\n",
      "Epoch: 1/50, Iter: 1170/2500, Train Loss: 0.1303\n",
      "Epoch: 1/50, Iter: 1180/2500, Train Loss: 0.2393\n",
      "Epoch: 1/50, Iter: 1190/2500, Train Loss: 0.0856\n",
      "Epoch: 1/50, Iter: 1200/2500, Train Loss: 0.0993\n",
      "Epoch: 1/50, Iter: 1210/2500, Train Loss: 0.0719\n",
      "Epoch: 1/50, Iter: 1220/2500, Train Loss: 0.0535\n",
      "Epoch: 1/50, Iter: 1230/2500, Train Loss: 0.1451\n",
      "Epoch: 1/50, Iter: 1240/2500, Train Loss: 0.1463\n",
      "Epoch: 1/50, Iter: 1250/2500, Train Loss: 0.0885\n",
      "Epoch: 1/50, Iter: 1260/2500, Train Loss: 0.1638\n",
      "Epoch: 1/50, Iter: 1270/2500, Train Loss: 0.1209\n",
      "Epoch: 1/50, Iter: 1280/2500, Train Loss: 0.1245\n",
      "Epoch: 1/50, Iter: 1290/2500, Train Loss: 0.0820\n",
      "Epoch: 1/50, Iter: 1300/2500, Train Loss: 0.0942\n",
      "Epoch: 1/50, Iter: 1310/2500, Train Loss: 0.1011\n",
      "Epoch: 1/50, Iter: 1320/2500, Train Loss: 0.1528\n",
      "Epoch: 1/50, Iter: 1330/2500, Train Loss: 0.0652\n",
      "Epoch: 1/50, Iter: 1340/2500, Train Loss: 0.1617\n",
      "Epoch: 1/50, Iter: 1350/2500, Train Loss: 0.1247\n",
      "Epoch: 1/50, Iter: 1360/2500, Train Loss: 0.1263\n",
      "Epoch: 1/50, Iter: 1370/2500, Train Loss: 0.1191\n",
      "Epoch: 1/50, Iter: 1380/2500, Train Loss: 0.1048\n",
      "Epoch: 1/50, Iter: 1390/2500, Train Loss: 0.1032\n",
      "Epoch: 1/50, Iter: 1400/2500, Train Loss: 0.2385\n",
      "Epoch: 1/50, Iter: 1410/2500, Train Loss: 0.1374\n",
      "Epoch: 1/50, Iter: 1420/2500, Train Loss: 0.2042\n",
      "Epoch: 1/50, Iter: 1430/2500, Train Loss: 0.0916\n",
      "Epoch: 1/50, Iter: 1440/2500, Train Loss: 0.1384\n",
      "Epoch: 1/50, Iter: 1450/2500, Train Loss: 0.1026\n",
      "Epoch: 1/50, Iter: 1460/2500, Train Loss: 0.0769\n",
      "Epoch: 1/50, Iter: 1470/2500, Train Loss: 0.1138\n",
      "Epoch: 1/50, Iter: 1480/2500, Train Loss: 0.0955\n",
      "Epoch: 1/50, Iter: 1490/2500, Train Loss: 0.1506\n",
      "Epoch: 1/50, Iter: 1500/2500, Train Loss: 0.0930\n",
      "Epoch: 1/50, Iter: 1510/2500, Train Loss: 0.0719\n",
      "Epoch: 1/50, Iter: 1520/2500, Train Loss: 0.0921\n",
      "Epoch: 1/50, Iter: 1530/2500, Train Loss: 0.1044\n",
      "Epoch: 1/50, Iter: 1540/2500, Train Loss: 0.1686\n",
      "Epoch: 1/50, Iter: 1550/2500, Train Loss: 0.0842\n",
      "Epoch: 1/50, Iter: 1560/2500, Train Loss: 0.0514\n",
      "Epoch: 1/50, Iter: 1570/2500, Train Loss: 0.1215\n",
      "Epoch: 1/50, Iter: 1580/2500, Train Loss: 0.1761\n",
      "Epoch: 1/50, Iter: 1590/2500, Train Loss: 0.0999\n",
      "Epoch: 1/50, Iter: 1600/2500, Train Loss: 0.0911\n",
      "Epoch: 1/50, Iter: 1610/2500, Train Loss: 0.1259\n",
      "Epoch: 1/50, Iter: 1620/2500, Train Loss: 0.1185\n",
      "Epoch: 1/50, Iter: 1630/2500, Train Loss: 0.1180\n",
      "Epoch: 1/50, Iter: 1640/2500, Train Loss: 0.0967\n",
      "Epoch: 1/50, Iter: 1650/2500, Train Loss: 0.1013\n",
      "Epoch: 1/50, Iter: 1660/2500, Train Loss: 0.1935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50, Iter: 1670/2500, Train Loss: 0.1479\n",
      "Epoch: 1/50, Iter: 1680/2500, Train Loss: 0.0940\n",
      "Epoch: 1/50, Iter: 1690/2500, Train Loss: 0.1373\n",
      "Epoch: 1/50, Iter: 1700/2500, Train Loss: 0.0928\n",
      "Epoch: 1/50, Iter: 1710/2500, Train Loss: 0.0745\n",
      "Epoch: 1/50, Iter: 1720/2500, Train Loss: 0.0831\n",
      "Epoch: 1/50, Iter: 1730/2500, Train Loss: 0.1204\n",
      "Epoch: 1/50, Iter: 1740/2500, Train Loss: 0.0843\n",
      "Epoch: 1/50, Iter: 1750/2500, Train Loss: 0.0682\n",
      "Epoch: 1/50, Iter: 1760/2500, Train Loss: 0.1529\n",
      "Epoch: 1/50, Iter: 1770/2500, Train Loss: 0.1473\n",
      "Epoch: 1/50, Iter: 1780/2500, Train Loss: 0.0797\n",
      "Epoch: 1/50, Iter: 1790/2500, Train Loss: 0.1057\n",
      "Epoch: 1/50, Iter: 1800/2500, Train Loss: 0.0686\n",
      "Epoch: 1/50, Iter: 1810/2500, Train Loss: 0.0937\n",
      "Epoch: 1/50, Iter: 1820/2500, Train Loss: 0.0681\n",
      "Epoch: 1/50, Iter: 1830/2500, Train Loss: 0.1139\n",
      "Epoch: 1/50, Iter: 1840/2500, Train Loss: 0.1163\n",
      "Epoch: 1/50, Iter: 1850/2500, Train Loss: 0.1184\n",
      "Epoch: 1/50, Iter: 1860/2500, Train Loss: 0.0922\n",
      "Epoch: 1/50, Iter: 1870/2500, Train Loss: 0.0691\n",
      "Epoch: 1/50, Iter: 1880/2500, Train Loss: 0.0575\n",
      "Epoch: 1/50, Iter: 1890/2500, Train Loss: 0.1157\n",
      "Epoch: 1/50, Iter: 1900/2500, Train Loss: 0.1386\n",
      "Epoch: 1/50, Iter: 1910/2500, Train Loss: 0.0866\n",
      "Epoch: 1/50, Iter: 1920/2500, Train Loss: 0.0151\n",
      "Epoch: 1/50, Iter: 1930/2500, Train Loss: 0.0961\n",
      "Epoch: 1/50, Iter: 1940/2500, Train Loss: 0.0944\n",
      "Epoch: 1/50, Iter: 1950/2500, Train Loss: 0.1185\n",
      "Epoch: 1/50, Iter: 1960/2500, Train Loss: 0.1399\n",
      "Epoch: 1/50, Iter: 1970/2500, Train Loss: 0.1538\n",
      "Epoch: 1/50, Iter: 1980/2500, Train Loss: 0.0737\n",
      "Epoch: 1/50, Iter: 1990/2500, Train Loss: 0.0692\n",
      "Epoch: 1/50, Iter: 2000/2500, Train Loss: 0.1293\n",
      "Epoch: 1/50, Iter: 2010/2500, Train Loss: 0.1075\n",
      "Epoch: 1/50, Iter: 2020/2500, Train Loss: 0.2139\n",
      "Epoch: 1/50, Iter: 2030/2500, Train Loss: 0.1535\n",
      "Epoch: 1/50, Iter: 2040/2500, Train Loss: 0.1057\n",
      "Epoch: 1/50, Iter: 2050/2500, Train Loss: 0.0875\n",
      "Epoch: 1/50, Iter: 2060/2500, Train Loss: 0.1050\n",
      "Epoch: 1/50, Iter: 2070/2500, Train Loss: 0.1043\n",
      "Epoch: 1/50, Iter: 2080/2500, Train Loss: 0.1182\n",
      "Epoch: 1/50, Iter: 2090/2500, Train Loss: 0.0936\n",
      "Epoch: 1/50, Iter: 2100/2500, Train Loss: 0.1160\n",
      "Epoch: 1/50, Iter: 2110/2500, Train Loss: 0.2024\n",
      "Epoch: 1/50, Iter: 2120/2500, Train Loss: 0.1194\n",
      "Epoch: 1/50, Iter: 2130/2500, Train Loss: 0.1053\n",
      "Epoch: 1/50, Iter: 2140/2500, Train Loss: 0.0977\n",
      "Epoch: 1/50, Iter: 2150/2500, Train Loss: 0.1234\n",
      "Epoch: 1/50, Iter: 2160/2500, Train Loss: 0.0866\n",
      "Epoch: 1/50, Iter: 2170/2500, Train Loss: 0.0856\n",
      "Epoch: 1/50, Iter: 2180/2500, Train Loss: 0.1307\n",
      "Epoch: 1/50, Iter: 2190/2500, Train Loss: 0.0896\n",
      "Epoch: 1/50, Iter: 2200/2500, Train Loss: 0.1486\n",
      "Epoch: 1/50, Iter: 2210/2500, Train Loss: 0.0561\n",
      "Epoch: 1/50, Iter: 2220/2500, Train Loss: 0.0433\n",
      "Epoch: 1/50, Iter: 2230/2500, Train Loss: 0.0915\n",
      "Epoch: 1/50, Iter: 2240/2500, Train Loss: 0.2375\n",
      "Epoch: 1/50, Iter: 2250/2500, Train Loss: 0.1386\n",
      "Epoch: 1/50, Iter: 2260/2500, Train Loss: 0.1066\n",
      "Epoch: 1/50, Iter: 2270/2500, Train Loss: 0.0786\n",
      "Epoch: 1/50, Iter: 2280/2500, Train Loss: 0.1365\n",
      "Epoch: 1/50, Iter: 2290/2500, Train Loss: 0.1107\n",
      "Epoch: 1/50, Iter: 2300/2500, Train Loss: 0.1075\n",
      "Epoch: 1/50, Iter: 2310/2500, Train Loss: 0.1259\n",
      "Epoch: 1/50, Iter: 2320/2500, Train Loss: 0.0575\n",
      "Epoch: 1/50, Iter: 2330/2500, Train Loss: 0.1215\n",
      "Epoch: 1/50, Iter: 2340/2500, Train Loss: 0.1083\n",
      "Epoch: 1/50, Iter: 2350/2500, Train Loss: 0.0947\n",
      "Epoch: 1/50, Iter: 2360/2500, Train Loss: 0.1209\n",
      "Epoch: 1/50, Iter: 2370/2500, Train Loss: 0.1112\n",
      "Epoch: 1/50, Iter: 2380/2500, Train Loss: 0.0755\n",
      "Epoch: 1/50, Iter: 2390/2500, Train Loss: 0.0983\n",
      "Epoch: 1/50, Iter: 2400/2500, Train Loss: 0.1553\n",
      "Epoch: 1/50, Iter: 2410/2500, Train Loss: 0.1757\n",
      "Epoch: 1/50, Iter: 2420/2500, Train Loss: 0.1052\n",
      "Epoch: 1/50, Iter: 2430/2500, Train Loss: 0.0896\n",
      "Epoch: 1/50, Iter: 2440/2500, Train Loss: 0.2515\n",
      "Epoch: 1/50, Iter: 2450/2500, Train Loss: 0.1572\n",
      "Epoch: 1/50, Iter: 2460/2500, Train Loss: 0.1020\n",
      "Epoch: 1/50, Iter: 2470/2500, Train Loss: 0.0901\n",
      "Epoch: 1/50, Iter: 2480/2500, Train Loss: 0.0993\n",
      "Epoch: 1/50, Iter: 2490/2500, Train Loss: 0.0859\n",
      "Epoch: 1/50, Iter: 2500/2500, Train Loss: 0.1112\n",
      "Epoch 1 finished.\n",
      "Train Loss: 0.1206, Val Loss: 0.1098\n",
      "Saved best model to runs-fasterrcnn/faster_rcnn_resnet50_fpn_20250707-145842/best_model_epoch_1.pth\n",
      "Epoch: 2/50, Iter: 10/2500, Train Loss: 0.1258\n",
      "Epoch: 2/50, Iter: 20/2500, Train Loss: 0.0497\n",
      "Epoch: 2/50, Iter: 30/2500, Train Loss: 0.1107\n",
      "Epoch: 2/50, Iter: 40/2500, Train Loss: 0.1041\n",
      "Epoch: 2/50, Iter: 50/2500, Train Loss: 0.0683\n",
      "Epoch: 2/50, Iter: 60/2500, Train Loss: 0.0787\n",
      "Epoch: 2/50, Iter: 70/2500, Train Loss: 0.0924\n",
      "Epoch: 2/50, Iter: 80/2500, Train Loss: 0.1689\n",
      "Epoch: 2/50, Iter: 90/2500, Train Loss: 0.0840\n",
      "Epoch: 2/50, Iter: 100/2500, Train Loss: 0.1381\n",
      "Epoch: 2/50, Iter: 110/2500, Train Loss: 0.0392\n",
      "Epoch: 2/50, Iter: 120/2500, Train Loss: 0.1198\n",
      "Epoch: 2/50, Iter: 130/2500, Train Loss: 0.0618\n",
      "Epoch: 2/50, Iter: 140/2500, Train Loss: 0.0750\n",
      "Epoch: 2/50, Iter: 150/2500, Train Loss: 0.1050\n",
      "Epoch: 2/50, Iter: 160/2500, Train Loss: 0.0674\n",
      "Epoch: 2/50, Iter: 170/2500, Train Loss: 0.1263\n",
      "Epoch: 2/50, Iter: 180/2500, Train Loss: 0.1147\n",
      "Epoch: 2/50, Iter: 190/2500, Train Loss: 0.1017\n",
      "Epoch: 2/50, Iter: 200/2500, Train Loss: 0.0937\n",
      "Epoch: 2/50, Iter: 210/2500, Train Loss: 0.0749\n",
      "Epoch: 2/50, Iter: 220/2500, Train Loss: 0.0631\n",
      "Epoch: 2/50, Iter: 230/2500, Train Loss: 0.0524\n",
      "Epoch: 2/50, Iter: 240/2500, Train Loss: 0.1077\n",
      "Epoch: 2/50, Iter: 250/2500, Train Loss: 0.0824\n",
      "Epoch: 2/50, Iter: 260/2500, Train Loss: 0.1262\n",
      "Epoch: 2/50, Iter: 270/2500, Train Loss: 0.0930\n",
      "Epoch: 2/50, Iter: 280/2500, Train Loss: 0.1050\n",
      "Epoch: 2/50, Iter: 290/2500, Train Loss: 0.0781\n",
      "Epoch: 2/50, Iter: 300/2500, Train Loss: 0.0598\n",
      "Epoch: 2/50, Iter: 310/2500, Train Loss: 0.1597\n",
      "Epoch: 2/50, Iter: 320/2500, Train Loss: 0.1048\n",
      "Epoch: 2/50, Iter: 330/2500, Train Loss: 0.0951\n",
      "Epoch: 2/50, Iter: 340/2500, Train Loss: 0.0866\n",
      "Epoch: 2/50, Iter: 350/2500, Train Loss: 0.2630\n",
      "Epoch: 2/50, Iter: 360/2500, Train Loss: 0.1065\n",
      "Epoch: 2/50, Iter: 370/2500, Train Loss: 0.0723\n",
      "Epoch: 2/50, Iter: 380/2500, Train Loss: 0.1613\n",
      "Epoch: 2/50, Iter: 390/2500, Train Loss: 0.1727\n",
      "Epoch: 2/50, Iter: 400/2500, Train Loss: 0.1152\n",
      "Epoch: 2/50, Iter: 410/2500, Train Loss: 0.1030\n",
      "Epoch: 2/50, Iter: 420/2500, Train Loss: 0.0779\n",
      "Epoch: 2/50, Iter: 430/2500, Train Loss: 0.0784\n",
      "Epoch: 2/50, Iter: 440/2500, Train Loss: 0.0887\n",
      "Epoch: 2/50, Iter: 450/2500, Train Loss: 0.0604\n",
      "Epoch: 2/50, Iter: 460/2500, Train Loss: 0.0805\n",
      "Epoch: 2/50, Iter: 470/2500, Train Loss: 0.1378\n",
      "Epoch: 2/50, Iter: 480/2500, Train Loss: 0.0621\n",
      "Epoch: 2/50, Iter: 490/2500, Train Loss: 0.1576\n",
      "Epoch: 2/50, Iter: 500/2500, Train Loss: 0.1166\n",
      "Epoch: 2/50, Iter: 510/2500, Train Loss: 0.1315\n",
      "Epoch: 2/50, Iter: 520/2500, Train Loss: 0.0922\n",
      "Epoch: 2/50, Iter: 530/2500, Train Loss: 0.1520\n",
      "Epoch: 2/50, Iter: 540/2500, Train Loss: 0.1118\n",
      "Epoch: 2/50, Iter: 550/2500, Train Loss: 0.0829\n",
      "Epoch: 2/50, Iter: 560/2500, Train Loss: 0.0852\n",
      "Epoch: 2/50, Iter: 570/2500, Train Loss: 0.1092\n",
      "Epoch: 2/50, Iter: 580/2500, Train Loss: 0.2973\n",
      "Epoch: 2/50, Iter: 590/2500, Train Loss: 0.0856\n",
      "Epoch: 2/50, Iter: 600/2500, Train Loss: 0.1063\n",
      "Epoch: 2/50, Iter: 610/2500, Train Loss: 0.1156\n",
      "Epoch: 2/50, Iter: 620/2500, Train Loss: 0.0647\n",
      "Epoch: 2/50, Iter: 630/2500, Train Loss: 0.1007\n",
      "Epoch: 2/50, Iter: 640/2500, Train Loss: 0.1561\n",
      "Epoch: 2/50, Iter: 650/2500, Train Loss: 0.0691\n",
      "Epoch: 2/50, Iter: 660/2500, Train Loss: 0.0876\n",
      "Epoch: 2/50, Iter: 670/2500, Train Loss: 0.0898\n",
      "Epoch: 2/50, Iter: 680/2500, Train Loss: 0.0785\n",
      "Epoch: 2/50, Iter: 690/2500, Train Loss: 0.0904\n",
      "Epoch: 2/50, Iter: 700/2500, Train Loss: 0.0743\n",
      "Epoch: 2/50, Iter: 710/2500, Train Loss: 0.0456\n",
      "Epoch: 2/50, Iter: 720/2500, Train Loss: 0.1254\n",
      "Epoch: 2/50, Iter: 730/2500, Train Loss: 0.0940\n",
      "Epoch: 2/50, Iter: 740/2500, Train Loss: 0.0658\n",
      "Epoch: 2/50, Iter: 750/2500, Train Loss: 0.0785\n",
      "Epoch: 2/50, Iter: 760/2500, Train Loss: 0.1055\n",
      "Epoch: 2/50, Iter: 770/2500, Train Loss: 0.1122\n",
      "Epoch: 2/50, Iter: 780/2500, Train Loss: 0.1137\n",
      "Epoch: 2/50, Iter: 790/2500, Train Loss: 0.0946\n",
      "Epoch: 2/50, Iter: 800/2500, Train Loss: 0.1632\n",
      "Epoch: 2/50, Iter: 810/2500, Train Loss: 0.0816\n",
      "Epoch: 2/50, Iter: 820/2500, Train Loss: 0.0773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/50, Iter: 830/2500, Train Loss: 0.0554\n",
      "Epoch: 2/50, Iter: 840/2500, Train Loss: 0.0588\n",
      "Epoch: 2/50, Iter: 850/2500, Train Loss: 0.1694\n",
      "Epoch: 2/50, Iter: 860/2500, Train Loss: 0.0825\n",
      "Epoch: 2/50, Iter: 870/2500, Train Loss: 0.0783\n",
      "Epoch: 2/50, Iter: 880/2500, Train Loss: 0.0574\n",
      "Epoch: 2/50, Iter: 890/2500, Train Loss: 0.1271\n",
      "Epoch: 2/50, Iter: 900/2500, Train Loss: 0.0774\n",
      "Epoch: 2/50, Iter: 910/2500, Train Loss: 0.1085\n",
      "Epoch: 2/50, Iter: 920/2500, Train Loss: 0.0858\n",
      "Epoch: 2/50, Iter: 930/2500, Train Loss: 0.1020\n",
      "Epoch: 2/50, Iter: 940/2500, Train Loss: 0.1347\n",
      "Epoch: 2/50, Iter: 950/2500, Train Loss: 0.0722\n",
      "Epoch: 2/50, Iter: 960/2500, Train Loss: 0.0616\n",
      "Epoch: 2/50, Iter: 970/2500, Train Loss: 0.0770\n",
      "Epoch: 2/50, Iter: 980/2500, Train Loss: 0.0877\n",
      "Epoch: 2/50, Iter: 990/2500, Train Loss: 0.1331\n",
      "Epoch: 2/50, Iter: 1000/2500, Train Loss: 0.0763\n",
      "Epoch: 2/50, Iter: 1010/2500, Train Loss: 0.1068\n",
      "Epoch: 2/50, Iter: 1020/2500, Train Loss: 0.0705\n",
      "Epoch: 2/50, Iter: 1030/2500, Train Loss: 0.0984\n",
      "Epoch: 2/50, Iter: 1040/2500, Train Loss: 0.1416\n",
      "Epoch: 2/50, Iter: 1050/2500, Train Loss: 0.1038\n",
      "Epoch: 2/50, Iter: 1060/2500, Train Loss: 0.0994\n",
      "Epoch: 2/50, Iter: 1070/2500, Train Loss: 0.1365\n",
      "Epoch: 2/50, Iter: 1080/2500, Train Loss: 0.0716\n",
      "Epoch: 2/50, Iter: 1090/2500, Train Loss: 0.1214\n",
      "Epoch: 2/50, Iter: 1100/2500, Train Loss: 0.0616\n",
      "Epoch: 2/50, Iter: 1110/2500, Train Loss: 0.1228\n",
      "Epoch: 2/50, Iter: 1120/2500, Train Loss: 0.1907\n",
      "Epoch: 2/50, Iter: 1130/2500, Train Loss: 0.1584\n",
      "Epoch: 2/50, Iter: 1140/2500, Train Loss: 0.1188\n",
      "Epoch: 2/50, Iter: 1150/2500, Train Loss: 0.1109\n",
      "Epoch: 2/50, Iter: 1160/2500, Train Loss: 0.2226\n",
      "Epoch: 2/50, Iter: 1170/2500, Train Loss: 0.1133\n",
      "Epoch: 2/50, Iter: 1180/2500, Train Loss: 0.0446\n",
      "Epoch: 2/50, Iter: 1190/2500, Train Loss: 0.1508\n",
      "Epoch: 2/50, Iter: 1200/2500, Train Loss: 0.0778\n",
      "Epoch: 2/50, Iter: 1210/2500, Train Loss: 0.0849\n",
      "Epoch: 2/50, Iter: 1220/2500, Train Loss: 0.1191\n",
      "Epoch: 2/50, Iter: 1230/2500, Train Loss: 0.0765\n",
      "Epoch: 2/50, Iter: 1240/2500, Train Loss: 0.0758\n",
      "Epoch: 2/50, Iter: 1250/2500, Train Loss: 0.0630\n",
      "Epoch: 2/50, Iter: 1260/2500, Train Loss: 0.0802\n",
      "Epoch: 2/50, Iter: 1270/2500, Train Loss: 0.0571\n",
      "Epoch: 2/50, Iter: 1280/2500, Train Loss: 0.1058\n",
      "Epoch: 2/50, Iter: 1290/2500, Train Loss: 0.1291\n",
      "Epoch: 2/50, Iter: 1300/2500, Train Loss: 0.1256\n",
      "Epoch: 2/50, Iter: 1310/2500, Train Loss: 0.0749\n",
      "Epoch: 2/50, Iter: 1320/2500, Train Loss: 0.1423\n",
      "Epoch: 2/50, Iter: 1330/2500, Train Loss: 0.0917\n",
      "Epoch: 2/50, Iter: 1340/2500, Train Loss: 0.0909\n",
      "Epoch: 2/50, Iter: 1350/2500, Train Loss: 0.0837\n",
      "Epoch: 2/50, Iter: 1360/2500, Train Loss: 0.0393\n",
      "Epoch: 2/50, Iter: 1370/2500, Train Loss: 0.0296\n",
      "Epoch: 2/50, Iter: 1380/2500, Train Loss: 0.1299\n",
      "Epoch: 2/50, Iter: 1390/2500, Train Loss: 0.1197\n",
      "Epoch: 2/50, Iter: 1400/2500, Train Loss: 0.0981\n",
      "Epoch: 2/50, Iter: 1410/2500, Train Loss: 0.0373\n",
      "Epoch: 2/50, Iter: 1420/2500, Train Loss: 0.0952\n",
      "Epoch: 2/50, Iter: 1430/2500, Train Loss: 0.3105\n",
      "Epoch: 2/50, Iter: 1440/2500, Train Loss: 0.0814\n",
      "Epoch: 2/50, Iter: 1450/2500, Train Loss: 0.0912\n",
      "Epoch: 2/50, Iter: 1460/2500, Train Loss: 0.1282\n",
      "Epoch: 2/50, Iter: 1470/2500, Train Loss: 0.2191\n",
      "Epoch: 2/50, Iter: 1480/2500, Train Loss: 0.1450\n",
      "Epoch: 2/50, Iter: 1490/2500, Train Loss: 0.0816\n",
      "Epoch: 2/50, Iter: 1500/2500, Train Loss: 0.0731\n",
      "Epoch: 2/50, Iter: 1510/2500, Train Loss: 0.1331\n",
      "Epoch: 2/50, Iter: 1520/2500, Train Loss: 0.0450\n",
      "Epoch: 2/50, Iter: 1530/2500, Train Loss: 0.2132\n",
      "Epoch: 2/50, Iter: 1540/2500, Train Loss: 0.0657\n",
      "Epoch: 2/50, Iter: 1550/2500, Train Loss: 0.0813\n",
      "Epoch: 2/50, Iter: 1560/2500, Train Loss: 0.0647\n",
      "Epoch: 2/50, Iter: 1570/2500, Train Loss: 0.1048\n",
      "Epoch: 2/50, Iter: 1580/2500, Train Loss: 0.0715\n",
      "Epoch: 2/50, Iter: 1590/2500, Train Loss: 0.1256\n",
      "Epoch: 2/50, Iter: 1600/2500, Train Loss: 0.1411\n",
      "Epoch: 2/50, Iter: 1610/2500, Train Loss: 0.0871\n",
      "Epoch: 2/50, Iter: 1620/2500, Train Loss: 0.1028\n",
      "Epoch: 2/50, Iter: 1630/2500, Train Loss: 0.1154\n",
      "Epoch: 2/50, Iter: 1640/2500, Train Loss: 0.1006\n",
      "Epoch: 2/50, Iter: 1650/2500, Train Loss: 0.1596\n",
      "Epoch: 2/50, Iter: 1660/2500, Train Loss: 0.0366\n",
      "Epoch: 2/50, Iter: 1670/2500, Train Loss: 0.1160\n",
      "Epoch: 2/50, Iter: 1680/2500, Train Loss: 0.0463\n",
      "Epoch: 2/50, Iter: 1690/2500, Train Loss: 0.1015\n",
      "Epoch: 2/50, Iter: 1700/2500, Train Loss: 0.0690\n",
      "Epoch: 2/50, Iter: 1710/2500, Train Loss: 0.1267\n",
      "Epoch: 2/50, Iter: 1720/2500, Train Loss: 0.0835\n",
      "Epoch: 2/50, Iter: 1730/2500, Train Loss: 0.1122\n",
      "Epoch: 2/50, Iter: 1740/2500, Train Loss: 0.0763\n",
      "Epoch: 2/50, Iter: 1750/2500, Train Loss: 0.0830\n",
      "Epoch: 2/50, Iter: 1760/2500, Train Loss: 0.0753\n",
      "Epoch: 2/50, Iter: 1770/2500, Train Loss: 0.1068\n",
      "Epoch: 2/50, Iter: 1780/2500, Train Loss: 0.1227\n",
      "Epoch: 2/50, Iter: 1790/2500, Train Loss: 0.0655\n",
      "Epoch: 2/50, Iter: 1800/2500, Train Loss: 0.0956\n",
      "Epoch: 2/50, Iter: 1810/2500, Train Loss: 0.0528\n",
      "Epoch: 2/50, Iter: 1820/2500, Train Loss: 0.1226\n",
      "Epoch: 2/50, Iter: 1830/2500, Train Loss: 0.0752\n",
      "Epoch: 2/50, Iter: 1840/2500, Train Loss: 0.1039\n",
      "Epoch: 2/50, Iter: 1850/2500, Train Loss: 0.0558\n",
      "Epoch: 2/50, Iter: 1860/2500, Train Loss: 0.0914\n",
      "Epoch: 2/50, Iter: 1870/2500, Train Loss: 0.0988\n",
      "Epoch: 2/50, Iter: 1880/2500, Train Loss: 0.0998\n",
      "Epoch: 2/50, Iter: 1890/2500, Train Loss: 0.0797\n",
      "Epoch: 2/50, Iter: 1900/2500, Train Loss: 0.0872\n",
      "Epoch: 2/50, Iter: 1910/2500, Train Loss: 0.1225\n",
      "Epoch: 2/50, Iter: 1920/2500, Train Loss: 0.1421\n",
      "Epoch: 2/50, Iter: 1930/2500, Train Loss: 0.1154\n",
      "Epoch: 2/50, Iter: 1940/2500, Train Loss: 0.0870\n",
      "Epoch: 2/50, Iter: 1950/2500, Train Loss: 0.0663\n",
      "Epoch: 2/50, Iter: 1960/2500, Train Loss: 0.0799\n",
      "Epoch: 2/50, Iter: 1970/2500, Train Loss: 0.1050\n",
      "Epoch: 2/50, Iter: 1980/2500, Train Loss: 0.0905\n",
      "Epoch: 2/50, Iter: 1990/2500, Train Loss: 0.0820\n",
      "Epoch: 2/50, Iter: 2000/2500, Train Loss: 0.0755\n",
      "Epoch: 2/50, Iter: 2010/2500, Train Loss: 0.0417\n",
      "Epoch: 2/50, Iter: 2020/2500, Train Loss: 0.1059\n",
      "Epoch: 2/50, Iter: 2030/2500, Train Loss: 0.0763\n",
      "Epoch: 2/50, Iter: 2040/2500, Train Loss: 0.0387\n",
      "Epoch: 2/50, Iter: 2050/2500, Train Loss: 0.0873\n",
      "Epoch: 2/50, Iter: 2060/2500, Train Loss: 0.0939\n",
      "Epoch: 2/50, Iter: 2070/2500, Train Loss: 0.1212\n",
      "Epoch: 2/50, Iter: 2080/2500, Train Loss: 0.1139\n",
      "Epoch: 2/50, Iter: 2090/2500, Train Loss: 0.0802\n",
      "Epoch: 2/50, Iter: 2100/2500, Train Loss: 0.0750\n",
      "Epoch: 2/50, Iter: 2110/2500, Train Loss: 0.2025\n",
      "Epoch: 2/50, Iter: 2120/2500, Train Loss: 0.0698\n",
      "Epoch: 2/50, Iter: 2130/2500, Train Loss: 0.0741\n",
      "Epoch: 2/50, Iter: 2140/2500, Train Loss: 0.0500\n",
      "Epoch: 2/50, Iter: 2150/2500, Train Loss: 0.0991\n",
      "Epoch: 2/50, Iter: 2160/2500, Train Loss: 0.0959\n",
      "Epoch: 2/50, Iter: 2170/2500, Train Loss: 0.0484\n",
      "Epoch: 2/50, Iter: 2180/2500, Train Loss: 0.1020\n",
      "Epoch: 2/50, Iter: 2190/2500, Train Loss: 0.0475\n",
      "Epoch: 2/50, Iter: 2200/2500, Train Loss: 0.0528\n",
      "Epoch: 2/50, Iter: 2210/2500, Train Loss: 0.1483\n",
      "Epoch: 2/50, Iter: 2220/2500, Train Loss: 0.0874\n",
      "Epoch: 2/50, Iter: 2230/2500, Train Loss: 0.1551\n",
      "Epoch: 2/50, Iter: 2240/2500, Train Loss: 0.0796\n",
      "Epoch: 2/50, Iter: 2250/2500, Train Loss: 0.0638\n",
      "Epoch: 2/50, Iter: 2260/2500, Train Loss: 0.0549\n",
      "Epoch: 2/50, Iter: 2270/2500, Train Loss: 0.1009\n",
      "Epoch: 2/50, Iter: 2280/2500, Train Loss: 0.0825\n",
      "Epoch: 2/50, Iter: 2290/2500, Train Loss: 0.0519\n",
      "Epoch: 2/50, Iter: 2300/2500, Train Loss: 0.0778\n",
      "Epoch: 2/50, Iter: 2310/2500, Train Loss: 0.0848\n",
      "Epoch: 2/50, Iter: 2320/2500, Train Loss: 0.0696\n",
      "Epoch: 2/50, Iter: 2330/2500, Train Loss: 0.0779\n",
      "Epoch: 2/50, Iter: 2340/2500, Train Loss: 0.1137\n",
      "Epoch: 2/50, Iter: 2350/2500, Train Loss: 0.1490\n",
      "Epoch: 2/50, Iter: 2360/2500, Train Loss: 0.0988\n",
      "Epoch: 2/50, Iter: 2370/2500, Train Loss: 0.1479\n",
      "Epoch: 2/50, Iter: 2380/2500, Train Loss: 0.0414\n",
      "Epoch: 2/50, Iter: 2390/2500, Train Loss: 0.0670\n",
      "Epoch: 2/50, Iter: 2400/2500, Train Loss: 0.0989\n",
      "Epoch: 2/50, Iter: 2410/2500, Train Loss: 0.0674\n",
      "Epoch: 2/50, Iter: 2420/2500, Train Loss: 0.1271\n",
      "Epoch: 2/50, Iter: 2430/2500, Train Loss: 0.0713\n",
      "Epoch: 2/50, Iter: 2440/2500, Train Loss: 0.1069\n",
      "Epoch: 2/50, Iter: 2450/2500, Train Loss: 0.0744\n",
      "Epoch: 2/50, Iter: 2460/2500, Train Loss: 0.0557\n",
      "Epoch: 2/50, Iter: 2470/2500, Train Loss: 0.1125\n",
      "Epoch: 2/50, Iter: 2480/2500, Train Loss: 0.0358\n",
      "Epoch: 2/50, Iter: 2490/2500, Train Loss: 0.0699\n",
      "Epoch: 2/50, Iter: 2500/2500, Train Loss: 0.0534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 finished.\n",
      "Train Loss: 0.1017, Val Loss: 0.1006\n",
      "Saved best model to runs-fasterrcnn/faster_rcnn_resnet50_fpn_20250707-145842/best_model_epoch_2.pth\n",
      "Epoch: 3/50, Iter: 10/2500, Train Loss: 0.0685\n",
      "Epoch: 3/50, Iter: 20/2500, Train Loss: 0.0438\n",
      "Epoch: 3/50, Iter: 30/2500, Train Loss: 0.0899\n",
      "Epoch: 3/50, Iter: 40/2500, Train Loss: 0.0364\n",
      "Epoch: 3/50, Iter: 50/2500, Train Loss: 0.1644\n",
      "Epoch: 3/50, Iter: 60/2500, Train Loss: 0.0754\n",
      "Epoch: 3/50, Iter: 70/2500, Train Loss: 0.0747\n",
      "Epoch: 3/50, Iter: 80/2500, Train Loss: 0.1166\n",
      "Epoch: 3/50, Iter: 90/2500, Train Loss: 0.0861\n",
      "Epoch: 3/50, Iter: 100/2500, Train Loss: 0.0676\n",
      "Epoch: 3/50, Iter: 110/2500, Train Loss: 0.2648\n",
      "Epoch: 3/50, Iter: 120/2500, Train Loss: 0.0948\n",
      "Epoch: 3/50, Iter: 130/2500, Train Loss: 0.0767\n",
      "Epoch: 3/50, Iter: 140/2500, Train Loss: 0.0228\n",
      "Epoch: 3/50, Iter: 150/2500, Train Loss: 0.0820\n",
      "Epoch: 3/50, Iter: 160/2500, Train Loss: 0.1470\n",
      "Epoch: 3/50, Iter: 170/2500, Train Loss: 0.0738\n",
      "Epoch: 3/50, Iter: 180/2500, Train Loss: 0.0634\n",
      "Epoch: 3/50, Iter: 190/2500, Train Loss: 0.0283\n",
      "Epoch: 3/50, Iter: 200/2500, Train Loss: 0.1236\n",
      "Epoch: 3/50, Iter: 210/2500, Train Loss: 0.0756\n",
      "Epoch: 3/50, Iter: 220/2500, Train Loss: 0.0399\n",
      "Epoch: 3/50, Iter: 230/2500, Train Loss: 0.0742\n",
      "Epoch: 3/50, Iter: 240/2500, Train Loss: 0.1658\n",
      "Epoch: 3/50, Iter: 250/2500, Train Loss: 0.1016\n",
      "Epoch: 3/50, Iter: 260/2500, Train Loss: 0.0717\n",
      "Epoch: 3/50, Iter: 270/2500, Train Loss: 0.1475\n",
      "Epoch: 3/50, Iter: 280/2500, Train Loss: 0.0392\n",
      "Epoch: 3/50, Iter: 290/2500, Train Loss: 0.1052\n",
      "Epoch: 3/50, Iter: 300/2500, Train Loss: 0.0681\n",
      "Epoch: 3/50, Iter: 310/2500, Train Loss: 0.0899\n",
      "Epoch: 3/50, Iter: 320/2500, Train Loss: 0.0713\n",
      "Epoch: 3/50, Iter: 330/2500, Train Loss: 0.0611\n",
      "Epoch: 3/50, Iter: 340/2500, Train Loss: 0.0584\n",
      "Epoch: 3/50, Iter: 350/2500, Train Loss: 0.0878\n",
      "Epoch: 3/50, Iter: 360/2500, Train Loss: 0.0935\n",
      "Epoch: 3/50, Iter: 370/2500, Train Loss: 0.1333\n",
      "Epoch: 3/50, Iter: 380/2500, Train Loss: 0.0787\n",
      "Epoch: 3/50, Iter: 390/2500, Train Loss: 0.0755\n",
      "Epoch: 3/50, Iter: 400/2500, Train Loss: 0.0963\n",
      "Epoch: 3/50, Iter: 410/2500, Train Loss: 0.1162\n",
      "Epoch: 3/50, Iter: 420/2500, Train Loss: 0.0681\n",
      "Epoch: 3/50, Iter: 430/2500, Train Loss: 0.1027\n",
      "Epoch: 3/50, Iter: 440/2500, Train Loss: 0.1147\n",
      "Epoch: 3/50, Iter: 450/2500, Train Loss: 0.1119\n",
      "Epoch: 3/50, Iter: 460/2500, Train Loss: 0.0955\n",
      "Epoch: 3/50, Iter: 470/2500, Train Loss: 0.2101\n",
      "Epoch: 3/50, Iter: 480/2500, Train Loss: 0.0943\n",
      "Epoch: 3/50, Iter: 490/2500, Train Loss: 0.0991\n",
      "Epoch: 3/50, Iter: 500/2500, Train Loss: 0.0599\n",
      "Epoch: 3/50, Iter: 510/2500, Train Loss: 0.1588\n",
      "Epoch: 3/50, Iter: 520/2500, Train Loss: 0.1061\n",
      "Epoch: 3/50, Iter: 530/2500, Train Loss: 0.1299\n",
      "Epoch: 3/50, Iter: 540/2500, Train Loss: 0.1531\n",
      "Epoch: 3/50, Iter: 550/2500, Train Loss: 0.0760\n",
      "Epoch: 3/50, Iter: 560/2500, Train Loss: 0.0944\n",
      "Epoch: 3/50, Iter: 570/2500, Train Loss: 0.0625\n",
      "Epoch: 3/50, Iter: 580/2500, Train Loss: 0.0774\n",
      "Epoch: 3/50, Iter: 590/2500, Train Loss: 0.0649\n",
      "Epoch: 3/50, Iter: 600/2500, Train Loss: 0.1367\n",
      "Epoch: 3/50, Iter: 610/2500, Train Loss: 0.1113\n",
      "Epoch: 3/50, Iter: 620/2500, Train Loss: 0.0855\n",
      "Epoch: 3/50, Iter: 630/2500, Train Loss: 0.0632\n",
      "Epoch: 3/50, Iter: 640/2500, Train Loss: 0.0810\n",
      "Epoch: 3/50, Iter: 650/2500, Train Loss: 0.1339\n",
      "Epoch: 3/50, Iter: 660/2500, Train Loss: 0.1690\n",
      "Epoch: 3/50, Iter: 670/2500, Train Loss: 0.1046\n",
      "Epoch: 3/50, Iter: 680/2500, Train Loss: 0.0938\n",
      "Epoch: 3/50, Iter: 690/2500, Train Loss: 0.0730\n",
      "Epoch: 3/50, Iter: 700/2500, Train Loss: 0.1351\n",
      "Epoch: 3/50, Iter: 710/2500, Train Loss: 0.0601\n",
      "Epoch: 3/50, Iter: 720/2500, Train Loss: 0.0465\n",
      "Epoch: 3/50, Iter: 730/2500, Train Loss: 0.0956\n",
      "Epoch: 3/50, Iter: 740/2500, Train Loss: 0.1571\n",
      "Epoch: 3/50, Iter: 750/2500, Train Loss: 0.1214\n",
      "Epoch: 3/50, Iter: 760/2500, Train Loss: 0.1488\n",
      "Epoch: 3/50, Iter: 770/2500, Train Loss: 0.0905\n",
      "Epoch: 3/50, Iter: 780/2500, Train Loss: 0.0894\n",
      "Epoch: 3/50, Iter: 790/2500, Train Loss: 0.0653\n",
      "Epoch: 3/50, Iter: 800/2500, Train Loss: 0.0993\n",
      "Epoch: 3/50, Iter: 810/2500, Train Loss: 0.1217\n",
      "Epoch: 3/50, Iter: 820/2500, Train Loss: 0.0545\n",
      "Epoch: 3/50, Iter: 830/2500, Train Loss: 0.0462\n",
      "Epoch: 3/50, Iter: 840/2500, Train Loss: 0.1523\n",
      "Epoch: 3/50, Iter: 850/2500, Train Loss: 0.0686\n",
      "Epoch: 3/50, Iter: 860/2500, Train Loss: 0.0515\n",
      "Epoch: 3/50, Iter: 870/2500, Train Loss: 0.0614\n",
      "Epoch: 3/50, Iter: 880/2500, Train Loss: 0.0899\n",
      "Epoch: 3/50, Iter: 890/2500, Train Loss: 0.0544\n",
      "Epoch: 3/50, Iter: 900/2500, Train Loss: 0.1313\n",
      "Epoch: 3/50, Iter: 910/2500, Train Loss: 0.0900\n",
      "Epoch: 3/50, Iter: 920/2500, Train Loss: 0.1785\n",
      "Epoch: 3/50, Iter: 930/2500, Train Loss: 0.0923\n",
      "Epoch: 3/50, Iter: 940/2500, Train Loss: 0.0736\n",
      "Epoch: 3/50, Iter: 950/2500, Train Loss: 0.1283\n",
      "Epoch: 3/50, Iter: 960/2500, Train Loss: 0.0724\n",
      "Epoch: 3/50, Iter: 970/2500, Train Loss: 0.1832\n",
      "Epoch: 3/50, Iter: 980/2500, Train Loss: 0.0926\n",
      "Epoch: 3/50, Iter: 990/2500, Train Loss: 0.1558\n",
      "Epoch: 3/50, Iter: 1000/2500, Train Loss: 0.0675\n",
      "Epoch: 3/50, Iter: 1010/2500, Train Loss: 0.0943\n",
      "Epoch: 3/50, Iter: 1020/2500, Train Loss: 0.1032\n",
      "Epoch: 3/50, Iter: 1030/2500, Train Loss: 0.0849\n",
      "Epoch: 3/50, Iter: 1040/2500, Train Loss: 0.2300\n",
      "Epoch: 3/50, Iter: 1050/2500, Train Loss: 0.0678\n",
      "Epoch: 3/50, Iter: 1060/2500, Train Loss: 0.0813\n",
      "Epoch: 3/50, Iter: 1070/2500, Train Loss: 0.0819\n",
      "Epoch: 3/50, Iter: 1080/2500, Train Loss: 0.0850\n",
      "Epoch: 3/50, Iter: 1090/2500, Train Loss: 0.0734\n",
      "Epoch: 3/50, Iter: 1100/2500, Train Loss: 0.1234\n",
      "Epoch: 3/50, Iter: 1110/2500, Train Loss: 0.1024\n",
      "Epoch: 3/50, Iter: 1120/2500, Train Loss: 0.0645\n",
      "Epoch: 3/50, Iter: 1130/2500, Train Loss: 0.0758\n",
      "Epoch: 3/50, Iter: 1140/2500, Train Loss: 0.0229\n",
      "Epoch: 3/50, Iter: 1150/2500, Train Loss: 0.0674\n",
      "Epoch: 3/50, Iter: 1160/2500, Train Loss: 0.0761\n",
      "Epoch: 3/50, Iter: 1170/2500, Train Loss: 0.0860\n",
      "Epoch: 3/50, Iter: 1180/2500, Train Loss: 0.0904\n",
      "Epoch: 3/50, Iter: 1190/2500, Train Loss: 0.0871\n",
      "Epoch: 3/50, Iter: 1200/2500, Train Loss: 0.0536\n",
      "Epoch: 3/50, Iter: 1210/2500, Train Loss: 0.0473\n",
      "Epoch: 3/50, Iter: 1220/2500, Train Loss: 0.0576\n",
      "Epoch: 3/50, Iter: 1230/2500, Train Loss: 0.1322\n",
      "Epoch: 3/50, Iter: 1240/2500, Train Loss: 0.0996\n",
      "Epoch: 3/50, Iter: 1250/2500, Train Loss: 0.1535\n",
      "Epoch: 3/50, Iter: 1260/2500, Train Loss: 0.0639\n",
      "Epoch: 3/50, Iter: 1270/2500, Train Loss: 0.0775\n",
      "Epoch: 3/50, Iter: 1280/2500, Train Loss: 0.0755\n",
      "Epoch: 3/50, Iter: 1290/2500, Train Loss: 0.0439\n",
      "Epoch: 3/50, Iter: 1300/2500, Train Loss: 0.1077\n",
      "Epoch: 3/50, Iter: 1310/2500, Train Loss: 0.0920\n",
      "Epoch: 3/50, Iter: 1320/2500, Train Loss: 0.0459\n",
      "Epoch: 3/50, Iter: 1330/2500, Train Loss: 0.1213\n",
      "Epoch: 3/50, Iter: 1340/2500, Train Loss: 0.0867\n",
      "Epoch: 3/50, Iter: 1350/2500, Train Loss: 0.0792\n",
      "Epoch: 3/50, Iter: 1360/2500, Train Loss: 0.1577\n",
      "Epoch: 3/50, Iter: 1370/2500, Train Loss: 0.1307\n",
      "Epoch: 3/50, Iter: 1380/2500, Train Loss: 0.1025\n",
      "Epoch: 3/50, Iter: 1390/2500, Train Loss: 0.2137\n",
      "Epoch: 3/50, Iter: 1400/2500, Train Loss: 0.0473\n",
      "Epoch: 3/50, Iter: 1410/2500, Train Loss: 0.0245\n",
      "Epoch: 3/50, Iter: 1420/2500, Train Loss: 0.0474\n",
      "Epoch: 3/50, Iter: 1430/2500, Train Loss: 0.0753\n",
      "Epoch: 3/50, Iter: 1440/2500, Train Loss: 0.0873\n",
      "Epoch: 3/50, Iter: 1450/2500, Train Loss: 0.0669\n",
      "Epoch: 3/50, Iter: 1460/2500, Train Loss: 0.1091\n",
      "Epoch: 3/50, Iter: 1470/2500, Train Loss: 0.0833\n",
      "Epoch: 3/50, Iter: 1480/2500, Train Loss: 0.0673\n",
      "Epoch: 3/50, Iter: 1490/2500, Train Loss: 0.1161\n",
      "Epoch: 3/50, Iter: 1500/2500, Train Loss: 0.0650\n",
      "Epoch: 3/50, Iter: 1510/2500, Train Loss: 0.0765\n",
      "Epoch: 3/50, Iter: 1520/2500, Train Loss: 0.1222\n",
      "Epoch: 3/50, Iter: 1530/2500, Train Loss: 0.1065\n",
      "Epoch: 3/50, Iter: 1540/2500, Train Loss: 0.0533\n",
      "Epoch: 3/50, Iter: 1550/2500, Train Loss: 0.0986\n",
      "Epoch: 3/50, Iter: 1560/2500, Train Loss: 0.0711\n",
      "Epoch: 3/50, Iter: 1570/2500, Train Loss: 0.0809\n",
      "Epoch: 3/50, Iter: 1580/2500, Train Loss: 0.0812\n",
      "Epoch: 3/50, Iter: 1590/2500, Train Loss: 0.0956\n",
      "Epoch: 3/50, Iter: 1600/2500, Train Loss: 0.0919\n",
      "Epoch: 3/50, Iter: 1610/2500, Train Loss: 0.0493\n",
      "Epoch: 3/50, Iter: 1620/2500, Train Loss: 0.0850\n",
      "Epoch: 3/50, Iter: 1630/2500, Train Loss: 0.1271\n",
      "Epoch: 3/50, Iter: 1640/2500, Train Loss: 0.1033\n",
      "Epoch: 3/50, Iter: 1650/2500, Train Loss: 0.1034\n",
      "Epoch: 3/50, Iter: 1660/2500, Train Loss: 0.0960\n",
      "Epoch: 3/50, Iter: 1670/2500, Train Loss: 0.1151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/50, Iter: 1680/2500, Train Loss: 0.0781\n",
      "Epoch: 3/50, Iter: 1690/2500, Train Loss: 0.1294\n",
      "Epoch: 3/50, Iter: 1700/2500, Train Loss: 0.1026\n",
      "Epoch: 3/50, Iter: 1710/2500, Train Loss: 0.0896\n",
      "Epoch: 3/50, Iter: 1720/2500, Train Loss: 0.1270\n",
      "Epoch: 3/50, Iter: 1730/2500, Train Loss: 0.1171\n",
      "Epoch: 3/50, Iter: 1740/2500, Train Loss: 0.0641\n",
      "Epoch: 3/50, Iter: 1750/2500, Train Loss: 0.1144\n",
      "Epoch: 3/50, Iter: 1760/2500, Train Loss: 0.0456\n",
      "Epoch: 3/50, Iter: 1770/2500, Train Loss: 0.0958\n",
      "Epoch: 3/50, Iter: 1780/2500, Train Loss: 0.0374\n",
      "Epoch: 3/50, Iter: 1790/2500, Train Loss: 0.0946\n",
      "Epoch: 3/50, Iter: 1800/2500, Train Loss: 0.0986\n",
      "Epoch: 3/50, Iter: 1810/2500, Train Loss: 0.0853\n",
      "Epoch: 3/50, Iter: 1820/2500, Train Loss: 0.1268\n",
      "Epoch: 3/50, Iter: 1830/2500, Train Loss: 0.0394\n",
      "Epoch: 3/50, Iter: 1840/2500, Train Loss: 0.1345\n",
      "Epoch: 3/50, Iter: 1850/2500, Train Loss: 0.0481\n",
      "Epoch: 3/50, Iter: 1860/2500, Train Loss: 0.1473\n",
      "Epoch: 3/50, Iter: 1870/2500, Train Loss: 0.0316\n",
      "Epoch: 3/50, Iter: 1880/2500, Train Loss: 0.1380\n",
      "Epoch: 3/50, Iter: 1890/2500, Train Loss: 0.1896\n",
      "Epoch: 3/50, Iter: 1900/2500, Train Loss: 0.1447\n",
      "Epoch: 3/50, Iter: 1910/2500, Train Loss: 0.1422\n",
      "Epoch: 3/50, Iter: 1920/2500, Train Loss: 0.0349\n",
      "Epoch: 3/50, Iter: 1930/2500, Train Loss: 0.1141\n",
      "Epoch: 3/50, Iter: 1940/2500, Train Loss: 0.1010\n",
      "Epoch: 3/50, Iter: 1950/2500, Train Loss: 0.0929\n",
      "Epoch: 3/50, Iter: 1960/2500, Train Loss: 0.0769\n",
      "Epoch: 3/50, Iter: 1970/2500, Train Loss: 0.1903\n",
      "Epoch: 3/50, Iter: 1980/2500, Train Loss: 0.1068\n",
      "Epoch: 3/50, Iter: 1990/2500, Train Loss: 0.1071\n",
      "Epoch: 3/50, Iter: 2000/2500, Train Loss: 0.0733\n",
      "Epoch: 3/50, Iter: 2010/2500, Train Loss: 0.1149\n",
      "Epoch: 3/50, Iter: 2020/2500, Train Loss: 0.0898\n",
      "Epoch: 3/50, Iter: 2030/2500, Train Loss: 0.0804\n",
      "Epoch: 3/50, Iter: 2040/2500, Train Loss: 0.0735\n",
      "Epoch: 3/50, Iter: 2050/2500, Train Loss: 0.1491\n",
      "Epoch: 3/50, Iter: 2060/2500, Train Loss: 0.0812\n",
      "Epoch: 3/50, Iter: 2070/2500, Train Loss: 0.1307\n",
      "Epoch: 3/50, Iter: 2080/2500, Train Loss: 0.0945\n",
      "Epoch: 3/50, Iter: 2090/2500, Train Loss: 0.0598\n",
      "Epoch: 3/50, Iter: 2100/2500, Train Loss: 0.1296\n",
      "Epoch: 3/50, Iter: 2110/2500, Train Loss: 0.0823\n",
      "Epoch: 3/50, Iter: 2120/2500, Train Loss: 0.1169\n",
      "Epoch: 3/50, Iter: 2130/2500, Train Loss: 0.0976\n",
      "Epoch: 3/50, Iter: 2140/2500, Train Loss: 0.0440\n",
      "Epoch: 3/50, Iter: 2150/2500, Train Loss: 0.0620\n",
      "Epoch: 3/50, Iter: 2160/2500, Train Loss: 0.0569\n",
      "Epoch: 3/50, Iter: 2170/2500, Train Loss: 0.0941\n",
      "Epoch: 3/50, Iter: 2180/2500, Train Loss: 0.0923\n",
      "Epoch: 3/50, Iter: 2190/2500, Train Loss: 0.1045\n",
      "Epoch: 3/50, Iter: 2200/2500, Train Loss: 0.0696\n",
      "Epoch: 3/50, Iter: 2210/2500, Train Loss: 0.0682\n",
      "Epoch: 3/50, Iter: 2220/2500, Train Loss: 0.0388\n",
      "Epoch: 3/50, Iter: 2230/2500, Train Loss: 0.0723\n",
      "Epoch: 3/50, Iter: 2240/2500, Train Loss: 0.0637\n",
      "Epoch: 3/50, Iter: 2250/2500, Train Loss: 0.1227\n",
      "Epoch: 3/50, Iter: 2260/2500, Train Loss: 0.1261\n",
      "Epoch: 3/50, Iter: 2270/2500, Train Loss: 0.1798\n",
      "Epoch: 3/50, Iter: 2280/2500, Train Loss: 0.1536\n",
      "Epoch: 3/50, Iter: 2290/2500, Train Loss: 0.1234\n",
      "Epoch: 3/50, Iter: 2300/2500, Train Loss: 0.1127\n",
      "Epoch: 3/50, Iter: 2310/2500, Train Loss: 0.1047\n",
      "Epoch: 3/50, Iter: 2320/2500, Train Loss: 0.0494\n",
      "Epoch: 3/50, Iter: 2330/2500, Train Loss: 0.1130\n",
      "Epoch: 3/50, Iter: 2340/2500, Train Loss: 0.0754\n",
      "Epoch: 3/50, Iter: 2350/2500, Train Loss: 0.1010\n",
      "Epoch: 3/50, Iter: 2360/2500, Train Loss: 0.1054\n",
      "Epoch: 3/50, Iter: 2370/2500, Train Loss: 0.0277\n",
      "Epoch: 3/50, Iter: 2380/2500, Train Loss: 0.1738\n",
      "Epoch: 3/50, Iter: 2390/2500, Train Loss: 0.0822\n",
      "Epoch: 3/50, Iter: 2400/2500, Train Loss: 0.1281\n",
      "Epoch: 3/50, Iter: 2410/2500, Train Loss: 0.1523\n",
      "Epoch: 3/50, Iter: 2420/2500, Train Loss: 0.0933\n",
      "Epoch: 3/50, Iter: 2430/2500, Train Loss: 0.0855\n",
      "Epoch: 3/50, Iter: 2440/2500, Train Loss: 0.1044\n",
      "Epoch: 3/50, Iter: 2450/2500, Train Loss: 0.0540\n",
      "Epoch: 3/50, Iter: 2460/2500, Train Loss: 0.1182\n",
      "Epoch: 3/50, Iter: 2470/2500, Train Loss: 0.1497\n",
      "Epoch: 3/50, Iter: 2480/2500, Train Loss: 0.0627\n",
      "Epoch: 3/50, Iter: 2490/2500, Train Loss: 0.1423\n",
      "Epoch: 3/50, Iter: 2500/2500, Train Loss: 0.2207\n",
      "Epoch 3 finished.\n",
      "Train Loss: 0.0948, Val Loss: 0.1024\n",
      "Epoch: 4/50, Iter: 10/2500, Train Loss: 0.0522\n",
      "Epoch: 4/50, Iter: 20/2500, Train Loss: 0.0572\n",
      "Epoch: 4/50, Iter: 30/2500, Train Loss: 0.0870\n",
      "Epoch: 4/50, Iter: 40/2500, Train Loss: 0.1845\n",
      "Epoch: 4/50, Iter: 50/2500, Train Loss: 0.1335\n",
      "Epoch: 4/50, Iter: 60/2500, Train Loss: 0.0540\n",
      "Epoch: 4/50, Iter: 70/2500, Train Loss: 0.0899\n",
      "Epoch: 4/50, Iter: 80/2500, Train Loss: 0.1030\n",
      "Epoch: 4/50, Iter: 90/2500, Train Loss: 0.0986\n",
      "Epoch: 4/50, Iter: 100/2500, Train Loss: 0.1283\n",
      "Epoch: 4/50, Iter: 110/2500, Train Loss: 0.0660\n",
      "Epoch: 4/50, Iter: 120/2500, Train Loss: 0.0583\n",
      "Epoch: 4/50, Iter: 130/2500, Train Loss: 0.1652\n",
      "Epoch: 4/50, Iter: 140/2500, Train Loss: 0.0545\n",
      "Epoch: 4/50, Iter: 150/2500, Train Loss: 0.0777\n",
      "Epoch: 4/50, Iter: 160/2500, Train Loss: 0.0844\n",
      "Epoch: 4/50, Iter: 170/2500, Train Loss: 0.0977\n",
      "Epoch: 4/50, Iter: 180/2500, Train Loss: 0.1020\n",
      "Epoch: 4/50, Iter: 190/2500, Train Loss: 0.0878\n",
      "Epoch: 4/50, Iter: 200/2500, Train Loss: 0.0868\n",
      "Epoch: 4/50, Iter: 210/2500, Train Loss: 0.1674\n",
      "Epoch: 4/50, Iter: 220/2500, Train Loss: 0.0449\n",
      "Epoch: 4/50, Iter: 230/2500, Train Loss: 0.1339\n",
      "Epoch: 4/50, Iter: 240/2500, Train Loss: 0.1041\n",
      "Epoch: 4/50, Iter: 250/2500, Train Loss: 0.0528\n",
      "Epoch: 4/50, Iter: 260/2500, Train Loss: 0.0655\n",
      "Epoch: 4/50, Iter: 270/2500, Train Loss: 0.1025\n",
      "Epoch: 4/50, Iter: 280/2500, Train Loss: 0.0650\n",
      "Epoch: 4/50, Iter: 290/2500, Train Loss: 0.1022\n",
      "Epoch: 4/50, Iter: 300/2500, Train Loss: 0.1286\n",
      "Epoch: 4/50, Iter: 310/2500, Train Loss: 0.0527\n",
      "Epoch: 4/50, Iter: 320/2500, Train Loss: 0.0792\n",
      "Epoch: 4/50, Iter: 330/2500, Train Loss: 0.1114\n",
      "Epoch: 4/50, Iter: 340/2500, Train Loss: 0.1642\n",
      "Epoch: 4/50, Iter: 350/2500, Train Loss: 0.0604\n",
      "Epoch: 4/50, Iter: 360/2500, Train Loss: 0.0753\n",
      "Epoch: 4/50, Iter: 370/2500, Train Loss: 0.0905\n",
      "Epoch: 4/50, Iter: 380/2500, Train Loss: 0.1397\n",
      "Epoch: 4/50, Iter: 390/2500, Train Loss: 0.1202\n",
      "Epoch: 4/50, Iter: 400/2500, Train Loss: 0.0574\n",
      "Epoch: 4/50, Iter: 410/2500, Train Loss: 0.0541\n",
      "Epoch: 4/50, Iter: 420/2500, Train Loss: 0.1187\n",
      "Epoch: 4/50, Iter: 430/2500, Train Loss: 0.0893\n",
      "Epoch: 4/50, Iter: 440/2500, Train Loss: 0.0808\n",
      "Epoch: 4/50, Iter: 450/2500, Train Loss: 0.0319\n",
      "Epoch: 4/50, Iter: 460/2500, Train Loss: 0.0549\n",
      "Epoch: 4/50, Iter: 470/2500, Train Loss: 0.0740\n",
      "Epoch: 4/50, Iter: 480/2500, Train Loss: 0.3156\n",
      "Epoch: 4/50, Iter: 490/2500, Train Loss: 0.0463\n",
      "Epoch: 4/50, Iter: 500/2500, Train Loss: 0.0959\n",
      "Epoch: 4/50, Iter: 510/2500, Train Loss: 0.1215\n",
      "Epoch: 4/50, Iter: 520/2500, Train Loss: 0.0579\n",
      "Epoch: 4/50, Iter: 530/2500, Train Loss: 0.1118\n",
      "Epoch: 4/50, Iter: 540/2500, Train Loss: 0.0857\n",
      "Epoch: 4/50, Iter: 550/2500, Train Loss: 0.1093\n",
      "Epoch: 4/50, Iter: 560/2500, Train Loss: 0.0482\n",
      "Epoch: 4/50, Iter: 570/2500, Train Loss: 0.0665\n",
      "Epoch: 4/50, Iter: 580/2500, Train Loss: 0.0617\n",
      "Epoch: 4/50, Iter: 590/2500, Train Loss: 0.0702\n",
      "Epoch: 4/50, Iter: 600/2500, Train Loss: 0.0731\n",
      "Epoch: 4/50, Iter: 610/2500, Train Loss: 0.1098\n",
      "Epoch: 4/50, Iter: 620/2500, Train Loss: 0.1994\n",
      "Epoch: 4/50, Iter: 630/2500, Train Loss: 0.1399\n",
      "Epoch: 4/50, Iter: 640/2500, Train Loss: 0.1246\n",
      "Epoch: 4/50, Iter: 650/2500, Train Loss: 0.0993\n",
      "Epoch: 4/50, Iter: 660/2500, Train Loss: 0.1060\n",
      "Epoch: 4/50, Iter: 670/2500, Train Loss: 0.1047\n",
      "Epoch: 4/50, Iter: 680/2500, Train Loss: 0.0732\n",
      "Epoch: 4/50, Iter: 690/2500, Train Loss: 0.0738\n",
      "Epoch: 4/50, Iter: 700/2500, Train Loss: 0.0972\n",
      "Epoch: 4/50, Iter: 710/2500, Train Loss: 0.0811\n",
      "Epoch: 4/50, Iter: 720/2500, Train Loss: 0.1007\n",
      "Epoch: 4/50, Iter: 730/2500, Train Loss: 0.0560\n",
      "Epoch: 4/50, Iter: 740/2500, Train Loss: 0.0648\n",
      "Epoch: 4/50, Iter: 750/2500, Train Loss: 0.0274\n",
      "Epoch: 4/50, Iter: 760/2500, Train Loss: 0.0635\n",
      "Epoch: 4/50, Iter: 770/2500, Train Loss: 0.0581\n",
      "Epoch: 4/50, Iter: 780/2500, Train Loss: 0.0974\n",
      "Epoch: 4/50, Iter: 790/2500, Train Loss: 0.1515\n",
      "Epoch: 4/50, Iter: 800/2500, Train Loss: 0.0735\n",
      "Epoch: 4/50, Iter: 810/2500, Train Loss: 0.0833\n",
      "Epoch: 4/50, Iter: 820/2500, Train Loss: 0.0568\n",
      "Epoch: 4/50, Iter: 830/2500, Train Loss: 0.0765\n",
      "Epoch: 4/50, Iter: 840/2500, Train Loss: 0.0996\n",
      "Epoch: 4/50, Iter: 850/2500, Train Loss: 0.0799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/50, Iter: 860/2500, Train Loss: 0.0529\n",
      "Epoch: 4/50, Iter: 870/2500, Train Loss: 0.1230\n",
      "Epoch: 4/50, Iter: 880/2500, Train Loss: 0.0678\n",
      "Epoch: 4/50, Iter: 890/2500, Train Loss: 0.0596\n",
      "Epoch: 4/50, Iter: 900/2500, Train Loss: 0.0730\n",
      "Epoch: 4/50, Iter: 910/2500, Train Loss: 0.0755\n",
      "Epoch: 4/50, Iter: 920/2500, Train Loss: 0.0761\n",
      "Epoch: 4/50, Iter: 930/2500, Train Loss: 0.0782\n",
      "Epoch: 4/50, Iter: 940/2500, Train Loss: 0.1571\n",
      "Epoch: 4/50, Iter: 950/2500, Train Loss: 0.0762\n",
      "Epoch: 4/50, Iter: 960/2500, Train Loss: 0.0489\n",
      "Epoch: 4/50, Iter: 970/2500, Train Loss: 0.0887\n",
      "Epoch: 4/50, Iter: 980/2500, Train Loss: 0.0913\n",
      "Epoch: 4/50, Iter: 990/2500, Train Loss: 0.1748\n",
      "Epoch: 4/50, Iter: 1000/2500, Train Loss: 0.0428\n",
      "Epoch: 4/50, Iter: 1010/2500, Train Loss: 0.1836\n",
      "Epoch: 4/50, Iter: 1020/2500, Train Loss: 0.1049\n",
      "Epoch: 4/50, Iter: 1030/2500, Train Loss: 0.1604\n",
      "Epoch: 4/50, Iter: 1040/2500, Train Loss: 0.0857\n",
      "Epoch: 4/50, Iter: 1050/2500, Train Loss: 0.0639\n",
      "Epoch: 4/50, Iter: 1060/2500, Train Loss: 0.0661\n",
      "Epoch: 4/50, Iter: 1070/2500, Train Loss: 0.1449\n",
      "Epoch: 4/50, Iter: 1080/2500, Train Loss: 0.1120\n",
      "Epoch: 4/50, Iter: 1090/2500, Train Loss: 0.1036\n",
      "Epoch: 4/50, Iter: 1100/2500, Train Loss: 0.0730\n",
      "Epoch: 4/50, Iter: 1110/2500, Train Loss: 0.0874\n",
      "Epoch: 4/50, Iter: 1120/2500, Train Loss: 0.0783\n",
      "Epoch: 4/50, Iter: 1130/2500, Train Loss: 0.0694\n",
      "Epoch: 4/50, Iter: 1140/2500, Train Loss: 0.1285\n",
      "Epoch: 4/50, Iter: 1150/2500, Train Loss: 0.0671\n",
      "Epoch: 4/50, Iter: 1160/2500, Train Loss: 0.1126\n",
      "Epoch: 4/50, Iter: 1170/2500, Train Loss: 0.1941\n",
      "Epoch: 4/50, Iter: 1180/2500, Train Loss: 0.0692\n",
      "Epoch: 4/50, Iter: 1190/2500, Train Loss: 0.1528\n",
      "Epoch: 4/50, Iter: 1200/2500, Train Loss: 0.1255\n",
      "Epoch: 4/50, Iter: 1210/2500, Train Loss: 0.0696\n",
      "Epoch: 4/50, Iter: 1220/2500, Train Loss: 0.1318\n",
      "Epoch: 4/50, Iter: 1230/2500, Train Loss: 0.1037\n",
      "Epoch: 4/50, Iter: 1240/2500, Train Loss: 0.0670\n",
      "Epoch: 4/50, Iter: 1250/2500, Train Loss: 0.0992\n",
      "Epoch: 4/50, Iter: 1260/2500, Train Loss: 0.0530\n",
      "Epoch: 4/50, Iter: 1270/2500, Train Loss: 0.1500\n",
      "Epoch: 4/50, Iter: 1280/2500, Train Loss: 0.1217\n",
      "Epoch: 4/50, Iter: 1290/2500, Train Loss: 0.0455\n",
      "Epoch: 4/50, Iter: 1300/2500, Train Loss: 0.1070\n",
      "Epoch: 4/50, Iter: 1310/2500, Train Loss: 0.0761\n",
      "Epoch: 4/50, Iter: 1320/2500, Train Loss: 0.1212\n",
      "Epoch: 4/50, Iter: 1330/2500, Train Loss: 0.1193\n",
      "Epoch: 4/50, Iter: 1340/2500, Train Loss: 0.0700\n",
      "Epoch: 4/50, Iter: 1350/2500, Train Loss: 0.0381\n",
      "Epoch: 4/50, Iter: 1360/2500, Train Loss: 0.1066\n",
      "Epoch: 4/50, Iter: 1370/2500, Train Loss: 0.0574\n",
      "Epoch: 4/50, Iter: 1380/2500, Train Loss: 0.0603\n",
      "Epoch: 4/50, Iter: 1390/2500, Train Loss: 0.0671\n",
      "Epoch: 4/50, Iter: 1400/2500, Train Loss: 0.1274\n",
      "Epoch: 4/50, Iter: 1410/2500, Train Loss: 0.0851\n",
      "Epoch: 4/50, Iter: 1420/2500, Train Loss: 0.1173\n",
      "Epoch: 4/50, Iter: 1430/2500, Train Loss: 0.0812\n",
      "Epoch: 4/50, Iter: 1440/2500, Train Loss: 0.0808\n",
      "Epoch: 4/50, Iter: 1450/2500, Train Loss: 0.0633\n",
      "Epoch: 4/50, Iter: 1460/2500, Train Loss: 0.1089\n",
      "Epoch: 4/50, Iter: 1470/2500, Train Loss: 0.0776\n",
      "Epoch: 4/50, Iter: 1480/2500, Train Loss: 0.1066\n",
      "Epoch: 4/50, Iter: 1490/2500, Train Loss: 0.0772\n",
      "Epoch: 4/50, Iter: 1500/2500, Train Loss: 0.0790\n",
      "Epoch: 4/50, Iter: 1510/2500, Train Loss: 0.1132\n",
      "Epoch: 4/50, Iter: 1520/2500, Train Loss: 0.0565\n",
      "Epoch: 4/50, Iter: 1530/2500, Train Loss: 0.1150\n",
      "Epoch: 4/50, Iter: 1540/2500, Train Loss: 0.0708\n",
      "Epoch: 4/50, Iter: 1550/2500, Train Loss: 0.1112\n",
      "Epoch: 4/50, Iter: 1560/2500, Train Loss: 0.0557\n",
      "Epoch: 4/50, Iter: 1570/2500, Train Loss: 0.1670\n",
      "Epoch: 4/50, Iter: 1580/2500, Train Loss: 0.0894\n",
      "Epoch: 4/50, Iter: 1590/2500, Train Loss: 0.0775\n",
      "Epoch: 4/50, Iter: 1600/2500, Train Loss: 0.0872\n",
      "Epoch: 4/50, Iter: 1610/2500, Train Loss: 0.1233\n",
      "Epoch: 4/50, Iter: 1620/2500, Train Loss: 0.0730\n",
      "Epoch: 4/50, Iter: 1630/2500, Train Loss: 0.2201\n",
      "Epoch: 4/50, Iter: 1640/2500, Train Loss: 0.0673\n",
      "Epoch: 4/50, Iter: 1650/2500, Train Loss: 0.1256\n",
      "Epoch: 4/50, Iter: 1660/2500, Train Loss: 0.1094\n",
      "Epoch: 4/50, Iter: 1670/2500, Train Loss: 0.0703\n",
      "Epoch: 4/50, Iter: 1680/2500, Train Loss: 0.0778\n",
      "Epoch: 4/50, Iter: 1690/2500, Train Loss: 0.0674\n",
      "Epoch: 4/50, Iter: 1700/2500, Train Loss: 0.0872\n",
      "Epoch: 4/50, Iter: 1710/2500, Train Loss: 0.1417\n",
      "Epoch: 4/50, Iter: 1720/2500, Train Loss: 0.0669\n",
      "Epoch: 4/50, Iter: 1730/2500, Train Loss: 0.0941\n",
      "Epoch: 4/50, Iter: 1740/2500, Train Loss: 0.1159\n",
      "Epoch: 4/50, Iter: 1750/2500, Train Loss: 0.0991\n",
      "Epoch: 4/50, Iter: 1760/2500, Train Loss: 0.0873\n",
      "Epoch: 4/50, Iter: 1770/2500, Train Loss: 0.0895\n",
      "Epoch: 4/50, Iter: 1780/2500, Train Loss: 0.1185\n",
      "Epoch: 4/50, Iter: 1790/2500, Train Loss: 0.0447\n",
      "Epoch: 4/50, Iter: 1800/2500, Train Loss: 0.1512\n",
      "Epoch: 4/50, Iter: 1810/2500, Train Loss: 0.1237\n",
      "Epoch: 4/50, Iter: 1820/2500, Train Loss: 0.0739\n",
      "Epoch: 4/50, Iter: 1830/2500, Train Loss: 0.1111\n",
      "Epoch: 4/50, Iter: 1840/2500, Train Loss: 0.1414\n",
      "Epoch: 4/50, Iter: 1850/2500, Train Loss: 0.0901\n",
      "Epoch: 4/50, Iter: 1860/2500, Train Loss: 0.1076\n",
      "Epoch: 4/50, Iter: 1870/2500, Train Loss: 0.1376\n",
      "Epoch: 4/50, Iter: 1880/2500, Train Loss: 0.0897\n",
      "Epoch: 4/50, Iter: 1890/2500, Train Loss: 0.0789\n",
      "Epoch: 4/50, Iter: 1900/2500, Train Loss: 0.1169\n",
      "Epoch: 4/50, Iter: 1910/2500, Train Loss: 0.0444\n",
      "Epoch: 4/50, Iter: 1920/2500, Train Loss: 0.0655\n",
      "Epoch: 4/50, Iter: 1930/2500, Train Loss: 0.1005\n",
      "Epoch: 4/50, Iter: 1940/2500, Train Loss: 0.0897\n",
      "Epoch: 4/50, Iter: 1950/2500, Train Loss: 0.1173\n",
      "Epoch: 4/50, Iter: 1960/2500, Train Loss: 0.1102\n",
      "Epoch: 4/50, Iter: 1970/2500, Train Loss: 0.0812\n",
      "Epoch: 4/50, Iter: 1980/2500, Train Loss: 0.0691\n",
      "Epoch: 4/50, Iter: 1990/2500, Train Loss: 0.1559\n",
      "Epoch: 4/50, Iter: 2000/2500, Train Loss: 0.0878\n",
      "Epoch: 4/50, Iter: 2010/2500, Train Loss: 0.0552\n",
      "Epoch: 4/50, Iter: 2020/2500, Train Loss: 0.0900\n",
      "Epoch: 4/50, Iter: 2030/2500, Train Loss: 0.0938\n",
      "Epoch: 4/50, Iter: 2040/2500, Train Loss: 0.1298\n",
      "Epoch: 4/50, Iter: 2050/2500, Train Loss: 0.1130\n",
      "Epoch: 4/50, Iter: 2060/2500, Train Loss: 0.1316\n",
      "Epoch: 4/50, Iter: 2070/2500, Train Loss: 0.1023\n",
      "Epoch: 4/50, Iter: 2080/2500, Train Loss: 0.0575\n",
      "Epoch: 4/50, Iter: 2090/2500, Train Loss: 0.0764\n",
      "Epoch: 4/50, Iter: 2100/2500, Train Loss: 0.0648\n",
      "Epoch: 4/50, Iter: 2110/2500, Train Loss: 0.0869\n",
      "Epoch: 4/50, Iter: 2120/2500, Train Loss: 0.0235\n",
      "Epoch: 4/50, Iter: 2130/2500, Train Loss: 0.0721\n",
      "Epoch: 4/50, Iter: 2140/2500, Train Loss: 0.0749\n",
      "Epoch: 4/50, Iter: 2150/2500, Train Loss: 0.0953\n",
      "Epoch: 4/50, Iter: 2160/2500, Train Loss: 0.1377\n",
      "Epoch: 4/50, Iter: 2170/2500, Train Loss: 0.0920\n",
      "Epoch: 4/50, Iter: 2180/2500, Train Loss: 0.1278\n",
      "Epoch: 4/50, Iter: 2190/2500, Train Loss: 0.1025\n",
      "Epoch: 4/50, Iter: 2200/2500, Train Loss: 0.1079\n",
      "Epoch: 4/50, Iter: 2210/2500, Train Loss: 0.0317\n",
      "Epoch: 4/50, Iter: 2220/2500, Train Loss: 0.0847\n",
      "Epoch: 4/50, Iter: 2230/2500, Train Loss: 0.0825\n",
      "Epoch: 4/50, Iter: 2240/2500, Train Loss: 0.0589\n",
      "Epoch: 4/50, Iter: 2250/2500, Train Loss: 0.0972\n",
      "Epoch: 4/50, Iter: 2260/2500, Train Loss: 0.0666\n",
      "Epoch: 4/50, Iter: 2270/2500, Train Loss: 0.0432\n",
      "Epoch: 4/50, Iter: 2280/2500, Train Loss: 0.0536\n",
      "Epoch: 4/50, Iter: 2290/2500, Train Loss: 0.1177\n",
      "Epoch: 4/50, Iter: 2300/2500, Train Loss: 0.1268\n",
      "Epoch: 4/50, Iter: 2310/2500, Train Loss: 0.0742\n",
      "Epoch: 4/50, Iter: 2320/2500, Train Loss: 0.0643\n",
      "Epoch: 4/50, Iter: 2330/2500, Train Loss: 0.0674\n",
      "Epoch: 4/50, Iter: 2340/2500, Train Loss: 0.1291\n",
      "Epoch: 4/50, Iter: 2350/2500, Train Loss: 0.1000\n",
      "Epoch: 4/50, Iter: 2360/2500, Train Loss: 0.0814\n",
      "Epoch: 4/50, Iter: 2370/2500, Train Loss: 0.1209\n",
      "Epoch: 4/50, Iter: 2380/2500, Train Loss: 0.0371\n",
      "Epoch: 4/50, Iter: 2390/2500, Train Loss: 0.1260\n",
      "Epoch: 4/50, Iter: 2400/2500, Train Loss: 0.0897\n",
      "Epoch: 4/50, Iter: 2410/2500, Train Loss: 0.0837\n",
      "Epoch: 4/50, Iter: 2420/2500, Train Loss: 0.1487\n",
      "Epoch: 4/50, Iter: 2430/2500, Train Loss: 0.0496\n",
      "Epoch: 4/50, Iter: 2440/2500, Train Loss: 0.1001\n",
      "Epoch: 4/50, Iter: 2450/2500, Train Loss: 0.1440\n",
      "Epoch: 4/50, Iter: 2460/2500, Train Loss: 0.1091\n",
      "Epoch: 4/50, Iter: 2470/2500, Train Loss: 0.1043\n",
      "Epoch: 4/50, Iter: 2480/2500, Train Loss: 0.0741\n",
      "Epoch: 4/50, Iter: 2490/2500, Train Loss: 0.0890\n",
      "Epoch: 4/50, Iter: 2500/2500, Train Loss: 0.0374\n",
      "Epoch 4 finished.\n",
      "Train Loss: 0.0918, Val Loss: 0.0996\n",
      "Saved best model to runs-fasterrcnn/faster_rcnn_resnet50_fpn_20250707-145842/best_model_epoch_4.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/50, Iter: 10/2500, Train Loss: 0.0847\n",
      "Epoch: 5/50, Iter: 20/2500, Train Loss: 0.0913\n",
      "Epoch: 5/50, Iter: 30/2500, Train Loss: 0.1233\n",
      "Epoch: 5/50, Iter: 40/2500, Train Loss: 0.0994\n",
      "Epoch: 5/50, Iter: 50/2500, Train Loss: 0.1025\n",
      "Epoch: 5/50, Iter: 60/2500, Train Loss: 0.0676\n",
      "Epoch: 5/50, Iter: 70/2500, Train Loss: 0.0489\n",
      "Epoch: 5/50, Iter: 80/2500, Train Loss: 0.0496\n",
      "Epoch: 5/50, Iter: 90/2500, Train Loss: 0.1146\n",
      "Epoch: 5/50, Iter: 100/2500, Train Loss: 0.1703\n",
      "Epoch: 5/50, Iter: 110/2500, Train Loss: 0.0786\n",
      "Epoch: 5/50, Iter: 120/2500, Train Loss: 0.0443\n",
      "Epoch: 5/50, Iter: 130/2500, Train Loss: 0.0537\n",
      "Epoch: 5/50, Iter: 140/2500, Train Loss: 0.1122\n",
      "Epoch: 5/50, Iter: 150/2500, Train Loss: 0.0670\n",
      "Epoch: 5/50, Iter: 160/2500, Train Loss: 0.0573\n",
      "Epoch: 5/50, Iter: 170/2500, Train Loss: 0.0896\n",
      "Epoch: 5/50, Iter: 180/2500, Train Loss: 0.1581\n",
      "Epoch: 5/50, Iter: 190/2500, Train Loss: 0.1744\n",
      "Epoch: 5/50, Iter: 200/2500, Train Loss: 0.0483\n",
      "Epoch: 5/50, Iter: 210/2500, Train Loss: 0.0899\n",
      "Epoch: 5/50, Iter: 220/2500, Train Loss: 0.0663\n",
      "Epoch: 5/50, Iter: 230/2500, Train Loss: 0.0810\n",
      "Epoch: 5/50, Iter: 240/2500, Train Loss: 0.0937\n",
      "Epoch: 5/50, Iter: 250/2500, Train Loss: 0.0765\n",
      "Epoch: 5/50, Iter: 260/2500, Train Loss: 0.0999\n",
      "Epoch: 5/50, Iter: 270/2500, Train Loss: 0.0991\n",
      "Epoch: 5/50, Iter: 280/2500, Train Loss: 0.1030\n",
      "Epoch: 5/50, Iter: 290/2500, Train Loss: 0.0976\n",
      "Epoch: 5/50, Iter: 300/2500, Train Loss: 0.1506\n",
      "Epoch: 5/50, Iter: 310/2500, Train Loss: 0.0536\n",
      "Epoch: 5/50, Iter: 320/2500, Train Loss: 0.1011\n",
      "Epoch: 5/50, Iter: 330/2500, Train Loss: 0.0644\n",
      "Epoch: 5/50, Iter: 340/2500, Train Loss: 0.0567\n",
      "Epoch: 5/50, Iter: 350/2500, Train Loss: 0.1217\n",
      "Epoch: 5/50, Iter: 360/2500, Train Loss: 0.0787\n",
      "Epoch: 5/50, Iter: 370/2500, Train Loss: 0.0466\n",
      "Epoch: 5/50, Iter: 380/2500, Train Loss: 0.0916\n",
      "Epoch: 5/50, Iter: 390/2500, Train Loss: 0.0540\n",
      "Epoch: 5/50, Iter: 400/2500, Train Loss: 0.0390\n",
      "Epoch: 5/50, Iter: 410/2500, Train Loss: 0.0256\n",
      "Epoch: 5/50, Iter: 420/2500, Train Loss: 0.0637\n",
      "Epoch: 5/50, Iter: 430/2500, Train Loss: 0.0915\n",
      "Epoch: 5/50, Iter: 440/2500, Train Loss: 0.1111\n",
      "Epoch: 5/50, Iter: 450/2500, Train Loss: 0.0556\n",
      "Epoch: 5/50, Iter: 460/2500, Train Loss: 0.0744\n",
      "Epoch: 5/50, Iter: 470/2500, Train Loss: 0.1040\n",
      "Epoch: 5/50, Iter: 480/2500, Train Loss: 0.1885\n",
      "Epoch: 5/50, Iter: 490/2500, Train Loss: 0.1379\n",
      "Epoch: 5/50, Iter: 500/2500, Train Loss: 0.1177\n",
      "Epoch: 5/50, Iter: 510/2500, Train Loss: 0.0753\n",
      "Epoch: 5/50, Iter: 520/2500, Train Loss: 0.1298\n",
      "Epoch: 5/50, Iter: 530/2500, Train Loss: 0.0529\n",
      "Epoch: 5/50, Iter: 540/2500, Train Loss: 0.0841\n",
      "Epoch: 5/50, Iter: 550/2500, Train Loss: 0.0720\n",
      "Epoch: 5/50, Iter: 560/2500, Train Loss: 0.0851\n",
      "Epoch: 5/50, Iter: 570/2500, Train Loss: 0.0864\n",
      "Epoch: 5/50, Iter: 580/2500, Train Loss: 0.1316\n",
      "Epoch: 5/50, Iter: 590/2500, Train Loss: 0.1026\n",
      "Epoch: 5/50, Iter: 600/2500, Train Loss: 0.1457\n",
      "Epoch: 5/50, Iter: 610/2500, Train Loss: 0.1028\n",
      "Epoch: 5/50, Iter: 620/2500, Train Loss: 0.0586\n",
      "Epoch: 5/50, Iter: 630/2500, Train Loss: 0.0869\n",
      "Epoch: 5/50, Iter: 640/2500, Train Loss: 0.1112\n",
      "Epoch: 5/50, Iter: 650/2500, Train Loss: 0.1345\n",
      "Epoch: 5/50, Iter: 660/2500, Train Loss: 0.0634\n",
      "Epoch: 5/50, Iter: 670/2500, Train Loss: 0.0993\n",
      "Epoch: 5/50, Iter: 680/2500, Train Loss: 0.0575\n",
      "Epoch: 5/50, Iter: 690/2500, Train Loss: 0.0764\n",
      "Epoch: 5/50, Iter: 700/2500, Train Loss: 0.1100\n",
      "Epoch: 5/50, Iter: 710/2500, Train Loss: 0.1200\n",
      "Epoch: 5/50, Iter: 720/2500, Train Loss: 0.0764\n",
      "Epoch: 5/50, Iter: 730/2500, Train Loss: 0.1543\n",
      "Epoch: 5/50, Iter: 740/2500, Train Loss: 0.0578\n",
      "Epoch: 5/50, Iter: 750/2500, Train Loss: 0.0503\n",
      "Epoch: 5/50, Iter: 760/2500, Train Loss: 0.0588\n",
      "Epoch: 5/50, Iter: 770/2500, Train Loss: 0.0617\n",
      "Epoch: 5/50, Iter: 780/2500, Train Loss: 0.1075\n",
      "Epoch: 5/50, Iter: 790/2500, Train Loss: 0.0719\n",
      "Epoch: 5/50, Iter: 800/2500, Train Loss: 0.1848\n",
      "Epoch: 5/50, Iter: 810/2500, Train Loss: 0.0686\n",
      "Epoch: 5/50, Iter: 820/2500, Train Loss: 0.0555\n",
      "Epoch: 5/50, Iter: 830/2500, Train Loss: 0.0777\n",
      "Epoch: 5/50, Iter: 840/2500, Train Loss: 0.0570\n",
      "Epoch: 5/50, Iter: 850/2500, Train Loss: 0.0821\n",
      "Epoch: 5/50, Iter: 860/2500, Train Loss: 0.0647\n",
      "Epoch: 5/50, Iter: 870/2500, Train Loss: 0.0716\n",
      "Epoch: 5/50, Iter: 880/2500, Train Loss: 0.0361\n",
      "Epoch: 5/50, Iter: 890/2500, Train Loss: 0.1048\n",
      "Epoch: 5/50, Iter: 900/2500, Train Loss: 0.0381\n",
      "Epoch: 5/50, Iter: 910/2500, Train Loss: 0.0719\n",
      "Epoch: 5/50, Iter: 920/2500, Train Loss: 0.0557\n",
      "Epoch: 5/50, Iter: 930/2500, Train Loss: 0.0733\n",
      "Epoch: 5/50, Iter: 940/2500, Train Loss: 0.1012\n",
      "Epoch: 5/50, Iter: 950/2500, Train Loss: 0.1050\n",
      "Epoch: 5/50, Iter: 960/2500, Train Loss: 0.0528\n",
      "Epoch: 5/50, Iter: 970/2500, Train Loss: 0.0405\n",
      "Epoch: 5/50, Iter: 980/2500, Train Loss: 0.0732\n",
      "Epoch: 5/50, Iter: 990/2500, Train Loss: 0.1012\n",
      "Epoch: 5/50, Iter: 1000/2500, Train Loss: 0.0834\n",
      "Epoch: 5/50, Iter: 1010/2500, Train Loss: 0.0632\n",
      "Epoch: 5/50, Iter: 1020/2500, Train Loss: 0.0928\n",
      "Epoch: 5/50, Iter: 1030/2500, Train Loss: 0.1124\n",
      "Epoch: 5/50, Iter: 1040/2500, Train Loss: 0.1633\n",
      "Epoch: 5/50, Iter: 1050/2500, Train Loss: 0.0675\n",
      "Epoch: 5/50, Iter: 1060/2500, Train Loss: 0.0825\n",
      "Epoch: 5/50, Iter: 1070/2500, Train Loss: 0.0859\n",
      "Epoch: 5/50, Iter: 1080/2500, Train Loss: 0.0461\n",
      "Epoch: 5/50, Iter: 1090/2500, Train Loss: 0.0673\n",
      "Epoch: 5/50, Iter: 1100/2500, Train Loss: 0.1092\n",
      "Epoch: 5/50, Iter: 1110/2500, Train Loss: 0.1113\n",
      "Epoch: 5/50, Iter: 1120/2500, Train Loss: 0.1357\n",
      "Epoch: 5/50, Iter: 1130/2500, Train Loss: 0.1161\n",
      "Epoch: 5/50, Iter: 1140/2500, Train Loss: 0.1185\n",
      "Epoch: 5/50, Iter: 1150/2500, Train Loss: 0.0296\n",
      "Epoch: 5/50, Iter: 1160/2500, Train Loss: 0.1083\n",
      "Epoch: 5/50, Iter: 1170/2500, Train Loss: 0.0580\n",
      "Epoch: 5/50, Iter: 1180/2500, Train Loss: 0.0851\n",
      "Epoch: 5/50, Iter: 1190/2500, Train Loss: 0.0680\n",
      "Epoch: 5/50, Iter: 1200/2500, Train Loss: 0.0715\n",
      "Epoch: 5/50, Iter: 1210/2500, Train Loss: 0.0981\n",
      "Epoch: 5/50, Iter: 1220/2500, Train Loss: 0.0901\n",
      "Epoch: 5/50, Iter: 1230/2500, Train Loss: 0.1260\n",
      "Epoch: 5/50, Iter: 1240/2500, Train Loss: 0.0491\n",
      "Epoch: 5/50, Iter: 1250/2500, Train Loss: 0.3202\n",
      "Epoch: 5/50, Iter: 1260/2500, Train Loss: 0.0510\n",
      "Epoch: 5/50, Iter: 1270/2500, Train Loss: 0.0511\n",
      "Epoch: 5/50, Iter: 1280/2500, Train Loss: 0.0560\n",
      "Epoch: 5/50, Iter: 1290/2500, Train Loss: 0.1023\n",
      "Epoch: 5/50, Iter: 1300/2500, Train Loss: 0.1099\n",
      "Epoch: 5/50, Iter: 1310/2500, Train Loss: 0.0713\n",
      "Epoch: 5/50, Iter: 1320/2500, Train Loss: 0.1610\n",
      "Epoch: 5/50, Iter: 1330/2500, Train Loss: 0.1151\n",
      "Epoch: 5/50, Iter: 1340/2500, Train Loss: 0.1107\n",
      "Epoch: 5/50, Iter: 1350/2500, Train Loss: 0.0636\n",
      "Epoch: 5/50, Iter: 1360/2500, Train Loss: 0.1022\n",
      "Epoch: 5/50, Iter: 1370/2500, Train Loss: 0.1582\n",
      "Epoch: 5/50, Iter: 1380/2500, Train Loss: 0.0939\n",
      "Epoch: 5/50, Iter: 1390/2500, Train Loss: 0.0679\n",
      "Epoch: 5/50, Iter: 1400/2500, Train Loss: 0.1027\n",
      "Epoch: 5/50, Iter: 1410/2500, Train Loss: 0.1574\n",
      "Epoch: 5/50, Iter: 1420/2500, Train Loss: 0.0690\n",
      "Epoch: 5/50, Iter: 1430/2500, Train Loss: 0.0752\n",
      "Epoch: 5/50, Iter: 1440/2500, Train Loss: 0.0970\n",
      "Epoch: 5/50, Iter: 1450/2500, Train Loss: 0.1839\n",
      "Epoch: 5/50, Iter: 1460/2500, Train Loss: 0.0932\n",
      "Epoch: 5/50, Iter: 1470/2500, Train Loss: 0.0526\n",
      "Epoch: 5/50, Iter: 1480/2500, Train Loss: 0.0862\n",
      "Epoch: 5/50, Iter: 1490/2500, Train Loss: 0.0534\n",
      "Epoch: 5/50, Iter: 1500/2500, Train Loss: 0.0896\n",
      "Epoch: 5/50, Iter: 1510/2500, Train Loss: 0.0678\n",
      "Epoch: 5/50, Iter: 1520/2500, Train Loss: 0.0759\n",
      "Epoch: 5/50, Iter: 1530/2500, Train Loss: 0.1061\n",
      "Epoch: 5/50, Iter: 1540/2500, Train Loss: 0.1418\n",
      "Epoch: 5/50, Iter: 1550/2500, Train Loss: 0.1984\n",
      "Epoch: 5/50, Iter: 1560/2500, Train Loss: 0.1967\n",
      "Epoch: 5/50, Iter: 1570/2500, Train Loss: 0.0707\n",
      "Epoch: 5/50, Iter: 1580/2500, Train Loss: 0.0768\n",
      "Epoch: 5/50, Iter: 1590/2500, Train Loss: 0.0565\n",
      "Epoch: 5/50, Iter: 1600/2500, Train Loss: 0.0557\n",
      "Epoch: 5/50, Iter: 1610/2500, Train Loss: 0.0930\n",
      "Epoch: 5/50, Iter: 1620/2500, Train Loss: 0.0861\n",
      "Epoch: 5/50, Iter: 1630/2500, Train Loss: 0.1191\n",
      "Epoch: 5/50, Iter: 1640/2500, Train Loss: 0.0609\n",
      "Epoch: 5/50, Iter: 1650/2500, Train Loss: 0.1421\n",
      "Epoch: 5/50, Iter: 1660/2500, Train Loss: 0.0942\n",
      "Epoch: 5/50, Iter: 1670/2500, Train Loss: 0.1041\n",
      "Epoch: 5/50, Iter: 1680/2500, Train Loss: 0.0427\n",
      "Epoch: 5/50, Iter: 1690/2500, Train Loss: 0.1490\n",
      "Epoch: 5/50, Iter: 1700/2500, Train Loss: 0.0581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/50, Iter: 1710/2500, Train Loss: 0.1274\n",
      "Epoch: 5/50, Iter: 1720/2500, Train Loss: 0.0574\n",
      "Epoch: 5/50, Iter: 1730/2500, Train Loss: 0.0439\n",
      "Epoch: 5/50, Iter: 1740/2500, Train Loss: 0.1536\n",
      "Epoch: 5/50, Iter: 1750/2500, Train Loss: 0.1065\n",
      "Epoch: 5/50, Iter: 1760/2500, Train Loss: 0.0598\n",
      "Epoch: 5/50, Iter: 1770/2500, Train Loss: 0.1511\n",
      "Epoch: 5/50, Iter: 1780/2500, Train Loss: 0.0514\n",
      "Epoch: 5/50, Iter: 1790/2500, Train Loss: 0.0500\n",
      "Epoch: 5/50, Iter: 1800/2500, Train Loss: 0.1107\n",
      "Epoch: 5/50, Iter: 1810/2500, Train Loss: 0.0858\n",
      "Epoch: 5/50, Iter: 1820/2500, Train Loss: 0.0932\n",
      "Epoch: 5/50, Iter: 1830/2500, Train Loss: 0.2033\n",
      "Epoch: 5/50, Iter: 1840/2500, Train Loss: 0.1030\n",
      "Epoch: 5/50, Iter: 1850/2500, Train Loss: 0.0771\n",
      "Epoch: 5/50, Iter: 1860/2500, Train Loss: 0.2034\n",
      "Epoch: 5/50, Iter: 1870/2500, Train Loss: 0.1015\n",
      "Epoch: 5/50, Iter: 1880/2500, Train Loss: 0.1247\n",
      "Epoch: 5/50, Iter: 1890/2500, Train Loss: 0.0816\n",
      "Epoch: 5/50, Iter: 1900/2500, Train Loss: 0.0826\n",
      "Epoch: 5/50, Iter: 1910/2500, Train Loss: 0.0439\n",
      "Epoch: 5/50, Iter: 1920/2500, Train Loss: 0.1122\n",
      "Epoch: 5/50, Iter: 1930/2500, Train Loss: 0.0815\n",
      "Epoch: 5/50, Iter: 1940/2500, Train Loss: 0.1051\n",
      "Epoch: 5/50, Iter: 1950/2500, Train Loss: 0.1052\n",
      "Epoch: 5/50, Iter: 1960/2500, Train Loss: 0.1162\n",
      "Epoch: 5/50, Iter: 1970/2500, Train Loss: 0.0333\n",
      "Epoch: 5/50, Iter: 1980/2500, Train Loss: 0.1104\n",
      "Epoch: 5/50, Iter: 1990/2500, Train Loss: 0.0653\n",
      "Epoch: 5/50, Iter: 2000/2500, Train Loss: 0.1362\n",
      "Epoch: 5/50, Iter: 2010/2500, Train Loss: 0.1278\n",
      "Epoch: 5/50, Iter: 2020/2500, Train Loss: 0.0329\n",
      "Epoch: 5/50, Iter: 2030/2500, Train Loss: 0.0561\n",
      "Epoch: 5/50, Iter: 2040/2500, Train Loss: 0.0677\n",
      "Epoch: 5/50, Iter: 2050/2500, Train Loss: 0.0570\n",
      "Epoch: 5/50, Iter: 2060/2500, Train Loss: 0.0426\n",
      "Epoch: 5/50, Iter: 2070/2500, Train Loss: 0.0640\n",
      "Epoch: 5/50, Iter: 2080/2500, Train Loss: 0.0506\n",
      "Epoch: 5/50, Iter: 2090/2500, Train Loss: 0.0980\n",
      "Epoch: 5/50, Iter: 2100/2500, Train Loss: 0.0823\n",
      "Epoch: 5/50, Iter: 2110/2500, Train Loss: 0.0614\n",
      "Epoch: 5/50, Iter: 2120/2500, Train Loss: 0.0583\n",
      "Epoch: 5/50, Iter: 2130/2500, Train Loss: 0.0953\n",
      "Epoch: 5/50, Iter: 2140/2500, Train Loss: 0.0786\n",
      "Epoch: 5/50, Iter: 2150/2500, Train Loss: 0.0987\n",
      "Epoch: 5/50, Iter: 2160/2500, Train Loss: 0.2402\n",
      "Epoch: 5/50, Iter: 2170/2500, Train Loss: 0.0457\n",
      "Epoch: 5/50, Iter: 2180/2500, Train Loss: 0.0900\n",
      "Epoch: 5/50, Iter: 2190/2500, Train Loss: 0.0524\n",
      "Epoch: 5/50, Iter: 2200/2500, Train Loss: 0.0600\n",
      "Epoch: 5/50, Iter: 2210/2500, Train Loss: 0.1065\n",
      "Epoch: 5/50, Iter: 2220/2500, Train Loss: 0.0564\n",
      "Epoch: 5/50, Iter: 2230/2500, Train Loss: 0.0336\n",
      "Epoch: 5/50, Iter: 2240/2500, Train Loss: 0.0558\n",
      "Epoch: 5/50, Iter: 2250/2500, Train Loss: 0.0689\n",
      "Epoch: 5/50, Iter: 2260/2500, Train Loss: 0.0592\n",
      "Epoch: 5/50, Iter: 2270/2500, Train Loss: 0.0236\n",
      "Epoch: 5/50, Iter: 2280/2500, Train Loss: 0.0812\n",
      "Epoch: 5/50, Iter: 2290/2500, Train Loss: 0.0704\n",
      "Epoch: 5/50, Iter: 2300/2500, Train Loss: 0.2153\n",
      "Epoch: 5/50, Iter: 2310/2500, Train Loss: 0.0938\n",
      "Epoch: 5/50, Iter: 2320/2500, Train Loss: 0.0790\n",
      "Epoch: 5/50, Iter: 2330/2500, Train Loss: 0.0946\n",
      "Epoch: 5/50, Iter: 2340/2500, Train Loss: 0.0701\n",
      "Epoch: 5/50, Iter: 2350/2500, Train Loss: 0.0687\n",
      "Epoch: 5/50, Iter: 2360/2500, Train Loss: 0.0760\n",
      "Epoch: 5/50, Iter: 2370/2500, Train Loss: 0.0629\n",
      "Epoch: 5/50, Iter: 2380/2500, Train Loss: 0.1119\n",
      "Epoch: 5/50, Iter: 2390/2500, Train Loss: 0.0904\n",
      "Epoch: 5/50, Iter: 2400/2500, Train Loss: 0.1709\n",
      "Epoch: 5/50, Iter: 2410/2500, Train Loss: 0.1016\n",
      "Epoch: 5/50, Iter: 2420/2500, Train Loss: 0.0492\n",
      "Epoch: 5/50, Iter: 2430/2500, Train Loss: 0.0614\n",
      "Epoch: 5/50, Iter: 2440/2500, Train Loss: 0.1560\n",
      "Epoch: 5/50, Iter: 2450/2500, Train Loss: 0.1065\n",
      "Epoch: 5/50, Iter: 2460/2500, Train Loss: 0.0474\n",
      "Epoch: 5/50, Iter: 2470/2500, Train Loss: 0.2265\n",
      "Epoch: 5/50, Iter: 2480/2500, Train Loss: 0.1046\n",
      "Epoch: 5/50, Iter: 2490/2500, Train Loss: 0.0572\n",
      "Epoch: 5/50, Iter: 2500/2500, Train Loss: 0.2527\n",
      "Epoch 5 finished.\n",
      "Train Loss: 0.0881, Val Loss: 0.0978\n",
      "Saved best model to runs-fasterrcnn/faster_rcnn_resnet50_fpn_20250707-145842/best_model_epoch_5.pth\n",
      "Saved checkpoint to runs-fasterrcnn/faster_rcnn_resnet50_fpn_20250707-145842/checkpoint_epoch_5.pth\n",
      "Epoch: 6/50, Iter: 10/2500, Train Loss: 0.0590\n",
      "Epoch: 6/50, Iter: 20/2500, Train Loss: 0.1028\n",
      "Epoch: 6/50, Iter: 30/2500, Train Loss: 0.2084\n",
      "Epoch: 6/50, Iter: 40/2500, Train Loss: 0.0183\n",
      "Epoch: 6/50, Iter: 50/2500, Train Loss: 0.0627\n",
      "Epoch: 6/50, Iter: 60/2500, Train Loss: 0.0322\n",
      "Epoch: 6/50, Iter: 70/2500, Train Loss: 0.1013\n",
      "Epoch: 6/50, Iter: 80/2500, Train Loss: 0.0587\n",
      "Epoch: 6/50, Iter: 90/2500, Train Loss: 0.0669\n",
      "Epoch: 6/50, Iter: 100/2500, Train Loss: 0.0621\n",
      "Epoch: 6/50, Iter: 110/2500, Train Loss: 0.1427\n",
      "Epoch: 6/50, Iter: 120/2500, Train Loss: 0.0994\n",
      "Epoch: 6/50, Iter: 130/2500, Train Loss: 0.0669\n",
      "Epoch: 6/50, Iter: 140/2500, Train Loss: 0.1231\n",
      "Epoch: 6/50, Iter: 150/2500, Train Loss: 0.0478\n",
      "Epoch: 6/50, Iter: 160/2500, Train Loss: 0.0570\n",
      "Epoch: 6/50, Iter: 170/2500, Train Loss: 0.0422\n",
      "Epoch: 6/50, Iter: 180/2500, Train Loss: 0.0386\n",
      "Epoch: 6/50, Iter: 190/2500, Train Loss: 0.0379\n",
      "Epoch: 6/50, Iter: 200/2500, Train Loss: 0.0721\n",
      "Epoch: 6/50, Iter: 210/2500, Train Loss: 0.0677\n",
      "Epoch: 6/50, Iter: 220/2500, Train Loss: 0.1473\n",
      "Epoch: 6/50, Iter: 230/2500, Train Loss: 0.0959\n",
      "Epoch: 6/50, Iter: 240/2500, Train Loss: 0.1010\n",
      "Epoch: 6/50, Iter: 250/2500, Train Loss: 0.0716\n",
      "Epoch: 6/50, Iter: 260/2500, Train Loss: 0.0926\n",
      "Epoch: 6/50, Iter: 270/2500, Train Loss: 0.1097\n",
      "Epoch: 6/50, Iter: 280/2500, Train Loss: 0.0690\n",
      "Epoch: 6/50, Iter: 290/2500, Train Loss: 0.1440\n",
      "Epoch: 6/50, Iter: 300/2500, Train Loss: 0.0789\n",
      "Epoch: 6/50, Iter: 310/2500, Train Loss: 0.0662\n",
      "Epoch: 6/50, Iter: 320/2500, Train Loss: 0.0756\n",
      "Epoch: 6/50, Iter: 330/2500, Train Loss: 0.0839\n",
      "Epoch: 6/50, Iter: 340/2500, Train Loss: 0.0510\n",
      "Epoch: 6/50, Iter: 350/2500, Train Loss: 0.0594\n",
      "Epoch: 6/50, Iter: 360/2500, Train Loss: 0.0905\n",
      "Epoch: 6/50, Iter: 370/2500, Train Loss: 0.0535\n",
      "Epoch: 6/50, Iter: 380/2500, Train Loss: 0.0523\n",
      "Epoch: 6/50, Iter: 390/2500, Train Loss: 0.0902\n",
      "Epoch: 6/50, Iter: 400/2500, Train Loss: 0.0602\n",
      "Epoch: 6/50, Iter: 410/2500, Train Loss: 0.0934\n",
      "Epoch: 6/50, Iter: 420/2500, Train Loss: 0.0515\n",
      "Epoch: 6/50, Iter: 430/2500, Train Loss: 0.0686\n",
      "Epoch: 6/50, Iter: 440/2500, Train Loss: 0.0825\n",
      "Epoch: 6/50, Iter: 450/2500, Train Loss: 0.0454\n",
      "Epoch: 6/50, Iter: 460/2500, Train Loss: 0.0504\n",
      "Epoch: 6/50, Iter: 470/2500, Train Loss: 0.0659\n",
      "Epoch: 6/50, Iter: 480/2500, Train Loss: 0.0981\n",
      "Epoch: 6/50, Iter: 490/2500, Train Loss: 0.1200\n",
      "Epoch: 6/50, Iter: 500/2500, Train Loss: 0.1215\n",
      "Epoch: 6/50, Iter: 510/2500, Train Loss: 0.0500\n",
      "Epoch: 6/50, Iter: 520/2500, Train Loss: 0.0706\n",
      "Epoch: 6/50, Iter: 530/2500, Train Loss: 0.0536\n",
      "Epoch: 6/50, Iter: 540/2500, Train Loss: 0.0649\n",
      "Epoch: 6/50, Iter: 550/2500, Train Loss: 0.0662\n",
      "Epoch: 6/50, Iter: 560/2500, Train Loss: 0.0494\n",
      "Epoch: 6/50, Iter: 570/2500, Train Loss: 0.0658\n",
      "Epoch: 6/50, Iter: 580/2500, Train Loss: 0.1067\n",
      "Epoch: 6/50, Iter: 590/2500, Train Loss: 0.0633\n",
      "Epoch: 6/50, Iter: 600/2500, Train Loss: 0.0485\n",
      "Epoch: 6/50, Iter: 610/2500, Train Loss: 0.0835\n",
      "Epoch: 6/50, Iter: 620/2500, Train Loss: 0.0966\n",
      "Epoch: 6/50, Iter: 630/2500, Train Loss: 0.0889\n",
      "Epoch: 6/50, Iter: 640/2500, Train Loss: 0.0525\n",
      "Epoch: 6/50, Iter: 650/2500, Train Loss: 0.1409\n",
      "Epoch: 6/50, Iter: 660/2500, Train Loss: 0.0627\n",
      "Epoch: 6/50, Iter: 670/2500, Train Loss: 0.1612\n",
      "Epoch: 6/50, Iter: 680/2500, Train Loss: 0.0653\n",
      "Epoch: 6/50, Iter: 690/2500, Train Loss: 0.1432\n",
      "Epoch: 6/50, Iter: 700/2500, Train Loss: 0.1128\n",
      "Epoch: 6/50, Iter: 710/2500, Train Loss: 0.0628\n",
      "Epoch: 6/50, Iter: 720/2500, Train Loss: 0.0625\n",
      "Epoch: 6/50, Iter: 730/2500, Train Loss: 0.0683\n",
      "Epoch: 6/50, Iter: 740/2500, Train Loss: 0.0518\n",
      "Epoch: 6/50, Iter: 750/2500, Train Loss: 0.0580\n",
      "Epoch: 6/50, Iter: 760/2500, Train Loss: 0.0386\n",
      "Epoch: 6/50, Iter: 770/2500, Train Loss: 0.0934\n",
      "Epoch: 6/50, Iter: 780/2500, Train Loss: 0.1088\n",
      "Epoch: 6/50, Iter: 790/2500, Train Loss: 0.1004\n",
      "Epoch: 6/50, Iter: 800/2500, Train Loss: 0.0972\n",
      "Epoch: 6/50, Iter: 810/2500, Train Loss: 0.0567\n",
      "Epoch: 6/50, Iter: 820/2500, Train Loss: 0.1286\n",
      "Epoch: 6/50, Iter: 830/2500, Train Loss: 0.0764\n",
      "Epoch: 6/50, Iter: 840/2500, Train Loss: 0.0487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/50, Iter: 850/2500, Train Loss: 0.0534\n",
      "Epoch: 6/50, Iter: 860/2500, Train Loss: 0.1128\n",
      "Epoch: 6/50, Iter: 870/2500, Train Loss: 0.0796\n",
      "Epoch: 6/50, Iter: 880/2500, Train Loss: 0.0717\n",
      "Epoch: 6/50, Iter: 890/2500, Train Loss: 0.0801\n",
      "Epoch: 6/50, Iter: 900/2500, Train Loss: 0.0788\n",
      "Epoch: 6/50, Iter: 910/2500, Train Loss: 0.0695\n",
      "Epoch: 6/50, Iter: 920/2500, Train Loss: 0.0505\n",
      "Epoch: 6/50, Iter: 930/2500, Train Loss: 0.0783\n",
      "Epoch: 6/50, Iter: 940/2500, Train Loss: 0.0966\n",
      "Epoch: 6/50, Iter: 950/2500, Train Loss: 0.0637\n",
      "Epoch: 6/50, Iter: 960/2500, Train Loss: 0.0684\n",
      "Epoch: 6/50, Iter: 970/2500, Train Loss: 0.1053\n",
      "Epoch: 6/50, Iter: 980/2500, Train Loss: 0.0944\n",
      "Epoch: 6/50, Iter: 990/2500, Train Loss: 0.1028\n",
      "Epoch: 6/50, Iter: 1000/2500, Train Loss: 0.0769\n",
      "Epoch: 6/50, Iter: 1010/2500, Train Loss: 0.0820\n",
      "Epoch: 6/50, Iter: 1020/2500, Train Loss: 0.0884\n",
      "Epoch: 6/50, Iter: 1030/2500, Train Loss: 0.0761\n",
      "Epoch: 6/50, Iter: 1040/2500, Train Loss: 0.0832\n",
      "Epoch: 6/50, Iter: 1050/2500, Train Loss: 0.0782\n",
      "Epoch: 6/50, Iter: 1060/2500, Train Loss: 0.0761\n",
      "Epoch: 6/50, Iter: 1070/2500, Train Loss: 0.0826\n",
      "Epoch: 6/50, Iter: 1080/2500, Train Loss: 0.1069\n",
      "Epoch: 6/50, Iter: 1090/2500, Train Loss: 0.1557\n",
      "Epoch: 6/50, Iter: 1100/2500, Train Loss: 0.0602\n",
      "Epoch: 6/50, Iter: 1110/2500, Train Loss: 0.0885\n",
      "Epoch: 6/50, Iter: 1120/2500, Train Loss: 0.0720\n",
      "Epoch: 6/50, Iter: 1130/2500, Train Loss: 0.0897\n",
      "Epoch: 6/50, Iter: 1140/2500, Train Loss: 0.0192\n",
      "Epoch: 6/50, Iter: 1150/2500, Train Loss: 0.0567\n",
      "Epoch: 6/50, Iter: 1160/2500, Train Loss: 0.0827\n",
      "Epoch: 6/50, Iter: 1170/2500, Train Loss: 0.0671\n",
      "Epoch: 6/50, Iter: 1180/2500, Train Loss: 0.0969\n",
      "Epoch: 6/50, Iter: 1190/2500, Train Loss: 0.0701\n",
      "Epoch: 6/50, Iter: 1200/2500, Train Loss: 0.0980\n",
      "Epoch: 6/50, Iter: 1210/2500, Train Loss: 0.0576\n",
      "Epoch: 6/50, Iter: 1220/2500, Train Loss: 0.0624\n",
      "Epoch: 6/50, Iter: 1230/2500, Train Loss: 0.0891\n",
      "Epoch: 6/50, Iter: 1240/2500, Train Loss: 0.0896\n",
      "Epoch: 6/50, Iter: 1250/2500, Train Loss: 0.0731\n",
      "Epoch: 6/50, Iter: 1260/2500, Train Loss: 0.0494\n",
      "Epoch: 6/50, Iter: 1270/2500, Train Loss: 0.0467\n",
      "Epoch: 6/50, Iter: 1280/2500, Train Loss: 0.0705\n",
      "Epoch: 6/50, Iter: 1290/2500, Train Loss: 0.0667\n",
      "Epoch: 6/50, Iter: 1300/2500, Train Loss: 0.0572\n",
      "Epoch: 6/50, Iter: 1310/2500, Train Loss: 0.0976\n",
      "Epoch: 6/50, Iter: 1320/2500, Train Loss: 0.1214\n",
      "Epoch: 6/50, Iter: 1330/2500, Train Loss: 0.0985\n",
      "Epoch: 6/50, Iter: 1340/2500, Train Loss: 0.0953\n",
      "Epoch: 6/50, Iter: 1350/2500, Train Loss: 0.0568\n",
      "Epoch: 6/50, Iter: 1360/2500, Train Loss: 0.1601\n",
      "Epoch: 6/50, Iter: 1370/2500, Train Loss: 0.0611\n",
      "Epoch: 6/50, Iter: 1380/2500, Train Loss: 0.0879\n",
      "Epoch: 6/50, Iter: 1390/2500, Train Loss: 0.0579\n",
      "Epoch: 6/50, Iter: 1400/2500, Train Loss: 0.1370\n",
      "Epoch: 6/50, Iter: 1410/2500, Train Loss: 0.0817\n",
      "Epoch: 6/50, Iter: 1420/2500, Train Loss: 0.0276\n",
      "Epoch: 6/50, Iter: 1430/2500, Train Loss: 0.0601\n",
      "Epoch: 6/50, Iter: 1440/2500, Train Loss: 0.0942\n",
      "Epoch: 6/50, Iter: 1450/2500, Train Loss: 0.0779\n",
      "Epoch: 6/50, Iter: 1460/2500, Train Loss: 0.2158\n",
      "Epoch: 6/50, Iter: 1470/2500, Train Loss: 0.0677\n",
      "Epoch: 6/50, Iter: 1480/2500, Train Loss: 0.0901\n",
      "Epoch: 6/50, Iter: 1490/2500, Train Loss: 0.0541\n",
      "Epoch: 6/50, Iter: 1500/2500, Train Loss: 0.0729\n",
      "Epoch: 6/50, Iter: 1510/2500, Train Loss: 0.0639\n",
      "Epoch: 6/50, Iter: 1520/2500, Train Loss: 0.1123\n",
      "Epoch: 6/50, Iter: 1530/2500, Train Loss: 0.0931\n",
      "Epoch: 6/50, Iter: 1540/2500, Train Loss: 0.1092\n",
      "Epoch: 6/50, Iter: 1550/2500, Train Loss: 0.1153\n",
      "Epoch: 6/50, Iter: 1560/2500, Train Loss: 0.0710\n",
      "Epoch: 6/50, Iter: 1570/2500, Train Loss: 0.0313\n",
      "Epoch: 6/50, Iter: 1580/2500, Train Loss: 0.0605\n",
      "Epoch: 6/50, Iter: 1590/2500, Train Loss: 0.0950\n",
      "Epoch: 6/50, Iter: 1600/2500, Train Loss: 0.0708\n",
      "Epoch: 6/50, Iter: 1610/2500, Train Loss: 0.0724\n",
      "Epoch: 6/50, Iter: 1620/2500, Train Loss: 0.0270\n",
      "Epoch: 6/50, Iter: 1630/2500, Train Loss: 0.0385\n",
      "Epoch: 6/50, Iter: 1640/2500, Train Loss: 0.0736\n",
      "Epoch: 6/50, Iter: 1650/2500, Train Loss: 0.1042\n",
      "Epoch: 6/50, Iter: 1660/2500, Train Loss: 0.0778\n",
      "Epoch: 6/50, Iter: 1670/2500, Train Loss: 0.1219\n",
      "Epoch: 6/50, Iter: 1680/2500, Train Loss: 0.1043\n",
      "Epoch: 6/50, Iter: 1690/2500, Train Loss: 0.0757\n",
      "Epoch: 6/50, Iter: 1700/2500, Train Loss: 0.0500\n",
      "Epoch: 6/50, Iter: 1710/2500, Train Loss: 0.0565\n",
      "Epoch: 6/50, Iter: 1720/2500, Train Loss: 0.0775\n",
      "Epoch: 6/50, Iter: 1730/2500, Train Loss: 0.0716\n",
      "Epoch: 6/50, Iter: 1740/2500, Train Loss: 0.1096\n",
      "Epoch: 6/50, Iter: 1750/2500, Train Loss: 0.0608\n",
      "Epoch: 6/50, Iter: 1760/2500, Train Loss: 0.0536\n",
      "Epoch: 6/50, Iter: 1770/2500, Train Loss: 0.0473\n"
     ]
    }
   ],
   "source": [
    "# train_faster_rcnn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms as T\n",
    "import os\n",
    "import datetime\n",
    "import yaml # Import yaml to load class names\n",
    "\n",
    "# Import your custom dataset and collate_fn\n",
    "# from faster_rcnn_dataset import WakeDetectionDataset, collate_fn, Compose, ToTensor\n",
    "\n",
    "# --- Configuration ---\n",
    "DATASET_ROOT = \"../Dataset\"\n",
    "ANNOTATION_FILE = \"../Dataset/faster_rcnn_annotations.json\"\n",
    "NUM_CLASSES = 2 # Placeholder, will be determined from YAML\n",
    "BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 0.005\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0.0005\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "OUTPUT_DIR = \"runs-fasterrcnn\"\n",
    "MODEL_NAME = \"faster_rcnn_resnet50_fpn\"\n",
    "\n",
    "def get_model(num_classes):\n",
    "    model = fasterrcnn_resnet50_fpn(weights='DEFAULT')\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "def get_transform(is_train):\n",
    "    transforms = [ToTensor()]\n",
    "    # Add data augmentation for training\n",
    "    if is_train:\n",
    "        # Example: Random Horizontal Flip (make sure it handles bounding boxes correctly)\n",
    "        # For torchvision transforms, you often need to define custom ones or use libraries like Albumentations\n",
    "        # Here's a placeholder for torchvision's T.RandomHorizontalFlip which does NOT automatically flip boxes.\n",
    "        # For object detection, you typically need to implement your own transforms that modify both image and target.\n",
    "        pass # Add more transforms here compatible with object detection\n",
    "    return Compose(transforms)\n",
    "\n",
    "def train():\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    run_name = f\"{MODEL_NAME}_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    run_output_path = os.path.join(OUTPUT_DIR, run_name)\n",
    "    os.makedirs(run_output_path, exist_ok=True)\n",
    "\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    # Load class names from your YAML file to determine NUM_CLASSES\n",
    "    yaml_path = \"../Dataset/vessel_wakes.yaml\"\n",
    "    try:\n",
    "        with open(yaml_path, 'r') as f:\n",
    "            yaml_config = yaml.safe_load(f)\n",
    "            class_names_dict = yaml_config['names']\n",
    "            raw_class_names = [class_names_dict[i] for i in sorted(class_names_dict.keys())]\n",
    "            global NUM_CLASSES\n",
    "            NUM_CLASSES = len(raw_class_names) + 1 # +1 for background\n",
    "            print(f\"Inferred {len(raw_class_names)} custom classes. Setting NUM_CLASSES for Faster R-CNN to {NUM_CLASSES} (including background).\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {yaml_path} not found. Please check the path.\")\n",
    "        exit()\n",
    "    except KeyError:\n",
    "        print(\"Error: 'names' key not found in your YAML config. Ensure it defines your class names.\")\n",
    "        exit()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading YAML: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # Prepare datasets and dataloaders\n",
    "    # NOW WE LOAD TRAIN AND VALIDATION DATASETS SEPARATELY\n",
    "    train_dataset = WakeDetectionDataset(\n",
    "        root_dir=DATASET_ROOT,\n",
    "        annotation_file=ANNOTATION_FILE,\n",
    "        split='train', # Specify the 'train' split\n",
    "        transform=get_transform(is_train=True)\n",
    "    )\n",
    "\n",
    "    val_dataset = WakeDetectionDataset(\n",
    "        root_dir=DATASET_ROOT,\n",
    "        annotation_file=ANNOTATION_FILE,\n",
    "        split='valid', # Specify the 'valid' split\n",
    "        transform=get_transform(is_train=False) # No augmentation for validation\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=os.cpu_count() // 2,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False, # No need to shuffle validation data\n",
    "        num_workers=os.cpu_count() // 2,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "    print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "\n",
    "    model = get_model(NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=LEARNING_RATE,\n",
    "                                momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for i, (images, targets) in enumerate(train_loader):\n",
    "            images = list(image.to(DEVICE) for image in images)\n",
    "            targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += losses.item()\n",
    "\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"Epoch: {epoch+1}/{NUM_EPOCHS}, Iter: {i+1}/{len(train_loader)}, Train Loss: {losses.item():.4f}\")\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for i, (images, targets) in enumerate(val_loader):\n",
    "                images = list(image.to(DEVICE) for image in images)\n",
    "                targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "                # Model outputs are predictions during eval, not loss dict directly\n",
    "                # To calculate loss during validation, you can either:\n",
    "                # 1. Temporarily set model.train() then back to eval() after calculating loss.\n",
    "                # 2. Or, if you primarily care about eval metrics (mAP, etc.), you'd use a different loop\n",
    "                #    and an evaluator like pycocotools.\n",
    "                # For simplicity here, we will temporarily set to train to get losses,\n",
    "                # as direct loss calculation on validation set is common during training loop.\n",
    "                model.train() # Temporarily set to train mode to get loss_dict from model()\n",
    "                loss_dict = model(images, targets)\n",
    "                model.eval() # Set back to eval mode\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "                val_loss += losses.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} finished.\")\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Save the best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            model_save_path = os.path.join(run_output_path, f\"best_model_epoch_{epoch+1}.pth\")\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"Saved best model to {model_save_path}\")\n",
    "\n",
    "        # Save checkpoint periodically\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            checkpoint_path = os.path.join(run_output_path, f\"checkpoint_epoch_{epoch+1}.pth\")\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"Saved checkpoint to {checkpoint_path}\")\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "    return run_output_path # Return the path to the run directory\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90129b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Inferred 1 custom classes. Setting NUM_CLASSES to 2.\n",
      "Train dataset size: 9997\n",
      "Validation dataset size: 3443\n",
      "Starting training...\n",
      "Epoch: 1/50, Iter: 100/2500, Train Loss: 0.0467\n",
      "Epoch: 1/50, Iter: 200/2500, Train Loss: 0.0593\n",
      "Epoch: 1/50, Iter: 300/2500, Train Loss: 0.0406\n",
      "Epoch: 1/50, Iter: 400/2500, Train Loss: 0.0434\n",
      "Epoch: 1/50, Iter: 500/2500, Train Loss: 0.0291\n",
      "Epoch: 1/50, Iter: 600/2500, Train Loss: 0.0297\n",
      "Epoch: 1/50, Iter: 700/2500, Train Loss: 0.0346\n",
      "Epoch: 1/50, Iter: 800/2500, Train Loss: 0.0365\n",
      "Epoch: 1/50, Iter: 900/2500, Train Loss: 0.0312\n",
      "Epoch: 1/50, Iter: 1000/2500, Train Loss: 0.0259\n",
      "Epoch: 1/50, Iter: 1100/2500, Train Loss: 0.0355\n",
      "Epoch: 1/50, Iter: 1200/2500, Train Loss: 0.0255\n",
      "Epoch: 1/50, Iter: 1300/2500, Train Loss: 0.0127\n",
      "Epoch: 1/50, Iter: 1400/2500, Train Loss: 0.0237\n",
      "Epoch: 1/50, Iter: 1500/2500, Train Loss: 0.0284\n",
      "Epoch: 1/50, Iter: 1600/2500, Train Loss: 0.0220\n",
      "Epoch: 1/50, Iter: 1700/2500, Train Loss: 0.0146\n",
      "Epoch: 1/50, Iter: 1800/2500, Train Loss: 0.0224\n",
      "Epoch: 1/50, Iter: 1900/2500, Train Loss: 0.0268\n",
      "Epoch: 1/50, Iter: 2000/2500, Train Loss: 0.0183\n",
      "Epoch: 1/50, Iter: 2100/2500, Train Loss: 0.0244\n",
      "Epoch: 1/50, Iter: 2200/2500, Train Loss: 0.0293\n",
      "Epoch: 1/50, Iter: 2300/2500, Train Loss: 0.0274\n",
      "Epoch: 1/50, Iter: 2400/2500, Train Loss: 0.0265\n",
      "Epoch: 1/50, Iter: 2500/2500, Train Loss: 0.0336\n",
      "Epoch 1 finished.\n",
      "Train Loss: 0.0356, Val Loss: 0.0255\n",
      "Saved best model to runs-fasterrcnn2/faster_rcnn_resnet50_fpn_20250811-005533/best_model_epoch_1.pth\n",
      "Epoch: 2/50, Iter: 100/2500, Train Loss: 0.0202\n",
      "Epoch: 2/50, Iter: 200/2500, Train Loss: 0.0136\n",
      "Epoch: 2/50, Iter: 300/2500, Train Loss: 0.0419\n",
      "Epoch: 2/50, Iter: 400/2500, Train Loss: 0.0220\n",
      "Epoch: 2/50, Iter: 500/2500, Train Loss: 0.0116\n",
      "Epoch: 2/50, Iter: 600/2500, Train Loss: 0.0295\n",
      "Epoch: 2/50, Iter: 700/2500, Train Loss: 0.0260\n",
      "Epoch: 2/50, Iter: 800/2500, Train Loss: 0.0176\n",
      "Epoch: 2/50, Iter: 900/2500, Train Loss: 0.0154\n",
      "Epoch: 2/50, Iter: 1000/2500, Train Loss: 0.0336\n",
      "Epoch: 2/50, Iter: 1100/2500, Train Loss: 0.0262\n",
      "Epoch: 2/50, Iter: 1200/2500, Train Loss: 0.0235\n",
      "Epoch: 2/50, Iter: 1300/2500, Train Loss: 0.0225\n",
      "Epoch: 2/50, Iter: 1400/2500, Train Loss: 0.0260\n",
      "Epoch: 2/50, Iter: 1500/2500, Train Loss: 0.0171\n",
      "Epoch: 2/50, Iter: 1600/2500, Train Loss: 0.0279\n",
      "Epoch: 2/50, Iter: 1700/2500, Train Loss: 0.0213\n",
      "Epoch: 2/50, Iter: 1800/2500, Train Loss: 0.0259\n",
      "Epoch: 2/50, Iter: 1900/2500, Train Loss: 0.0100\n",
      "Epoch: 2/50, Iter: 2000/2500, Train Loss: 0.0226\n",
      "Epoch: 2/50, Iter: 2100/2500, Train Loss: 0.0253\n",
      "Epoch: 2/50, Iter: 2200/2500, Train Loss: 0.0351\n",
      "Epoch: 2/50, Iter: 2300/2500, Train Loss: 0.0431\n",
      "Epoch: 2/50, Iter: 2400/2500, Train Loss: 0.0276\n",
      "Epoch: 2/50, Iter: 2500/2500, Train Loss: 0.0554\n",
      "Epoch 2 finished.\n",
      "Train Loss: 0.0245, Val Loss: 0.0246\n",
      "Saved best model to runs-fasterrcnn2/faster_rcnn_resnet50_fpn_20250811-005533/best_model_epoch_2.pth\n",
      "Epoch: 3/50, Iter: 100/2500, Train Loss: 0.0209\n",
      "Epoch: 3/50, Iter: 200/2500, Train Loss: 0.0296\n",
      "Epoch: 3/50, Iter: 300/2500, Train Loss: 0.0192\n",
      "Epoch: 3/50, Iter: 400/2500, Train Loss: 0.0194\n",
      "Epoch: 3/50, Iter: 500/2500, Train Loss: 0.0158\n",
      "Epoch: 3/50, Iter: 600/2500, Train Loss: 0.0212\n",
      "Epoch: 3/50, Iter: 700/2500, Train Loss: 0.0179\n",
      "Epoch: 3/50, Iter: 800/2500, Train Loss: 0.0286\n",
      "Epoch: 3/50, Iter: 900/2500, Train Loss: 0.0363\n",
      "Epoch: 3/50, Iter: 1000/2500, Train Loss: 0.0168\n",
      "Epoch: 3/50, Iter: 1100/2500, Train Loss: 0.0239\n",
      "Epoch: 3/50, Iter: 1200/2500, Train Loss: 0.0067\n",
      "Epoch: 3/50, Iter: 1300/2500, Train Loss: 0.0303\n",
      "Epoch: 3/50, Iter: 1400/2500, Train Loss: 0.0284\n",
      "Epoch: 3/50, Iter: 1500/2500, Train Loss: 0.0222\n",
      "Epoch: 3/50, Iter: 1600/2500, Train Loss: 0.0148\n",
      "Epoch: 3/50, Iter: 1700/2500, Train Loss: 0.0248\n",
      "Epoch: 3/50, Iter: 1800/2500, Train Loss: 0.0544\n",
      "Epoch: 3/50, Iter: 1900/2500, Train Loss: 0.0103\n",
      "Epoch: 3/50, Iter: 2000/2500, Train Loss: 0.0209\n",
      "Epoch: 3/50, Iter: 2100/2500, Train Loss: 0.0319\n",
      "Epoch: 3/50, Iter: 2200/2500, Train Loss: 0.0207\n",
      "Epoch: 3/50, Iter: 2300/2500, Train Loss: 0.0177\n",
      "Epoch: 3/50, Iter: 2400/2500, Train Loss: 0.0126\n",
      "Epoch: 3/50, Iter: 2500/2500, Train Loss: 0.0453\n",
      "Epoch 3 finished.\n",
      "Train Loss: 0.0227, Val Loss: 0.0212\n",
      "Saved best model to runs-fasterrcnn2/faster_rcnn_resnet50_fpn_20250811-005533/best_model_epoch_3.pth\n",
      "Epoch: 4/50, Iter: 100/2500, Train Loss: 0.0147\n",
      "Epoch: 4/50, Iter: 200/2500, Train Loss: 0.0171\n",
      "Epoch: 4/50, Iter: 300/2500, Train Loss: 0.0290\n",
      "Epoch: 4/50, Iter: 400/2500, Train Loss: 0.0262\n",
      "Epoch: 4/50, Iter: 500/2500, Train Loss: 0.0222\n",
      "Epoch: 4/50, Iter: 600/2500, Train Loss: 0.0111\n",
      "Epoch: 4/50, Iter: 700/2500, Train Loss: 0.0160\n",
      "Epoch: 4/50, Iter: 800/2500, Train Loss: 0.0178\n",
      "Epoch: 4/50, Iter: 900/2500, Train Loss: 0.0177\n",
      "Epoch: 4/50, Iter: 1000/2500, Train Loss: 0.0102\n",
      "Epoch: 4/50, Iter: 1100/2500, Train Loss: 0.0167\n",
      "Epoch: 4/50, Iter: 1200/2500, Train Loss: 0.0155\n",
      "Epoch: 4/50, Iter: 1300/2500, Train Loss: 0.0216\n",
      "Epoch: 4/50, Iter: 1400/2500, Train Loss: 0.0127\n",
      "Epoch: 4/50, Iter: 1500/2500, Train Loss: 0.0164\n",
      "Epoch: 4/50, Iter: 1600/2500, Train Loss: 0.0151\n",
      "Epoch: 4/50, Iter: 1700/2500, Train Loss: 0.0153\n",
      "Epoch: 4/50, Iter: 1800/2500, Train Loss: 0.0292\n",
      "Epoch: 4/50, Iter: 1900/2500, Train Loss: 0.0257\n",
      "Epoch: 4/50, Iter: 2000/2500, Train Loss: 0.0125\n",
      "Epoch: 4/50, Iter: 2100/2500, Train Loss: 0.0353\n",
      "Epoch: 4/50, Iter: 2200/2500, Train Loss: 0.0178\n",
      "Epoch: 4/50, Iter: 2300/2500, Train Loss: 0.0179\n",
      "Epoch: 4/50, Iter: 2400/2500, Train Loss: 0.0155\n",
      "Epoch: 4/50, Iter: 2500/2500, Train Loss: 0.0297\n",
      "Epoch 4 finished.\n",
      "Train Loss: 0.0208, Val Loss: 0.0207\n",
      "Saved best model to runs-fasterrcnn2/faster_rcnn_resnet50_fpn_20250811-005533/best_model_epoch_4.pth\n",
      "Epoch: 5/50, Iter: 100/2500, Train Loss: 0.0079\n",
      "Epoch: 5/50, Iter: 200/2500, Train Loss: 0.0147\n",
      "Epoch: 5/50, Iter: 300/2500, Train Loss: 0.0254\n",
      "Epoch: 5/50, Iter: 400/2500, Train Loss: 0.0235\n",
      "Epoch: 5/50, Iter: 500/2500, Train Loss: 0.0144\n",
      "Epoch: 5/50, Iter: 600/2500, Train Loss: 0.0136\n",
      "Epoch: 5/50, Iter: 700/2500, Train Loss: 0.0081\n",
      "Epoch: 5/50, Iter: 800/2500, Train Loss: 0.0215\n",
      "Epoch: 5/50, Iter: 900/2500, Train Loss: 0.0637\n",
      "Epoch: 5/50, Iter: 1000/2500, Train Loss: 0.0221\n",
      "Epoch: 5/50, Iter: 1100/2500, Train Loss: 0.0210\n",
      "Epoch: 5/50, Iter: 1200/2500, Train Loss: 0.0203\n",
      "Epoch: 5/50, Iter: 1300/2500, Train Loss: 0.0246\n",
      "Epoch: 5/50, Iter: 1400/2500, Train Loss: 0.0182\n",
      "Epoch: 5/50, Iter: 1500/2500, Train Loss: 0.0235\n",
      "Epoch: 5/50, Iter: 1600/2500, Train Loss: 0.0226\n",
      "Epoch: 5/50, Iter: 1700/2500, Train Loss: 0.0148\n",
      "Epoch: 5/50, Iter: 1800/2500, Train Loss: 0.0137\n",
      "Epoch: 5/50, Iter: 1900/2500, Train Loss: 0.0237\n",
      "Epoch: 5/50, Iter: 2000/2500, Train Loss: 0.0124\n",
      "Epoch: 5/50, Iter: 2100/2500, Train Loss: 0.0439\n",
      "Epoch: 5/50, Iter: 2200/2500, Train Loss: 0.0235\n",
      "Epoch: 5/50, Iter: 2300/2500, Train Loss: 0.0246\n",
      "Epoch: 5/50, Iter: 2400/2500, Train Loss: 0.0295\n",
      "Epoch: 5/50, Iter: 2500/2500, Train Loss: 0.0519\n",
      "Epoch 5 finished.\n",
      "Train Loss: 0.0202, Val Loss: 0.0219\n",
      "Saved checkpoint to runs-fasterrcnn2/faster_rcnn_resnet50_fpn_20250811-005533/checkpoint_epoch_5.pth\n",
      "Epoch: 6/50, Iter: 100/2500, Train Loss: 0.0241\n",
      "Epoch: 6/50, Iter: 200/2500, Train Loss: 0.0126\n",
      "Epoch: 6/50, Iter: 300/2500, Train Loss: 0.0066\n",
      "Epoch: 6/50, Iter: 400/2500, Train Loss: 0.0132\n",
      "Epoch: 6/50, Iter: 500/2500, Train Loss: 0.0200\n",
      "Epoch: 6/50, Iter: 600/2500, Train Loss: 0.0135\n",
      "Epoch: 6/50, Iter: 700/2500, Train Loss: 0.0206\n",
      "Epoch: 6/50, Iter: 800/2500, Train Loss: 0.0164\n",
      "Epoch: 6/50, Iter: 900/2500, Train Loss: 0.0171\n",
      "Epoch: 6/50, Iter: 1000/2500, Train Loss: 0.0199\n",
      "Epoch: 6/50, Iter: 1100/2500, Train Loss: 0.0113\n",
      "Epoch: 6/50, Iter: 1200/2500, Train Loss: 0.0149\n",
      "Epoch: 6/50, Iter: 1300/2500, Train Loss: 0.0053\n",
      "Epoch: 6/50, Iter: 1400/2500, Train Loss: 0.0119\n",
      "Epoch: 6/50, Iter: 1500/2500, Train Loss: 0.0119\n",
      "Epoch: 6/50, Iter: 1600/2500, Train Loss: 0.0096\n",
      "Epoch: 6/50, Iter: 1700/2500, Train Loss: 0.0094\n",
      "Epoch: 6/50, Iter: 1800/2500, Train Loss: 0.0242\n",
      "Epoch: 6/50, Iter: 1900/2500, Train Loss: 0.0122\n",
      "Epoch: 6/50, Iter: 2000/2500, Train Loss: 0.0062\n",
      "Epoch: 6/50, Iter: 2100/2500, Train Loss: 0.0098\n",
      "Epoch: 6/50, Iter: 2200/2500, Train Loss: 0.0169\n",
      "Epoch: 6/50, Iter: 2300/2500, Train Loss: 0.0173\n",
      "Epoch: 6/50, Iter: 2400/2500, Train Loss: 0.0103\n",
      "Epoch: 6/50, Iter: 2500/2500, Train Loss: 0.0096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 finished.\n",
      "Train Loss: 0.0177, Val Loss: 0.0184\n",
      "Saved best model to runs-fasterrcnn2/faster_rcnn_resnet50_fpn_20250811-005533/best_model_epoch_6.pth\n",
      "Epoch: 7/50, Iter: 100/2500, Train Loss: 0.0154\n",
      "Epoch: 7/50, Iter: 200/2500, Train Loss: 0.0167\n",
      "Epoch: 7/50, Iter: 300/2500, Train Loss: 0.0912\n",
      "Epoch: 7/50, Iter: 400/2500, Train Loss: 0.0146\n",
      "Epoch: 7/50, Iter: 500/2500, Train Loss: 0.0302\n",
      "Epoch: 7/50, Iter: 600/2500, Train Loss: 0.0384\n",
      "Epoch: 7/50, Iter: 700/2500, Train Loss: 0.0137\n",
      "Epoch: 7/50, Iter: 800/2500, Train Loss: 0.0144\n",
      "Epoch: 7/50, Iter: 900/2500, Train Loss: 0.0390\n",
      "Epoch: 7/50, Iter: 1000/2500, Train Loss: 0.0092\n",
      "Epoch: 7/50, Iter: 1100/2500, Train Loss: 0.0121\n",
      "Epoch: 7/50, Iter: 1200/2500, Train Loss: 0.0090\n",
      "Epoch: 7/50, Iter: 1300/2500, Train Loss: 0.0145\n",
      "Epoch: 7/50, Iter: 1400/2500, Train Loss: 0.0337\n",
      "Epoch: 7/50, Iter: 1500/2500, Train Loss: 0.0088\n",
      "Epoch: 7/50, Iter: 1600/2500, Train Loss: 0.0230\n",
      "Epoch: 7/50, Iter: 1700/2500, Train Loss: 0.0170\n",
      "Epoch: 7/50, Iter: 1800/2500, Train Loss: 0.0128\n",
      "Epoch: 7/50, Iter: 1900/2500, Train Loss: 0.0181\n",
      "Epoch: 7/50, Iter: 2000/2500, Train Loss: 0.0058\n",
      "Epoch: 7/50, Iter: 2100/2500, Train Loss: 0.0390\n",
      "Epoch: 7/50, Iter: 2200/2500, Train Loss: 0.0095\n",
      "Epoch: 7/50, Iter: 2300/2500, Train Loss: 0.0283\n",
      "Epoch: 7/50, Iter: 2400/2500, Train Loss: 0.0099\n",
      "Epoch: 7/50, Iter: 2500/2500, Train Loss: 0.0172\n",
      "Epoch 7 finished.\n",
      "Train Loss: 0.0173, Val Loss: 0.0179\n",
      "Saved best model to runs-fasterrcnn2/faster_rcnn_resnet50_fpn_20250811-005533/best_model_epoch_7.pth\n",
      "Epoch: 8/50, Iter: 100/2500, Train Loss: 0.0143\n",
      "Epoch: 8/50, Iter: 200/2500, Train Loss: 0.0127\n",
      "Epoch: 8/50, Iter: 300/2500, Train Loss: 0.0153\n",
      "Epoch: 8/50, Iter: 400/2500, Train Loss: 0.0098\n",
      "Epoch: 8/50, Iter: 500/2500, Train Loss: 0.0119\n",
      "Epoch: 8/50, Iter: 600/2500, Train Loss: 0.0104\n",
      "Epoch: 8/50, Iter: 700/2500, Train Loss: 0.0201\n",
      "Epoch: 8/50, Iter: 800/2500, Train Loss: 0.0100\n",
      "Epoch: 8/50, Iter: 900/2500, Train Loss: 0.0163\n",
      "Epoch: 8/50, Iter: 1000/2500, Train Loss: 0.0080\n",
      "Epoch: 8/50, Iter: 1100/2500, Train Loss: 0.0187\n",
      "Epoch: 8/50, Iter: 1200/2500, Train Loss: 0.0110\n",
      "Epoch: 8/50, Iter: 1300/2500, Train Loss: 0.0079\n",
      "Epoch: 8/50, Iter: 1400/2500, Train Loss: 0.0161\n",
      "Epoch: 8/50, Iter: 1500/2500, Train Loss: 0.0266\n",
      "Epoch: 8/50, Iter: 1600/2500, Train Loss: 0.0131\n",
      "Epoch: 8/50, Iter: 1700/2500, Train Loss: 0.0153\n",
      "Epoch: 8/50, Iter: 1800/2500, Train Loss: 0.0031\n",
      "Epoch: 8/50, Iter: 1900/2500, Train Loss: 0.0159\n",
      "Epoch: 8/50, Iter: 2000/2500, Train Loss: 0.0049\n",
      "Epoch: 8/50, Iter: 2100/2500, Train Loss: 0.0196\n",
      "Epoch: 8/50, Iter: 2200/2500, Train Loss: 0.0091\n",
      "Epoch: 8/50, Iter: 2300/2500, Train Loss: 0.0103\n",
      "Epoch: 8/50, Iter: 2400/2500, Train Loss: 0.0362\n",
      "Epoch: 8/50, Iter: 2500/2500, Train Loss: 0.0082\n",
      "Epoch 8 finished.\n",
      "Train Loss: 0.0170, Val Loss: 0.0180\n",
      "Epoch: 9/50, Iter: 100/2500, Train Loss: 0.0273\n",
      "Epoch: 9/50, Iter: 200/2500, Train Loss: 0.0153\n",
      "Epoch: 9/50, Iter: 300/2500, Train Loss: 0.0241\n",
      "Epoch: 9/50, Iter: 400/2500, Train Loss: 0.0086\n",
      "Epoch: 9/50, Iter: 500/2500, Train Loss: 0.0169\n",
      "Epoch: 9/50, Iter: 600/2500, Train Loss: 0.0124\n",
      "Epoch: 9/50, Iter: 700/2500, Train Loss: 0.0151\n",
      "Epoch: 9/50, Iter: 800/2500, Train Loss: 0.0125\n",
      "Epoch: 9/50, Iter: 900/2500, Train Loss: 0.0159\n",
      "Epoch: 9/50, Iter: 1000/2500, Train Loss: 0.0362\n",
      "Epoch: 9/50, Iter: 1100/2500, Train Loss: 0.0124\n",
      "Epoch: 9/50, Iter: 1200/2500, Train Loss: 0.0210\n",
      "Epoch: 9/50, Iter: 1300/2500, Train Loss: 0.0325\n",
      "Epoch: 9/50, Iter: 1400/2500, Train Loss: 0.0101\n",
      "Epoch: 9/50, Iter: 1500/2500, Train Loss: 0.0282\n",
      "Epoch: 9/50, Iter: 1600/2500, Train Loss: 0.0230\n",
      "Epoch: 9/50, Iter: 1700/2500, Train Loss: 0.0156\n",
      "Epoch: 9/50, Iter: 1800/2500, Train Loss: 0.0097\n",
      "Epoch: 9/50, Iter: 1900/2500, Train Loss: 0.0209\n",
      "Epoch: 9/50, Iter: 2000/2500, Train Loss: 0.0132\n",
      "Epoch: 9/50, Iter: 2100/2500, Train Loss: 0.0086\n",
      "Epoch: 9/50, Iter: 2200/2500, Train Loss: 0.0110\n",
      "Epoch: 9/50, Iter: 2300/2500, Train Loss: 0.0109\n",
      "Epoch: 9/50, Iter: 2400/2500, Train Loss: 0.0280\n",
      "Epoch: 9/50, Iter: 2500/2500, Train Loss: 0.0130\n",
      "Epoch 9 finished.\n",
      "Train Loss: 0.0169, Val Loss: 0.0179\n",
      "Epoch: 10/50, Iter: 100/2500, Train Loss: 0.0190\n",
      "Epoch: 10/50, Iter: 200/2500, Train Loss: 0.0201\n",
      "Epoch: 10/50, Iter: 300/2500, Train Loss: 0.0292\n",
      "Epoch: 10/50, Iter: 400/2500, Train Loss: 0.0102\n",
      "Epoch: 10/50, Iter: 500/2500, Train Loss: 0.0164\n",
      "Epoch: 10/50, Iter: 600/2500, Train Loss: 0.0182\n",
      "Epoch: 10/50, Iter: 700/2500, Train Loss: 0.0187\n",
      "Epoch: 10/50, Iter: 800/2500, Train Loss: 0.0128\n",
      "Epoch: 10/50, Iter: 900/2500, Train Loss: 0.0107\n",
      "Epoch: 10/50, Iter: 1000/2500, Train Loss: 0.0137\n"
     ]
    }
   ],
   "source": [
    "# second train maybe it reduce FP\n",
    "\n",
    "# train_faster_rcnn (modified saving = same names, new folder)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms as T\n",
    "import os\n",
    "import datetime\n",
    "import yaml\n",
    "\n",
    "\n",
    "# Use your custom dataset + collate + transforms that handle boxes\n",
    "# from faster_rcnn_dataset import WakeDetectionDataset, collate_fn, Compose, ToTensor\n",
    "\n",
    "# --- Configuration ---\n",
    "DATASET_ROOT = \"../Dataset\"\n",
    "ANNOTATION_FILE = \"../Dataset/faster_rcnn_annotations.json\"\n",
    "NUM_CLASSES = 2  # will be inferred from YAML (+1 for background)\n",
    "BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 0.005\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0.0005\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "OUTPUT_DIR = \"runs-fasterrcnn2\"\n",
    "MODEL_NAME = \"faster_rcnn_resnet50_fpn\"\n",
    "\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "try:\n",
    "    from torchvision.models import ResNet50_Weights\n",
    "    BACKBONE_WEIGHTS = ResNet50_Weights.DEFAULT\n",
    "except Exception:\n",
    "    BACKBONE_WEIGHTS = None\n",
    "\n",
    "def get_model(num_classes):\n",
    "    # Use the same number of sizes on each of the 5 FPN levels (here: 1 per level).\n",
    "    sizes = ((32,), (64,), (128,), (256,), (512,))\n",
    "    aspect = (0.2, 0.33, 0.5, 1.0, 2.0, 3.0, 5.0)  # long/thin-friendly\n",
    "    aspect_ratios = (aspect, aspect, aspect, aspect, aspect)\n",
    "\n",
    "    anchor_generator = AnchorGenerator(\n",
    "        sizes=sizes,\n",
    "        aspect_ratios=aspect_ratios\n",
    "    )\n",
    "\n",
    "    model = fasterrcnn_resnet50_fpn(\n",
    "        weights=None,                       # avoid head mismatch when anchors change\n",
    "        weights_backbone=BACKBONE_WEIGHTS,  # init backbone from ImageNet\n",
    "        rpn_anchor_generator=anchor_generator,\n",
    "        rpn_nms_thresh=0.5,\n",
    "        box_score_thresh=0.35,\n",
    "        box_nms_thresh=0.30,\n",
    "        box_detections_per_img=75,\n",
    "        box_fg_iou_thresh=0.60,\n",
    "        box_bg_iou_thresh=0.40,\n",
    "        box_batch_size_per_image=512,\n",
    "        box_positive_fraction=0.20,\n",
    "        min_size=1000,\n",
    "        max_size=1800,\n",
    "    )\n",
    "\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "def get_transform(is_train):\n",
    "    transforms = [ToTensor()]\n",
    "    # (Optional) add your train-time aug here if your Compose supports boxes\n",
    "    return Compose(transforms)\n",
    "\n",
    "def train():\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    run_name = f\"{MODEL_NAME}_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    run_output_path = os.path.join(OUTPUT_DIR, run_name)\n",
    "    os.makedirs(run_output_path, exist_ok=True)\n",
    "\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    # Load class names from YAML to determine NUM_CLASSES\n",
    "    yaml_path = \"../Dataset/vessel_wakes.yaml\"\n",
    "    try:\n",
    "        with open(yaml_path, 'r') as f:\n",
    "            yaml_config = yaml.safe_load(f)\n",
    "            class_names = yaml_config['names']\n",
    "            if isinstance(class_names, dict):\n",
    "                raw_class_names = [class_names[i] for i in sorted(class_names.keys())]\n",
    "            else:\n",
    "                raw_class_names = list(class_names)\n",
    "            global NUM_CLASSES\n",
    "            NUM_CLASSES = len(raw_class_names) + 1  # +1 for background\n",
    "            print(f\"Inferred {len(raw_class_names)} custom classes. Setting NUM_CLASSES to {NUM_CLASSES}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {yaml_path} not found. Please check the path.\")\n",
    "        exit()\n",
    "    except KeyError:\n",
    "        print(\"Error: 'names' key not found in your YAML config. Ensure it defines your class names.\")\n",
    "        exit()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading YAML: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # Datasets and loaders\n",
    "    train_dataset = WakeDetectionDataset(\n",
    "        root_dir=DATASET_ROOT,\n",
    "        annotation_file=ANNOTATION_FILE,\n",
    "        split='train',\n",
    "        transform=get_transform(is_train=True)\n",
    "    )\n",
    "    val_dataset = WakeDetectionDataset(\n",
    "        root_dir=DATASET_ROOT,\n",
    "        annotation_file=ANNOTATION_FILE,\n",
    "        split='valid',\n",
    "        transform=get_transform(is_train=False)\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=os.cpu_count() // 2,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=os.cpu_count() // 2,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "    print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "\n",
    "    model = get_model(NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for i, (images, targets) in enumerate(train_loader):\n",
    "            images = [img.to(DEVICE) for img in images]\n",
    "            targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += losses.item()\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Epoch: {epoch+1}/{NUM_EPOCHS}, Iter: {i+1}/{len(train_loader)}, Train Loss: {losses.item():.4f}\")\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # Validation (compute loss; keep same pattern)\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, (images, targets) in enumerate(val_loader):\n",
    "                images = [img.to(DEVICE) for img in images]\n",
    "                targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "                model.train()  # temporarily to get loss dict\n",
    "                loss_dict = model(images, targets)\n",
    "                model.eval()\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "                val_loss += losses.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} finished.\")\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # --- Save EXACTLY like your original (just inside the new run folder) ---\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            model_save_path = os.path.join(run_output_path, f\"best_model_epoch_{epoch+1}.pth\")\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"Saved best model to {model_save_path}\")\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            checkpoint_path = os.path.join(run_output_path, f\"checkpoint_epoch_{epoch+1}.pth\")\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"Saved checkpoint to {checkpoint_path}\")\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "    return run_output_path\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52a8c34a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.26.4\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.6\n",
      "    Uninstalling numpy-2.2.6:\n",
      "      Successfully uninstalled numpy-2.2.6\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.26.4 --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f41b4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# post processing\n",
    "# Keep only predictions with score > 0.6\n",
    "\n",
    "# Apply Non-Maximum Suppression (NMS) with IoU > 0.9 to remove boxes that almost\n",
    "# entirely overlap (it keeps the higher score box)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def postprocess_predictions(boxes, scores, labels, score_thresh=0.6, iou_thresh=0.9):\n",
    "    \"\"\"\n",
    "    Filters boxes by confidence score, then applies NMS to remove duplicates.\n",
    "\n",
    "    Args:\n",
    "        boxes (ndarray): shape [N, 4] -> [xmin, ymin, xmax, ymax]\n",
    "        scores (ndarray): shape [N]\n",
    "        labels (ndarray): shape [N]\n",
    "        score_thresh (float): score cutoff (keep boxes with score > this)\n",
    "        iou_thresh (float): IoU threshold for removing overlapping boxes\n",
    "\n",
    "    Returns:\n",
    "        filtered_boxes, filtered_scores, filtered_labels\n",
    "    \"\"\"\n",
    "\n",
    "    # Track how many boxes we start with\n",
    "    initial_count = len(scores)\n",
    "\n",
    "    # --- 1️⃣ Keep only boxes with score > threshold ---\n",
    "#     keep = scores > score_thresh\n",
    "#     boxes = boxes[keep]\n",
    "#     scores = scores[keep]\n",
    "#     labels = labels[keep]\n",
    "\n",
    "    removed_low_score = initial_count - len(scores)\n",
    "\n",
    "#     if len(boxes) == 0:\n",
    "# #         print(f\"[Postprocess] Removed {removed_low_score} low-score boxes, nothing left after filtering.\")\n",
    "#         return boxes, scores, labels, initial_count, removed_low_score, 0  # nothing left to filter 0 = no overlap was removed\n",
    "\n",
    "    # --- 2️⃣ Sort boxes by score (highest first) ---\n",
    "    order = scores.argsort()[::-1]\n",
    "    boxes = boxes[order]\n",
    "    scores = scores[order]\n",
    "    labels = labels[order]\n",
    "\n",
    "    # --- 3️⃣ NMS: Remove duplicates with IoU > iou_thresh ---\n",
    "    keep_indices = []\n",
    "    removed_nms = 0  # counter for how many boxes we remove due to overlap\n",
    "\n",
    "    while len(boxes) > 0:\n",
    "        # Always keep the top-scoring box\n",
    "        keep_indices.append(0)\n",
    "\n",
    "        if len(boxes) == 1:\n",
    "            break\n",
    "\n",
    "        # Compute IoU between the kept box and the rest\n",
    "        ious = compute_iou_batch(boxes[0], boxes[1:])\n",
    "\n",
    "        # Boxes to keep are those with IoU <= threshold\n",
    "        remain = np.where(ious <= iou_thresh)[0] + 1  # +1 because of slice offset\n",
    "\n",
    "        # Count how many overlapping boxes were removed in this round\n",
    "        removed_nms += (len(ious) - len(remain))\n",
    "\n",
    "        # Update the list of boxes and scores\n",
    "        boxes = boxes[remain]\n",
    "        scores = scores[remain]\n",
    "        labels = labels[remain]\n",
    "\n",
    "    # Return only the boxes we kept\n",
    "    boxes = boxes[keep_indices]\n",
    "    scores = scores[keep_indices]\n",
    "    labels = labels[keep_indices]\n",
    "\n",
    "    total_removed = removed_low_score + removed_nms\n",
    "#     print(f\"[Postprocess] Removed {removed_low_score} low-score boxes and {removed_nms} overlapping boxes \"\n",
    "#           f\"(total removed: {total_removed}).\")\n",
    "\n",
    "    return boxes, scores, labels, initial_count, removed_low_score, total_removed\n",
    "\n",
    "\n",
    "def compute_iou_batch(box, boxes):\n",
    "    \"\"\"Vectorized IoU for one box vs. many boxes.\"\"\"\n",
    "    xA = np.maximum(box[0], boxes[:, 0])\n",
    "    yA = np.maximum(box[1], boxes[:, 1])\n",
    "    xB = np.minimum(box[2], boxes[:, 2])\n",
    "    yB = np.minimum(box[3], boxes[:, 3])\n",
    "\n",
    "    inter_w = np.maximum(0, xB - xA)\n",
    "    inter_h = np.maximum(0, yB - yA)\n",
    "    inter = inter_w * inter_h\n",
    "\n",
    "    area1 = (box[2] - box[0]) * (box[3] - box[1])\n",
    "    area2 = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
    "\n",
    "    return inter / (area1 + area2 - inter + 1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "41050407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second post processing\n",
    "import torch\n",
    "import torchvision.ops as ops\n",
    "import numpy as np\n",
    "\n",
    "def calculate_iou(boxA, boxB):\n",
    "    # Determine the coordinates of the intersection rectangle\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    # Compute the area of intersection\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "\n",
    "    # Compute the area of both the prediction and ground-truth rectangles\n",
    "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "\n",
    "    # Compute the intersection over union\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    return iou\n",
    "\n",
    "def postprocess_predictions(boxes, scores, labels, iou_thresh):\n",
    "    \"\"\"\n",
    "    Applies Non-Maximum Suppression (NMS) to the predicted bounding boxes.\n",
    "\n",
    "    Args:\n",
    "        boxes (np.array): Predicted bounding box coordinates in (x1, y1, x2, y2) format.\n",
    "        scores (np.array): Confidence scores for each predicted box.\n",
    "        labels (np.array): Labels for each predicted box.\n",
    "        iou_thresh (float): The IoU threshold for NMS. Boxes with IoU above this\n",
    "                            with a higher-scoring box will be suppressed.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (filtered_boxes, filtered_scores, filtered_labels, initial_count, duplicated_removed)\n",
    "               - filtered_boxes: Bounding boxes after NMS.\n",
    "               - filtered_scores: Scores after NMS.\n",
    "               - filtered_labels: Labels after NMS.\n",
    "               - initial_count: Number of boxes before NMS.\n",
    "               - duplicated_removed: Number of boxes removed by NMS.\n",
    "    \"\"\"\n",
    "    initial_count = len(boxes)\n",
    "\n",
    "    if initial_count == 0:\n",
    "        return boxes, scores, labels, initial_count, 0\n",
    "\n",
    "    # Convert to torch tensors for torchvision.ops.nms\n",
    "    # torchvision.ops.nms expects boxes in (x1, y1, x2, y2) format\n",
    "    boxes_tensor = torch.from_numpy(boxes).float()\n",
    "    scores_tensor = torch.from_numpy(scores).float()\n",
    "\n",
    "    # Apply NMS\n",
    "    # The `nms` function returns the indices of the boxes to keep\n",
    "    keep_indices = ops.nms(boxes_tensor, scores_tensor, iou_thresh)\n",
    "\n",
    "    filtered_boxes = boxes[keep_indices.cpu().numpy()]\n",
    "    filtered_scores = scores[keep_indices.cpu().numpy()]\n",
    "    filtered_labels = labels[keep_indices.cpu().numpy()]\n",
    "\n",
    "    duplicated_removed = initial_count - len(filtered_boxes)\n",
    "\n",
    "    return filtered_boxes, filtered_scores, filtered_labels, initial_count, duplicated_removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a7502a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "################### (Soft-NMS) ###################\n",
    "import torch\n",
    "import torchvision.ops as ops\n",
    "import numpy as np\n",
    "\n",
    "# You can keep calculate_iou if you want to manually test IoU,\n",
    "# but it's not strictly needed for torchvision.ops.soft_nms\n",
    "\n",
    "def postprocess_predictions_soft_nms(boxes, scores, labels, iou_thresh, sigma=0.5, score_threshold=0.001):\n",
    "    \"\"\"\n",
    "    Applies Soft-Non-Maximum Suppression (Soft-NMS) to the predicted bounding boxes.\n",
    "\n",
    "    Args:\n",
    "        boxes (np.array): Predicted bounding box coordinates in (x1, y1, x2, y2) format.\n",
    "        scores (np.array): Confidence scores for each predicted box.\n",
    "        labels (np.array): Labels for each predicted box.\n",
    "        iou_thresh (float): The IoU threshold for Soft-NMS. Overlapping boxes\n",
    "                            will have their scores reduced based on this.\n",
    "        sigma (float): The sigma parameter for the Gaussian penalty function.\n",
    "                       Higher sigma means a \"softer\" penalty (scores reduce less).\n",
    "                       Lower sigma means a \"harder\" penalty, closer to traditional NMS.\n",
    "                       Common range: 0.1 to 0.7.\n",
    "        score_threshold (float): A final threshold to filter out boxes whose scores\n",
    "                                 have been reduced below this value by Soft-NMS.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (filtered_boxes, filtered_scores, filtered_labels, initial_count, duplicated_removed)\n",
    "               - filtered_boxes: Bounding boxes after Soft-NMS.\n",
    "               - filtered_scores: Scores after Soft-NMS.\n",
    "               - filtered_labels: Labels after Soft-NMS.\n",
    "               - initial_count: Number of boxes before NMS.\n",
    "               - duplicated_removed: Number of boxes effectively removed (score below final threshold).\n",
    "    \"\"\"\n",
    "    initial_count = len(boxes)\n",
    "\n",
    "    if initial_count == 0:\n",
    "        return boxes, scores, labels, initial_count, 0\n",
    "\n",
    "    boxes_tensor = torch.from_numpy(boxes).float()\n",
    "    scores_tensor = torch.from_numpy(scores).float()\n",
    "    labels_tensor = torch.from_numpy(labels).long() # Labels are needed for batched_nms, though Soft-NMS on its own doesn't use them\n",
    "\n",
    "    # torchvision.ops.soft_nms doesn't return indices directly for `keep_indices` in the same way\n",
    "    # it returns updated scores and then you threshold those scores.\n",
    "    # Also, soft_nms does NOT have a label parameter, so if you have multiple classes\n",
    "    # you would typically run NMS/Soft-NMS per class or use batched_nms for NMS.\n",
    "    # For single class (vessel wake), this is fine.\n",
    "\n",
    "    # Soft-NMS (assumes single class or you handle per class before this)\n",
    "    # The `soft_nms` function returns updated scores and indices of original boxes\n",
    "    # in the sorted order. We need to apply it to unsorted boxes and scores for correct behavior.\n",
    "\n",
    "    # torchvision.ops.soft_nms works best when applied directly to unsorted predictions\n",
    "    # and then you filter based on the new scores.\n",
    "    # However, a common way to use it if you want to still get indices to filter\n",
    "    # and if you have multiple labels is to combine with batched_nms for batched Soft-NMS logic,\n",
    "    # or apply it directly and then threshold.\n",
    "\n",
    "    # Let's use the direct approach with torchvision.ops.soft_nms and then threshold\n",
    "    # Note: torchvision's soft_nms returns (kept_boxes, kept_scores) where kept_boxes\n",
    "    # are indices and kept_scores are the modified scores.\n",
    "    # This requires a bit of re-indexing.\n",
    "\n",
    "    # For simpler integration, let's stick to the common pattern for object detection\n",
    "    # where NMS/Soft-NMS are applied based on scores and then labels are filtered.\n",
    "    # We will sort by score first, which is standard.\n",
    "\n",
    "    # Sort by scores in descending order\n",
    "    order = scores_tensor.argsort(descending=True)\n",
    "    boxes_sorted = boxes_tensor[order]\n",
    "    scores_sorted = scores_tensor[order]\n",
    "    labels_sorted = labels_tensor[order]\n",
    "\n",
    "    # Apply Soft-NMS\n",
    "    # soft_nms returns scores, and then you apply a final score_threshold\n",
    "    # It does not directly return indices to keep in the original tensor.\n",
    "    # It typically returns a tensor of updated scores.\n",
    "    # Let's re-evaluate torchvision's soft_nms\n",
    "    # It returns (updated_scores, keep_indices) where keep_indices are relative to the input tensor.\n",
    "\n",
    "    updated_scores, keep_indices_relative = ops.soft_nms(boxes_sorted, scores_sorted, iou_thresh, sigma)\n",
    "\n",
    "    # Filter based on the final score_threshold after Soft-NMS\n",
    "    final_keep_indices_mask = updated_scores >= score_threshold\n",
    "    \n",
    "    # Map back to original indices if needed, or just apply to sorted tensors\n",
    "    final_keep_indices = keep_indices_relative[final_keep_indices_mask]\n",
    "    \n",
    "    # Apply these indices to the sorted tensors\n",
    "    filtered_boxes = boxes_sorted[final_keep_indices].cpu().numpy()\n",
    "    filtered_scores = updated_scores[final_keep_indices].cpu().numpy()\n",
    "    filtered_labels = labels_sorted[final_keep_indices].cpu().numpy() # Keep the original labels corresponding to sorted boxes\n",
    "\n",
    "    duplicated_removed = initial_count - len(filtered_boxes)\n",
    "\n",
    "    return filtered_boxes, filtered_scores, filtered_labels, initial_count, duplicated_removed\n",
    "\n",
    "# In your validation loop:\n",
    "# boxes, scores, labels, initial_count, duplicated_removed = postprocess_predictions_soft_nms(\n",
    "#     boxes, scores, labels,\n",
    "#     iou_thresh=0.5, # Adjust based on your findings (maybe 0.3-0.5)\n",
    "#     sigma=0.5,      # Tune this. Higher = less aggressive, lower = more aggressive\n",
    "#     score_threshold=0.001 # A very low threshold to keep most boxes unless suppressed significantly\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "265668f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- NEW: IoS + WBF helpers ----------\n",
    "import numpy as np\n",
    "\n",
    "def _area(b):  # b = [x1,y1,x2,y2]\n",
    "    return max(0.0, b[2]-b[0]) * max(0.0, b[3]-b[1])\n",
    "\n",
    "def _inter(a,b):\n",
    "    x1 = max(a[0], b[0]); y1 = max(a[1], b[1])\n",
    "    x2 = min(a[2], b[2]); y2 = min(a[3], b[3])\n",
    "    return max(0.0, x2-x1) * max(0.0, y2-y1)\n",
    "\n",
    "def _ios(a,b):\n",
    "    I = _inter(a,b)\n",
    "    return I / (min(_area(a), _area(b)) + 1e-9)\n",
    "\n",
    "def _ios_suppress_single_class(boxes, scores, ios_thr=0.90):\n",
    "    \"\"\"Keep high-score boxes; drop others if IoS>=thr to any kept box.\"\"\"\n",
    "    order = np.argsort(scores)[::-1]\n",
    "    keep = []\n",
    "    for idx_m, i in enumerate(order):\n",
    "        keep_it = True\n",
    "        for j in keep:  # j are indices in original array\n",
    "            if _ios(boxes[i], boxes[j]) >= ios_thr:\n",
    "                keep_it = False\n",
    "                break\n",
    "        if keep_it:\n",
    "            keep.append(i)\n",
    "    keep = np.array(keep, dtype=int)\n",
    "    return boxes[keep], scores[keep], keep\n",
    "\n",
    "def _wbf_single_class(boxes, scores, ios_thr=0.90, p=2.0, score_fuse='max', strategy='fuse'):\n",
    "    \"\"\"\n",
    "    Cluster boxes by IoS and either fuse them (weighted mean) or keep the 'best' one.\n",
    "    boxes: (N,4) np.array [x1,y1,x2,y2]\n",
    "    scores: (N,) np.array\n",
    "    \"\"\"\n",
    "    n = len(scores)\n",
    "    if n == 0:\n",
    "        return np.empty((0,4)), np.empty((0,))\n",
    "    used = np.zeros(n, dtype=bool)\n",
    "    order = np.argsort(scores)[::-1]\n",
    "    fused_boxes, fused_scores = [], []\n",
    "\n",
    "    for i in order:\n",
    "        if used[i]:\n",
    "            continue\n",
    "        # form a cluster around i\n",
    "        cluster_idx = [i]\n",
    "        used[i] = True\n",
    "        for j in order:\n",
    "            if used[j]:\n",
    "                continue\n",
    "            if _ios(boxes[i], boxes[j]) >= ios_thr:\n",
    "                cluster_idx.append(j)\n",
    "                used[j] = True\n",
    "\n",
    "        b = boxes[cluster_idx]\n",
    "        s = scores[cluster_idx]\n",
    "\n",
    "        if strategy == 'best':           # keep highest-score box only\n",
    "            k = int(np.argmax(s))\n",
    "            fused = b[k]\n",
    "            fused_score = float(s[k])\n",
    "\n",
    "        elif strategy == 'min_area':     # keep smallest area box in cluster\n",
    "            areas = (b[:,2]-b[:,0]) * (b[:,3]-b[:,1])\n",
    "            k = int(np.argmin(areas))\n",
    "            fused = b[k]\n",
    "            fused_score = float(s[k])\n",
    "\n",
    "        else:                            # 'fuse' (weighted average of coords)\n",
    "            w = (s ** p)[:, None]\n",
    "            fused = (w * b).sum(axis=0) / w.sum()\n",
    "            fused_score = float(s.max() if score_fuse == 'max' else s.mean())\n",
    "\n",
    "        fused_boxes.append(fused)\n",
    "        fused_scores.append(fused_score)\n",
    "\n",
    "    return np.vstack(fused_boxes), np.array(fused_scores)\n",
    "\n",
    "def postprocess_ios_only(boxes, scores, labels, ios_thr=0.90):\n",
    "    \"\"\"Per-class IoS containment suppression.\"\"\"\n",
    "    initial_count = len(boxes)\n",
    "    if initial_count == 0:\n",
    "        return boxes, scores, labels, 0, 0\n",
    "\n",
    "    out_b, out_s, out_l = [], [], []\n",
    "    for c in np.unique(labels):\n",
    "        m = labels == c\n",
    "        b, s = boxes[m], scores[m]\n",
    "        if len(b) == 0: \n",
    "            continue\n",
    "        b2, s2, keep_idx = _ios_suppress_single_class(b, s, ios_thr=ios_thr)\n",
    "        out_b.append(b2)\n",
    "        out_s.append(s2)\n",
    "        out_l.append(np.full(len(s2), c, dtype=labels.dtype))\n",
    "\n",
    "    if out_b:\n",
    "        boxes_f = np.concatenate(out_b, axis=0)\n",
    "        scores_f = np.concatenate(out_s, axis=0)\n",
    "        labels_f = np.concatenate(out_l, axis=0)\n",
    "        order = np.argsort(scores_f)[::-1]\n",
    "        boxes_f, scores_f, labels_f = boxes_f[order], scores_f[order], labels_f[order]\n",
    "    else:\n",
    "        boxes_f, scores_f, labels_f = boxes, scores, labels\n",
    "\n",
    "    duplicated_removed = initial_count - len(boxes_f)\n",
    "    return boxes_f, scores_f, labels_f, initial_count, duplicated_removed\n",
    "\n",
    "def postprocess_ios_wbf(boxes, scores, labels, ios_thr=0.90, p=2.0, score_fuse='max', strategy='fuse'):\n",
    "    initial_count = len(boxes)\n",
    "    if initial_count == 0:\n",
    "        return boxes, scores, labels, 0, 0\n",
    "\n",
    "    out_b, out_s, out_l = [], [], []\n",
    "    for c in np.unique(labels):\n",
    "        m = labels == c\n",
    "        b, s = boxes[m], scores[m]\n",
    "        if len(b) == 0:\n",
    "            continue\n",
    "        fb, fs = _wbf_single_class(b, s, ios_thr=ios_thr, p=p, score_fuse=score_fuse, strategy=strategy)\n",
    "        out_b.append(fb); out_s.append(fs); out_l.append(np.full(len(fs), c, dtype=labels.dtype))\n",
    "\n",
    "    if out_b:\n",
    "        boxes_f = np.concatenate(out_b, axis=0)\n",
    "        scores_f = np.concatenate(out_s, axis=0)\n",
    "        labels_f = np.concatenate(out_l, axis=0)\n",
    "        order = np.argsort(scores_f)[::-1]\n",
    "        boxes_f, scores_f, labels_f = boxes_f[order], scores_f[order], labels_f[order]\n",
    "    else:\n",
    "        boxes_f, scores_f, labels_f = boxes, scores, labels\n",
    "\n",
    "    duplicated_removed = initial_count - len(boxes_f)\n",
    "    return boxes_f, scores_f, labels_f, initial_count, duplicated_removed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "059a6d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# __________________________ draw FN causing by post processing _____________________\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def draw_debug_pil(img_np, gt_boxes, pre_boxes, post_boxes, became_fn, save_path):\n",
    "    # expect img_np in [0,1] float or uint8\n",
    "    if img_np.dtype != np.uint8:\n",
    "        img = Image.fromarray((img_np * 255).clip(0,255).astype(np.uint8))\n",
    "    else:\n",
    "        img = Image.fromarray(img_np)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    # GT (red, thin)\n",
    "    for (x1,y1,x2,y2) in gt_boxes:\n",
    "        draw.rectangle([x1,y1,x2,y2], outline=(255,0,0), width=2)\n",
    "\n",
    "    # pre (blue)\n",
    "    for (x1,y1,x2,y2) in pre_boxes:\n",
    "        draw.rectangle([x1,y1,x2,y2], outline=(255, 255, 0), width=2)\n",
    "\n",
    "    # post (green, thicker)\n",
    "    for (x1,y1,x2,y2) in post_boxes:\n",
    "        draw.rectangle([x1,y1,x2,y2], outline=(0,255,0), width=3)\n",
    "\n",
    "    # became-FN GTs (thick red highlight)\n",
    "    for (x1,y1,x2,y2) in became_fn:\n",
    "        draw.rectangle([x1,y1,x2,y2], outline=(255,0,0), width=5)\n",
    "\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    if save_path.lower().endswith(\".jpg\"):\n",
    "        save_path = save_path[:-4] + \".png\"   # prefer lossless\n",
    "    img.save(save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2496f390",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycocotools\n",
      "  Downloading pycocotools-2.0.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pycocotools) (2.2.6)\n",
      "Downloading pycocotools-2.0.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (455 kB)\n",
      "Installing collected packages: pycocotools\n",
      "Successfully installed pycocotools-2.0.10\n"
     ]
    }
   ],
   "source": [
    "!pip install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05f37565",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (2025.7.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4eacd32",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: torch==2.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torchvision) (2.6.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (4.14.1)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (2025.7.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d7289c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_22365/1084551879.py\", line 20, in <module>\n",
      "    from sklearn.metrics import confusion_matrix\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/__init__.py\", line 73, in <module>\n",
      "    from .base import clone  # noqa: E402\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/base.py\", line 19, in <module>\n",
      "    from .utils._metadata_requests import _MetadataRequester, _routing_enabled\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/utils/__init__.py\", line 9, in <module>\n",
      "    from ._chunking import gen_batches, gen_even_slices\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/utils/_chunking.py\", line 11, in <module>\n",
      "    from ._param_validation import Interval, validate_params\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 17, in <module>\n",
      "    from .validation import _is_arraylike_not_scalar\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 21, in <module>\n",
      "    from ..utils._array_api import _asarray_with_order, _is_numpy_namespace, get_namespace\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/utils/_array_api.py\", line 20, in <module>\n",
      "    from .fixes import parse_version\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/utils/fixes.py\", line 20, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/arrays/__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/arrays/arrow/__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/arrays/arrow/array.py\", line 50, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/ops/__init__.py\", line 8, in <module>\n",
      "    from pandas.core.ops.array_ops import (\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/ops/array_ops.py\", line 56, in <module>\n",
      "    from pandas.core.computation import expressions\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py\", line 21, in <module>\n",
      "    from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/check.py\", line 5, in <module>\n",
      "    ne = import_optional_dependency(\"numexpr\", errors=\"warn\")\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/compat/_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/numexpr/__init__.py\", line 26, in <module>\n",
      "    from numexpr.interpreter import MAX_THREADS, use_vml, __BLOCK_SIZE1__\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms as T\n",
    "import os\n",
    "import json\n",
    "import yaml\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "# Pycoc|otools for evaluation\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "# For confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42790928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_normal(num_classes, model_path):\n",
    "    model = fasterrcnn_resnet50_fpn(weights=None) # Load without pre-trained weights first\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "    model.to(DEVICE)\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    return model\n",
    "\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "import torch\n",
    "\n",
    "# optional (for newer torchvision); safe to keep\n",
    "try:\n",
    "    from torchvision.models import ResNet50_Weights\n",
    "    BACKBONE_WEIGHTS = ResNet50_Weights.DEFAULT\n",
    "except Exception:\n",
    "    BACKBONE_WEIGHTS = None\n",
    "\n",
    "def get_model_7anchor(num_classes, ckpt_path=None, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n",
    "    # ==== MUST MATCH TRAINING ====\n",
    "    # 5 FPN levels, same count per level\n",
    "    sizes = ((32,), (64,), (128,), (256,), (512,))\n",
    "    aspect = (0.2, 0.33, 0.5, 1.0, 2.0, 3.0, 5.0)\n",
    "    aspect_ratios = (aspect, aspect, aspect, aspect, aspect)\n",
    "\n",
    "    anchor_generator = AnchorGenerator(sizes=sizes, aspect_ratios=aspect_ratios)\n",
    "\n",
    "    model = fasterrcnn_resnet50_fpn(\n",
    "        weights=None,                       # do NOT load COCO head (shapes won't match)\n",
    "        weights_backbone=BACKBONE_WEIGHTS,  # OK to use ImageNet backbone\n",
    "        rpn_anchor_generator=anchor_generator,\n",
    "        rpn_nms_thresh=0.5,\n",
    "        box_score_thresh=0.35,\n",
    "        box_nms_thresh=0.30,\n",
    "        box_detections_per_img=75,\n",
    "        box_fg_iou_thresh=0.60,\n",
    "        box_bg_iou_thresh=0.40,\n",
    "        box_batch_size_per_image=512,\n",
    "        box_positive_fraction=0.20,\n",
    "        min_size=1000,\n",
    "        max_size=1800,\n",
    "    )\n",
    "\n",
    "    # class head must match your dataset size\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    if ckpt_path is not None:\n",
    "        state = torch.load(ckpt_path, map_location='cpu')\n",
    "        # strict=True should work if anchors/classes match; set False if you want leniency\n",
    "        missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "        print(\"Loaded checkpoint. Missing keys:\", len(missing), \"Unexpected keys:\", len(unexpected))\n",
    "\n",
    "    model.to(device).eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_transform():\n",
    "    return Compose([\n",
    "        ToTensor(),\n",
    "        # No augmentation for evaluation\n",
    "    ])\n",
    "\n",
    "def create_coco_annotations(dataset, output_file_path, class_names_list):\n",
    "    \"\"\"\n",
    "    Creates a COCO format annotation file from the WakeDetectionDataset.\n",
    "    This is needed for pycocotools.\n",
    "    \"\"\"\n",
    "    coco_format = {\n",
    "        \"info\": {\n",
    "            \"description\": \"Wake Detection Dataset - Faster R-CNN Evaluation\",\n",
    "            \"version\": \"1.0\",\n",
    "            \"year\": datetime.datetime.now().year, # Current year\n",
    "            \"contributor\": \"Your Name/Organization\",\n",
    "            \"date_created\": datetime.datetime.now().strftime(\"%Y/%m/%d\")\n",
    "        },\n",
    "        \"licenses\": [\n",
    "            {\n",
    "                \"id\": 1,\n",
    "                \"name\": \"Unknown License\",\n",
    "                \"url\": \"\"\n",
    "            }\n",
    "        ],\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": []\n",
    "    }\n",
    "\n",
    "    # Categories (start from 1, class 0 is background)\n",
    "    for i, class_name in enumerate(class_names_list[1:]): # Skip '__background__' at index 0\n",
    "        coco_format['categories'].append({\n",
    "            \"id\": i + 1, # COCO category IDs start from 1\n",
    "            \"name\": class_name,\n",
    "            \"supercategory\": \"none\"\n",
    "        })\n",
    "\n",
    "    ann_id_counter = 0\n",
    "    # Collect all image data first from the dataset's internal structure\n",
    "    image_data_map = dataset.img_data # This is a dict of image_id -> annotation_dict\n",
    "\n",
    "    # Populate images and annotations\n",
    "    for img_id in dataset.image_ids:\n",
    "        data = image_data_map[img_id]\n",
    "        image_info = {\n",
    "            \"id\": img_id,\n",
    "            \"file_name\": data['file_name'],\n",
    "            \"width\": data['width'],\n",
    "            \"height\": data['height']\n",
    "        }\n",
    "        coco_format['images'].append(image_info)\n",
    "\n",
    "        for ann_item in data['annotations']:\n",
    "            xmin, ymin, xmax, ymax = ann_item['bbox']\n",
    "            width = xmax - xmin\n",
    "            height = ymax - ymin\n",
    "            bbox_coco = [xmin, ymin, width, height] # COCO uses [xmin, ymin, width, height]\n",
    "\n",
    "            annotation_info = {\n",
    "                \"id\": ann_id_counter,\n",
    "                \"image_id\": img_id,\n",
    "                \"category_id\": ann_item['category_id'],\n",
    "                \"bbox\": bbox_coco,\n",
    "                \"area\": ann_item['area'],\n",
    "                \"iscrowd\": ann_item['iscrowd']\n",
    "            }\n",
    "            coco_format['annotations'].append(annotation_info)\n",
    "            ann_id_counter += 1\n",
    "\n",
    "    with open(output_file_path, 'w') as f:\n",
    "        json.dump(coco_format, f, indent=4)\n",
    "    print(f\"COCO format annotations saved to {output_file_path}\")\n",
    "    return coco_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1e68ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration (Match with Training) ---\n",
    "DATASET_ROOT = \"../Dataset\"\n",
    "ANNOTATION_FILE = \"../Dataset/faster_rcnn_annotations.json\"\n",
    "NUM_CLASSES = 2 # Placeholder, will be determined from YAML\n",
    "BATCH_SIZE = 4 # Can be higher for evaluation if memory allows\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# --- Path to Trained Model ---\n",
    "# first train\n",
    "# TRAINED_MODEL_PATH = \"runs-fasterrcnn/faster_rcnn_resnet50_fpn_20250707-145842/best_model_epoch_7.pth\"\n",
    "# second train\n",
    "anchor_model = True\n",
    "TRAINED_MODEL_PATH = \"runs-fasterrcnn2/faster_rcnn_resnet50_fpn_20250811-005533/best_model_epoch_20.pth\"\n",
    "EVAL_OUTPUT_DIR = os.path.dirname(TRAINED_MODEL_PATH) # Save eval results in the same run directory\n",
    "\n",
    "# --- Post-processing mode ---\n",
    "POSTPROC_MODE = \"none\"      # options: \"none\", \"ios\", \"ios_wbf\"\n",
    "IOS_THR = 0.92\n",
    "WBF_P = 3.0\n",
    "WBF_SCORE_FUSE = \"max\"     # \"max\" or \"mean\"\n",
    "WBF_STRATEGY = \"fused\"   # options: \"fuse\", \"best\", \"min_area\"\n",
    "SCORE_THRESH = 0.5\n",
    "MATCH_IOU_THR = 0.5\n",
    "\n",
    "\"\"\"\n",
    "strategy='fuse' → green is a weighted average of the clustered yellows.\n",
    "\n",
    "strategy='best' → green is exactly one of the yellows (the top-score in the cluster).\"\"\"\n",
    "\n",
    "\n",
    "SAVE_FN_DEBUG = True # save the images that became FN after post processing\n",
    "fn_debug_dir = os.path.join(EVAL_OUTPUT_DIR, \"debug_fn_due_to_postproc\")\n",
    "if SAVE_FN_DEBUG:\n",
    "    os.makedirs(fn_debug_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d34f4746",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred 1 custom classes. Setting NUM_CLASSES for Faster R-CNN to 2 (including background).\n",
      "Validation dataset size: 3443\n",
      "COCO format annotations saved to runs-fasterrcnn2/faster_rcnn_resnet50_fpn_20250811-005533/val_gt_coco.json\n",
      "Loading model from runs-fasterrcnn2/faster_rcnn_resnet50_fpn_20250811-005533/best_model_epoch_20.pth...\n",
      "Loaded checkpoint. Missing keys: 0 Unexpected keys: 0\n",
      "Model loaded.\n",
      "\n",
      "--- Postprocess Summary ---\n",
      "\n",
      " total_initial_count =  4107\n",
      "\n",
      " total_removed_low_score =  0\n",
      "\n",
      " total_duplicated_removed =  0\n",
      "Predictions saved to runs-fasterrcnn2/faster_rcnn_resnet50_fpn_20250811-005533/predictions_coco.json\n",
      "\n",
      "--- Running COCO Evaluation ---\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.96s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.16s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.434\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.849\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.404\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.434\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.465\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.518\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.518\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.518\n",
      "\n",
      "--- Generating Confusion Matrix (Simplified) ---\n",
      "Total Ground Truth Objects (Wake): 3523\n",
      "Total Predicted Objects (Wake, score >= 0.5): 3802\n",
      "\n",
      "Note: A detailed object detection confusion matrix (like YOLO's) requires IoU matching, which is not directly implemented here. Use COCOeval metrics for primary evaluation.\n",
      "\n",
      "--- Matching-based Metrics (IoU ≥ 0.5) ---\n",
      "TP: 3075, FP: 727, FN: 448\n",
      "Precision: 0.809, Recall: 0.873, F1 Score: 0.840\n",
      "Evaluation summary saved to runs-fasterrcnn2/faster_rcnn_resnet50_fpn_20250811-005533/evaluation_summary.txt\n"
     ]
    }
   ],
   "source": [
    "def evaluate():\n",
    "    os.makedirs(EVAL_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    # Load class names from your YAML file to determine NUM_CLASSES\n",
    "    yaml_path = \"../Dataset/vessel_wakes.yaml\"\n",
    "    try:\n",
    "        with open(yaml_path, 'r') as f:\n",
    "            yaml_config = yaml.safe_load(f)\n",
    "            class_names_dict = yaml_config['names']\n",
    "            raw_class_names = [class_names_dict[i] for i in sorted(class_names_dict.keys())]\n",
    "            global NUM_CLASSES\n",
    "            NUM_CLASSES = len(raw_class_names) + 1 # +1 for background\n",
    "            fasterrcnn_class_names = ['__background__'] + raw_class_names # For indexing\n",
    "            print(f\"Inferred {len(raw_class_names)} custom classes. Setting NUM_CLASSES for Faster R-CNN to {NUM_CLASSES} (including background).\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {yaml_path} not found. Please check the path.\")\n",
    "        exit()\n",
    "    except KeyError:\n",
    "        print(\"Error: 'names' key not found in your YAML config. Ensure it defines your class names.\")\n",
    "        exit()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading YAML: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # Create validation dataset and dataloader\n",
    "    val_dataset = WakeDetectionDataset(\n",
    "        root_dir=DATASET_ROOT,\n",
    "        annotation_file=ANNOTATION_FILE,\n",
    "        split='valid', # **************************************************** metrics for validation\n",
    "        transform=get_transform()\n",
    "    )\n",
    "    \n",
    "    # keep a quick handle to the dataset file map so we can load images (for ploting FN)\n",
    "    img_data_map = val_dataset.img_data  # img_id -> {'file_name','width','height',...}\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=os.cpu_count() // 2,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "\n",
    "    # Create COCO ground truth file\n",
    "    gt_coco_file = os.path.join(EVAL_OUTPUT_DIR, \"val_gt_coco.json\")\n",
    "    # Need to pass fasterrcnn_class_names (which includes background)\n",
    "    gt_coco_data = create_coco_annotations(val_dataset, gt_coco_file, fasterrcnn_class_names)\n",
    "\n",
    "    # Load model\n",
    "    print(f\"Loading model from {TRAINED_MODEL_PATH}...\")\n",
    "    if anchor_model:\n",
    "        model = get_model_7anchor(NUM_CLASSES, TRAINED_MODEL_PATH)\n",
    "    else:\n",
    "        model = get_model(NUM_CLASSES, TRAINED_MODEL_PATH)\n",
    "    print(\"Model loaded.\")\n",
    "\n",
    "    # --- Generate Predictions ---\n",
    "    predictions = []\n",
    "    all_gt_labels = [] # List of actual class IDs (1-indexed) for each detected object\n",
    "    all_pred_labels = [] # (for confusion matrix): List of predicted class IDs\n",
    "    all_pred_scores = [] # (for confusion matrix thresholding)\n",
    "    \n",
    "    total_removed_low_score = 0\n",
    "    total_duplicated_removed = 0\n",
    "    total_initial_count = 0\n",
    "    gt_boxes_by_image = defaultdict(list)\n",
    "    pred_boxes_by_image = defaultdict(list)\n",
    "    with torch.no_grad():\n",
    "        for i, (images, targets) in enumerate(val_loader):\n",
    "            images = list(image.to(DEVICE) for image in images)\n",
    "            outputs = model(images)\n",
    "\n",
    "            for img_idx, output in enumerate(outputs):\n",
    "                img_id = targets[img_idx]['image_id'].item()\n",
    "                gt_boxes = targets[img_idx]['boxes'].cpu().detach().numpy() # for CM loop\n",
    "                boxes = output['boxes'].cpu().numpy()\n",
    "                labels = output['labels'].cpu().numpy()\n",
    "                scores = output['scores'].cpu().numpy()\n",
    "                \n",
    "                # --- keep a copy BEFORE post-processing ---\n",
    "                pre_boxes_all   = boxes.copy()\n",
    "                pre_scores_all  = scores.copy()\n",
    "                pre_labels_all  = labels.copy()\n",
    "                # image for plotting\n",
    "                img_np = images[img_idx].detach().cpu().permute(1,2,0).numpy()\n",
    "                img_np = img_np.clip(0,1)\n",
    "\n",
    "\n",
    "                \n",
    "                # Apply post processing\n",
    "                # initial_count, removed_low_score, duplicated_removed\n",
    "#                 print(\"\\n boxes\", boxes)\n",
    "#                 boxes, scores, labels, initial_count, duplicated_removed = postprocess_predictions(\n",
    "#                     boxes, scores, labels,\n",
    "# #                     score_thresh=0.6,   # only keep >0.6\n",
    "#                     iou_thresh=0.40      # remove duplicates if IoU > 0.9\n",
    "#                 )\n",
    "#                 boxes, scores, labels, initial_count, duplicated_removed = postprocess_predictions_soft_nms(\n",
    "#                     boxes, scores, labels,\n",
    "#                     iou_thresh=0.5, # Adjust based on your findings (maybe 0.3-0.5)\n",
    "#                     sigma=0.5,      # Tune this. Higher = less aggressive, lower = more aggressive\n",
    "#                     score_threshold=0.001 # A very low threshold to keep most boxes unless suppressed significantly\n",
    "#                 )\n",
    "\n",
    "                # === NEW: choose post-processing ===\n",
    "                if POSTPROC_MODE == \"ios\":\n",
    "                    boxes, scores, labels, initial_count, duplicated_removed = postprocess_ios_only(\n",
    "                        boxes, scores, labels, ios_thr=IOS_THR\n",
    "                    )\n",
    "                elif POSTPROC_MODE == \"ios_wbf\":\n",
    "                     boxes, scores, labels, initial_count, duplicated_removed = postprocess_ios_wbf(\n",
    "                            boxes, scores, labels,\n",
    "                            ios_thr=IOS_THR, p=WBF_P, score_fuse=WBF_SCORE_FUSE, strategy=WBF_STRATEGY\n",
    "                        )\n",
    "                else:\n",
    "                    # no extra postproc (keep your original NMS/Soft-NMS if you like)\n",
    "                    initial_count = len(boxes)\n",
    "                    duplicated_removed = 0\n",
    "                    \n",
    "#                 total_removed_low_score += removed_low_score\n",
    "                total_duplicated_removed += duplicated_removed\n",
    "                total_initial_count += initial_count\n",
    "        \n",
    "                # for ploting FN after post processing\n",
    "                # Filter by score for matching + debug\n",
    "                pre_keep = pre_scores_all >= SCORE_THRESH\n",
    "                pre_boxes_kept  = pre_boxes_all[pre_keep]\n",
    "                pre_scores_kept = pre_scores_all[pre_keep]\n",
    "                pre_labels_kept = pre_labels_all[pre_keep]\n",
    "\n",
    "                post_keep = scores >= SCORE_THRESH\n",
    "                post_boxes_kept  = boxes[post_keep]\n",
    "                post_scores_kept = scores[post_keep]\n",
    "                post_labels_kept = labels[post_keep]\n",
    "\n",
    "                def _iou(a,b):\n",
    "                    x1 = max(a[0], b[0]); y1 = max(a[1], b[1])\n",
    "                    x2 = min(a[2], b[2]); y2 = min(a[3], b[3])\n",
    "                    inter = max(0, x2-x1) * max(0, y2-y1)\n",
    "                    Aa = max(0, a[2]-a[0]) * max(0, a[3]-a[1])\n",
    "                    Ab = max(0, b[2]-b[0]) * max(0, b[3]-b[1])\n",
    "                    den = Aa + Ab - inter + 1e-9\n",
    "                    return inter/den\n",
    "\n",
    "                if SAVE_FN_DEBUG:\n",
    "                    # Find GTs matched before but not after\n",
    "                    gt_bxs = gt_boxes  # already numpy from your code\n",
    "                    became_fn = []\n",
    "                    for g in gt_bxs:\n",
    "                        matched_pre  = any(_iou(g, p) >= MATCH_IOU_THR for p in pre_boxes_kept)\n",
    "                        matched_post = any(_iou(g, p) >= MATCH_IOU_THR for p in post_boxes_kept)\n",
    "                        if matched_pre and not matched_post:\n",
    "                            became_fn.append(g)\n",
    "\n",
    "                    if len(became_fn):\n",
    "                        # Draw with emphasis on became-FN GTs\n",
    "                        # We’ll reuse drawer, then add thick red overlays for became-FN\n",
    "                        save_path = os.path.join(fn_debug_dir, f\"{img_id}_became_FN.jpg\")\n",
    "                        draw_debug_pil(img_np, gt_boxes, pre_boxes_kept, post_boxes_kept,\n",
    "                           became_fn, os.path.join(fn_debug_dir, f\"{img_id}_became_FN.png\"))\n",
    "                        # Thicken the specific became-FN boxes (optional second pass)\n",
    "                        # quick overlay: draw again only those boxes\n",
    "                        fig, ax = plt.subplots(figsize=(12,6))\n",
    "                        ax.imshow(img_np)\n",
    "                        for (x1,y1,x2,y2) in became_fn:\n",
    "                            ax.add_patch(plt.Rectangle((x1,y1), x2-x1, y2-y1, fill=False, lw=3.0, edgecolor='r'))\n",
    "                        ax.axis('off')\n",
    "                        fig.savefig(os.path.join(fn_debug_dir, f\"{img_id}_became_FN_only.jpg\"), bbox_inches='tight', dpi=150)\n",
    "                        plt.close(fig)\n",
    "\n",
    "                # Collect GT labels for confusion matrix\n",
    "                gt_labels_for_image = targets[img_idx]['labels'].cpu().numpy()\n",
    "                all_gt_labels.extend(gt_labels_for_image) # Add all ground truth labels\n",
    "                gt_boxes_by_image[img_id].extend(gt_boxes) # for CM loop\n",
    "\n",
    "                # Prepare predictions for COCO evaluation\n",
    "                for box, label, score in zip(boxes, labels, scores):\n",
    "                    xmin, ymin, xmax, ymax = box\n",
    "                    # COCO format requires [xmin, ymin, width, height]\n",
    "                    bbox_coco = [float(xmin), float(ymin), float(xmax - xmin), float(ymax - ymin)]\n",
    "                    predictions.append({\n",
    "                        \"image_id\": img_id,\n",
    "                        \"category_id\": int(label), # Use the 1-indexed label\n",
    "                        \"bbox\": bbox_coco,\n",
    "                        \"score\": float(score)\n",
    "                    })\n",
    "                    if score >= SCORE_THRESH:\n",
    "                        pred_boxes_by_image[img_id].append(box)\n",
    "\n",
    "                all_pred_labels.extend(labels)\n",
    "                all_pred_scores.extend(scores)\n",
    "\n",
    "\n",
    "    print(f\"\\n--- Postprocess Summary ---\")\n",
    "    print(\"\\n total_initial_count = \", total_initial_count)\n",
    "    print(\"\\n total_removed_low_score = \", total_removed_low_score)\n",
    "    print(\"\\n total_duplicated_removed = \", total_duplicated_removed)\n",
    "\n",
    "\n",
    "    pred_coco_file = os.path.join(EVAL_OUTPUT_DIR, \"predictions_coco.json\")\n",
    "    with open(pred_coco_file, 'w') as f:\n",
    "        json.dump(predictions, f, indent=4)\n",
    "    print(f\"Predictions saved to {pred_coco_file}\")\n",
    "\n",
    "    # --- COCO Evaluation ---\n",
    "    print(\"\\n--- Running COCO Evaluation ---\")\n",
    "    cocoGt = COCO(gt_coco_file)\n",
    "    cocoDt = cocoGt.loadRes(pred_coco_file)\n",
    "\n",
    "    cocoEval = COCOeval(cocoGt, cocoDt, 'bbox')\n",
    "    cocoEval.evaluate()\n",
    "    cocoEval.accumulate()\n",
    "    cocoEval.summarize()\n",
    "\n",
    "    # --- Confusion Matrix Generation (Simplified) --- \n",
    "    # This is a simplified confusion matrix for object detection.\n",
    "    # A full object detection confusion matrix involves matching predicted boxes to ground truth\n",
    "    # boxes based on IoU. For simplicity, we'll generate one based on all predicted labels\n",
    "    # vs. all ground truth labels, assuming a reasonable confidence threshold.\n",
    "    # This will NOT be exactly like YOLO's, which does box matching.\n",
    "\n",
    "    print(\"\\n--- Generating Confusion Matrix (Simplified) ---\")\n",
    "    # Filter predictions by a confidence threshold\n",
    "    pred_labels_filtered = [\n",
    "        label for label, score in zip(all_pred_labels, all_pred_scores)\n",
    "        if score >= SCORE_THRESH # Confidence threshold for CM.\n",
    "    ]\n",
    "\n",
    "    # Total actual wakes (sum of GT labels == 1)\n",
    "    num_gt_wakes = sum(1 for label in all_gt_labels if label == 1)\n",
    "    \n",
    "    # Total predicted wakes (sum of pred labels == 1 above threshold)\n",
    "    num_pred_wakes = sum(1 for label in pred_labels_filtered if label == 1)\n",
    "\n",
    "    print(f\"Total Ground Truth Objects (Wake): {num_gt_wakes}\")\n",
    "    print(f\"Total Predicted Objects (Wake, score >= 0.5): {num_pred_wakes}\")\n",
    "    print(\"\\nNote: A detailed object detection confusion matrix (like YOLO's) requires IoU matching, which is not directly implemented here. Use COCOeval metrics for primary evaluation.\")\n",
    "\n",
    "    # IoU function\n",
    "    def compute_iou(boxA, boxB):\n",
    "        xA = max(boxA[0], boxB[0])\n",
    "        yA = max(boxA[1], boxB[1])\n",
    "        xB = min(boxA[2], boxB[2])\n",
    "        yB = min(boxA[3], boxB[3])\n",
    "        interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "        boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "        boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "        return interArea / float(boxAArea + boxBArea - interArea)\n",
    "\n",
    "    # Matching-based evaluation\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    iou_threshold = 0.5\n",
    "\n",
    "    for img_id in gt_boxes_by_image:\n",
    "        gt = gt_boxes_by_image[img_id]\n",
    "        pred = pred_boxes_by_image.get(img_id, [])\n",
    "\n",
    "        matched_gt = set()\n",
    "        matched_pred = set()\n",
    "\n",
    "        for i, pbox in enumerate(pred):\n",
    "            for j, gbox in enumerate(gt):\n",
    "                if j in matched_gt:\n",
    "                    continue\n",
    "                if compute_iou(pbox, gbox) >= iou_threshold:\n",
    "                    TP += 1\n",
    "                    matched_pred.add(i)\n",
    "                    matched_gt.add(j)\n",
    "                    break\n",
    "\n",
    "        FP += len(pred) - len(matched_pred)\n",
    "        FN += len(gt) - len(matched_gt)\n",
    "\n",
    "    # Calculate metrics\n",
    "    precision = TP / (TP + FP) if (TP + FP) else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) else 0\n",
    "\n",
    "    \n",
    "    print(f\"\\n--- Matching-based Metrics (IoU \\u2265 {iou_threshold}) ---\")\n",
    "    print(f\"TP: {TP}, FP: {FP}, FN: {FN}\")\n",
    "    print(f\"Precision: {precision:.3f}, Recall: {recall:.3f}, F1 Score: {f1:.3f}\")\n",
    "    \n",
    "\n",
    "# ============================\n",
    "    # The `cocoEval.summarize()` output provides the most important numbers.\n",
    "\n",
    "    # Save evaluation results\n",
    "    with open(os.path.join(EVAL_OUTPUT_DIR, \"evaluation_summary.txt\"), 'w') as f:\n",
    "        f.write(\"COCO Evaluation Summary:\\n\")\n",
    "        # Redirect stdout of cocoEval.summarize() to file\n",
    "        import sys\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = f\n",
    "        cocoEval.summarize()\n",
    "        sys.stdout = old_stdout # Restore stdout\n",
    "    print(f\"Evaluation summary saved to {os.path.join(EVAL_OUTPUT_DIR, 'evaluation_summary.txt')}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure you update TRAINED_MODEL_PATH before running!\n",
    "    if TRAINED_MODEL_PATH == \"PATH_TO_YOUR_BEST_MODEL.pth\":\n",
    "        print(\"WARNING: TRAINED_MODEL_PATH not updated. Please set it to your actual trained model path.\")\n",
    "        # As a fallback, try to find the latest run and best model if you ran train()\n",
    "        # This is a hack, a better way is to pass the path from train script.\n",
    "        try:\n",
    "            latest_run = sorted([d for d in os.listdir(\"runs-fasterrcnn\") if os.path.isdir(os.path.join(\"runs-fasterrcnn\", d))], reverse=True)[0]\n",
    "            latest_run_path = os.path.join(\"runs-fasterrcnn\", latest_run)\n",
    "            best_model_file = [f for f in os.listdir(latest_run_path) if f.startswith(\"best_model\") and f.endswith(\".pth\")]\n",
    "            if best_model_file:\n",
    "                TRAINED_MODEL_PATH = os.path.join(latest_run_path, best_model_file[0])\n",
    "                global EVAL_OUTPUT_DIR\n",
    "                EVAL_OUTPUT_DIR = latest_run_path\n",
    "                print(f\"Attempting to use latest best model: {TRAINED_MODEL_PATH}\")\n",
    "            else:\n",
    "                print(\"Could not find a 'best_model' in the latest run directory. Exiting.\")\n",
    "                exit()\n",
    "        except Exception as e:\n",
    "            print(f\"Error finding latest model: {e}. Please manually set TRAINED_MODEL_PATH.\")\n",
    "            exit()\n",
    "            \n",
    "    evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5afae530",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scipy\n",
      "  Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting numpy<2.5,>=1.23.5 (from scipy)\n",
      "  Downloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m214.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m251.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy, scipy\n",
      "\u001b[2K  Attempting uninstall: numpy\n",
      "\u001b[2K    Found existing installation: numpy 2.2.6\n",
      "\u001b[2K    Uninstalling numpy-2.2.6:\n",
      "\u001b[2K      Successfully uninstalled numpy-2.2.6\n",
      "\u001b[2K  Attempting uninstall: scipy━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/2\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: scipy 1.15.3[0m \u001b[32m0/2\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling scipy-1.15.3:━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/2\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled scipy-1.15.3[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [scipy]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [scipy]32m1/2\u001b[0m [scipy]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sagemaker 2.247.0 requires numpy==1.26.4, but you have numpy 2.2.6 which is incompatible.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-2.2.6 scipy-1.15.3\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --force-reinstall --no-cache-dir scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "91998f19",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting numpy>=1.22.0 (from scikit-learn)\n",
      "  Downloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.7.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m154.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m248.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m262.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, numpy, joblib, scipy, scikit-learn\n",
      "\u001b[2K  Attempting uninstall: threadpoolctl\n",
      "\u001b[2K    Found existing installation: threadpoolctl 3.6.0\n",
      "\u001b[2K    Uninstalling threadpoolctl-3.6.0:\n",
      "\u001b[2K      Successfully uninstalled threadpoolctl-3.6.00m \u001b[32m0/5\u001b[0m [threadpoolctl]\n",
      "\u001b[2K  Attempting uninstall: numpy━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/5\u001b[0m [threadpoolctl]\n",
      "\u001b[2K    Found existing installation: numpy 2.2.6\u001b[0m \u001b[32m0/5\u001b[0m [threadpoolctl]\n",
      "\u001b[2K    Uninstalling numpy-2.2.6:m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled numpy-2.2.6━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: joblib\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: joblib 1.5.1━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling joblib-1.5.1:m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled joblib-1.5.1━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: scipy[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/5\u001b[0m [joblib]\n",
      "\u001b[2K    Found existing installation: scipy 1.15.3━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/5\u001b[0m [joblib]\n",
      "\u001b[2K    Uninstalling scipy-1.15.3:━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [scipy]\n",
      "\u001b[2K      Successfully uninstalled scipy-1.15.3[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [scipy]\n",
      "\u001b[2K  Attempting uninstall: scikit-learn\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [scipy]\n",
      "\u001b[2K    Found existing installation: scikit-learn 1.7.1━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [scipy]\n",
      "\u001b[2K    Uninstalling scikit-learn-1.7.1:m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [scipy]\n",
      "\u001b[2K      Successfully uninstalled scikit-learn-1.7.1━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [scipy]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [scikit-learn][0m [scikit-learn]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sagemaker 2.247.0 requires numpy==1.26.4, but you have numpy 2.2.6 which is incompatible.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed joblib-1.5.1 numpy-2.2.6 scikit-learn-1.7.1 scipy-1.15.3 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --force-reinstall --no-cache-dir scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7539c313",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Using cached numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m140.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.7.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m158.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, numpy, joblib, scipy, scikit-learn\n",
      "\u001b[2K  Attempting uninstall: threadpoolctl\n",
      "\u001b[2K    Found existing installation: threadpoolctl 3.6.0\n",
      "\u001b[2K    Uninstalling threadpoolctl-3.6.0:\n",
      "\u001b[2K      Successfully uninstalled threadpoolctl-3.6.00m \u001b[32m0/5\u001b[0m [threadpoolctl]\n",
      "\u001b[2K  Attempting uninstall: numpy━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/5\u001b[0m [threadpoolctl]\n",
      "\u001b[2K    Found existing installation: numpy 1.26.4[0m \u001b[32m0/5\u001b[0m [threadpoolctl]\n",
      "\u001b[2K    Uninstalling numpy-1.26.4:\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled numpy-1.26.4━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: joblib\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: joblib 1.5.1━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling joblib-1.5.1:m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled joblib-1.5.1━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: scipy[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/5\u001b[0m [joblib]\n",
      "\u001b[2K    Found existing installation: scipy 1.15.3━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/5\u001b[0m [joblib]\n",
      "\u001b[2K    Uninstalling scipy-1.15.3:━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [scipy]\n",
      "\u001b[2K      Successfully uninstalled scipy-1.15.3[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [scipy]\n",
      "\u001b[2K  Attempting uninstall: scikit-learn\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [scipy]\n",
      "\u001b[2K    Found existing installation: scikit-learn 1.7.0━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [scipy]\n",
      "\u001b[2K    Uninstalling scikit-learn-1.7.0:━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m4/5\u001b[0m [scikit-learn]\n",
      "\u001b[2K      Successfully uninstalled scikit-learn-1.7.0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m4/5\u001b[0m [scikit-learn]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [scikit-learn][0m [scikit-learn]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sagemaker 2.247.0 requires numpy==1.26.4, but you have numpy 2.2.6 which is incompatible.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed joblib-1.5.1 numpy-2.2.6 scikit-learn-1.7.1 scipy-1.15.3 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --force-reinstall numpy scipy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "90273ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n",
      "1.15.2\n"
     ]
    }
   ],
   "source": [
    "import numpy, scipy\n",
    "print(numpy.__version__)\n",
    "print(scipy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978fb089",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
